<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<title>생성 모델과 판별 모델</title>
	<link rel="stylesheet" href="/assets/css/default.css">
	<link rel="stylesheet" href="/assets/css/code.css">
	<link rel="icon" type="image/png" href="/assets/favicon/favicon.ico">
	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
	</script>

	<!-- head.html 또는 기본 레이아웃 파일에 추가 -->
	<link href='https://cdn.jsdelivr.net/npm/fullcalendar@5.9.0/main.min.css' rel='stylesheet' />
	<script src='https://cdn.jsdelivr.net/npm/fullcalendar@5.9.0/main.min.js'></script>
</head>

<body>
	<div class="header">
  <div class="blog_image">
    <img src="/assets/images/coffee.png" title="coffee">
  </div>
  <div class="main_text">
    <h1><a href='/'>In수</a></h1>
  </div>

  <div id="music-controls" style="text-align: center;">
    <!-- 첫 줄: GIF와 버튼 -->
    <div class="top-controls">
      <img id="musicGif" src="https://cdn.pixabay.com/animation/2022/12/05/15/23/15-23-06-837_512.gif" alt="Music GIF"
        style="display: inline-block; width: 50px; height: 50px; vertical-align: middle; margin-right: 0.5vw;">

      <button id="playButton" onclick="playMusic()">Play</button>
      <button id="pauseButton" onclick="pauseMusic()">Pause</button>
      <button id="nextButton" onclick="nextMusic()">Next</button>
    </div>

    <!-- 두 번째 줄: Volume 라벨과 슬라이더 -->
    <div class="volume-controls" style="margin-top: 0.5vw;">
      <label for="volumeSlider">Volume:</label>
      <input type="range" id="volumeSlider" min="0" max="1" step="0.01" value="1">
    </div>
  </div>

</div>
	<div class="container">
		<div class="navigation">
			<ul>
				
				<li><a href="/index.html">HOME</a></li>
				
				<li><a href="/posts/index.html">POSTS</a></li>
				
				<li><a href="/tags.html">TAGS</a></li>
				
				<li><a href="/books.html">BOOKS</a></li>
				
				<li><a href="/project.html">PROJECT</a></li>
				
			</ul>
		</div>
		<div class="main">
			<div class="markdown-body">
  <div class="md-content">
    <h1 id="생성-모델과-판별-모델---2025년02월27일">생성 모델과 판별 모델 - 2025년02월27일</h1>

<!-- - tag : DL COMPUTER_VISION  -->
<ul>
  <li>tag : <a href=" /tags/tag_DL.html" class="btn btn-default navbar-btn cursorNorm" role="button">DL</a>
|
<a href=" /tags/tag_COMPUTER_VISION.html" class="btn btn-default navbar-btn cursorNorm" role="button">COMPUTER_VISION</a>
|</li>
</ul>

<hr />

<h1 id="생성-모델과-판별-모델">생성 모델과 판별 모델</h1>

<h2 id="생성-모델-generative-models">생성 모델 (Generative Models)</h2>

<p>생성 모델은 데이터 X와 특성 Y의 결합 분포 p(X, Y) 또는 조건부 분포 p(X|Y)를 추정합니다. Y가 없는 경우, 데이터의 주변 분포 p(X)를 추정합니다.</p>

<p><strong>가정:</strong> 데이터는 저차원의 필수적인 정보로부터 생성 가능하다고 가정합니다.</p>

<p><strong>예시:</strong> 가우시안 혼합 모델 (Gaussian Mixture Model, GMM)</p>

<h3 id="특징">특징</h3>

<ol>
  <li><strong>어려움:</strong>
    <ul>
      <li>고차원 데이터 모델링: 복잡한 모든 특성의 분포를 알아야 함</li>
      <li>평가 지표: 생성된 데이터에 대한 정량적 평가가 어려움</li>
    </ul>
  </li>
  <li><strong>활용:</strong>
    <ul>
      <li>이미지 품질 개선</li>
      <li>맥락에 맞는 이미지 자동 완성</li>
    </ul>
  </li>
</ol>

<h3 id="최대-가능도-추정법-maximum-likelihood-estimation-mle">최대 가능도 추정법 (Maximum Likelihood Estimation, MLE)</h3>

<p>가능도를 최대화하는 파라미터 값을 찾는 방법입니다. 일반적으로 가능도 함수의 미분을 통해 계산합니다.</p>

<blockquote>
  <p><strong>Kullback-Leibler Divergence:</strong> 쿨백-라이블러 발산 최소화 = 로그가능도 최대화</p>
</blockquote>

<h3 id="평가-지표">평가 지표</h3>

<ol>
  <li>Inception Score(IS)
    <ul>
      <li>예리함과 다양성 두 가지를 주요하게 고려</li>
      <li>IS = Sharpness(S) * Diversity(D)</li>
      <li>한계점</li>
    </ul>
    <ol>
      <li>분류기 모델의 훈련 데이터 셋과 다른 데이터를 생성하는 경우 제대로 평가하기 어려움</li>
      <li>IS가 높은 데이터를 생성하면 계속 같은 데이터를 생성(Mode Collapse)</li>
      <li>기울기 기반(Gradient Based) 공격, 리플레이(Replay) 공격을 통해 점수 조작 가능</li>
    </ol>
  </li>
  <li>Frechet Inception Distance(FID)
    <ul>
      <li>생성된 데이터의 특징 벡터를 이용하여 훈련 데이터와의 거리를 계산</li>
      <li>훈련 데이터와 생성 데이터를 모두 활용</li>
      <li>훈련 데이터와 생성 데이터의 각 분포를 정규 분포로 가정하고, 두 분포의 거리를 Frechet 거리로 계산</li>
      <li>한계점
        <ol>
          <li>FID 점수는 Fidelity와 Diversity를 각각 평가할 수 없음</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>개선된 정밀도, 재현율(Improved Precision &amp; Recall)
    <ul>
      <li>Precision : 생성된 데이터 중에서, 실제 데이터 분포에 아주 가까운 데이터 = (TP) / (TP + FP)</li>
      <li>Recall : 실제 데이터 중에서, 생성된 데이터 분포에 아주 가까운 데이터 = (TP) / (TP + FN)</li>
      <li>한계점
        <ol>
          <li>이상치에 민감</li>
          <li>실제 데이터와 생성된 데이터의 분포가 동일하더라도 샘플링에 따라 점수가 낮을 수 있음</li>
        </ol>
      </li>
      <li>문제 완화
        <ol>
          <li>Density : 반경의 합집합이 아닌 가중 합집합으로 계산하여 이상치에 대해 상대적으로 덜 민감</li>
          <li>Coverage : 생성된 데이터에 대해 매번 계산하지 않고 실제 데이터 집합으로 미리 계산하여 안정적이고 계산량 감소</li>
        </ol>
      </li>
    </ul>
  </li>
  <li>조건부 정확도(Conditional Accuracy)</li>
  <li>Learned Perceptual Image Patch Similarity(LPIPS)
    <ul>
      <li>모델 특징 비교를 통한 영상간 유사도 측정</li>
    </ul>
  </li>
  <li>CLIP-Score
    <ul>
      <li>Text와 Image 간의 유사도 측정</li>
    </ul>
  </li>
</ol>

<h2 id="판별-모델-discriminative-model">판별 모델 (Discriminative Model)</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>판별 모델은 데이터 X가 주어졌을 때, 특성 Y가 나타날 조건부 확률 p(Y\|X)를 직접적으로 반환합니다.  
판별 모델은 정답(Ground Truth, GT)가 존재하므로 모델의 출력을 정답과 비교하기 용이  
분류, 회귀 문제로 나눌 수 있음
</code></pre></div></div>
<p><strong>특징:</strong> 주어진 데이터를 통해 데이터 사이의 경계를 예측합니다.</p>

<p><strong>예시:</strong> 로지스틱 회귀 분석</p>

<h2 id="생성-모델-vs-판별-모델">생성 모델 vs 판별 모델</h2>

<table>
  <thead>
    <tr>
      <th>특성</th>
      <th>생성 모델</th>
      <th>판별 모델</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>추정</td>
      <td>p(X, Y) 또는 p(X|Y)</td>
      <td>p(Y|X)</td>
    </tr>
    <tr>
      <td>접근 방식</td>
      <td>데이터 생성 과정 모델링</td>
      <td>클래스 간 경계 학습</td>
    </tr>
    <tr>
      <td>복잡도</td>
      <td>상대적으로 높음</td>
      <td>상대적으로 낮음</td>
    </tr>
    <tr>
      <td>데이터 요구량</td>
      <td>더 많은 데이터 필요</td>
      <td>적은 데이터로도 가능</td>
    </tr>
    <tr>
      <td>유연성</td>
      <td>새로운 클래스 추가 용이</td>
      <td>새로운 클래스 추가 어려움</td>
    </tr>
  </tbody>
</table>

<h1 id="오토-인코더">오토 인코더</h1>

<p>입력 데이터의 패턴을 학습하여 데이터를 재건하는 모델 -&gt; 비선형 차원 축소 기법으로 활용 가능</p>

<ul>
  <li><strong>인코더</strong> : 데이터를 저차원 잠재 표현으로 요약</li>
  <li><strong>디코더</strong> : 저차원 잠재 표현으로부터 데이터를 재구성(Reconstruction)</li>
</ul>

<h2 id="손실함수">손실함수</h2>

<p>잠재 표현으로부터 복구한 데이터와 입력 데이터의 평균제곱오차(MSE)</p>

<h2 id="디노이징denoising-오토-인코더">디노이징(Denoising) 오토 인코더</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>입력 데이터에 랜덤 노이즈를 주입하거나 Dropout 레이어를 적용  
노이즈가 없는 원래 데이터로 재구성
</code></pre></div></div>

<ul>
  <li>원리 : 노이즈에 강건한 잠재 표현(미세하게 변형된 데이터도 같은 잠재 벡터로 표현되도록)</li>
</ul>

<h2 id="활용">활용</h2>

<ol>
  <li>특징 추출기
    <ul>
      <li>잠재 벡터로부터 분류, 클러스터링 문제 해결</li>
    </ul>
  </li>
  <li>이상치 탐지(Anomaly Detection)
    <ul>
      <li>이상치는 재구성 했을 때 평균제곱오차가 크게 나올 것!</li>
      <li>특정 임계값을 넘으면 이상치로 판단</li>
    </ul>
  </li>
</ol>

<h1 id="변분-오토-인코더variation-autoencoder-vae">변분 오토 인코더(Variation Autoencoder, VAE)</h1>

<p>오토 인코더와 동일한 구조(Encoder + Decoder)를 가지는 생성 모델<br />
잠재 변수 모델 : 데이터는 저차원의 잠재 변수로부터 생성됨</p>

<ul>
  <li>잠재 벡터의 분포 : 표준정규분포</li>
  <li>장점
    <ol>
      <li>q(z|x)로부터 데이터를 요약하는 유용한 잠재 표현을 찾을 수 있음</li>
    </ol>
  </li>
  <li>단점
    <ol>
      <li>가능도가 아닌 가능도의 하한을 최대화</li>
      <li>흐릿한 이미지를 생성</li>
    </ol>
  </li>
</ul>

<h2 id="evidence-of-lower-boundelbo">Evidence of Lower BOund(ELBO)</h2>

<p>현재 모델이 우리가 가진 현상을 얼마나 잘 설명하는가 = 가능도(Likelihood)<br />
직접 계산이 어려우니, 간점적으로 계산하여 최대화함</p>

<h1 id="vqvaevector-quantized-variational-autoencoder">VQVAE(Vector Quantized Variational Autoencoder)</h1>

<p>유한한 잠재 표현을 활용하는 변분 오토 인코더</p>

<ul>
  <li>이산(Discrete) 잠재 변수
    <ul>
      <li>범주 : K개의 D차원 임베딩(Embedding) 벡터</li>
    </ul>
  </li>
</ul>

<h1 id="적대적-생성-신경망-generative-adverarial-networks-gans">적대적 생성 신경망 (Generative Adverarial Networks, GANs)</h1>

<p>적대적으로 학습하는 신경망들로 구성되며, 생성 모댈로써 활용함</p>

<ul>
  <li>생성된 데이터와 실제 데이터를 판별하고 속이는 과정을 거치며 생성 모델을 개선</li>
  <li>데이터를 생성하는 생성 모델과 데이터의 진위를 구별하는 판별 모델(Discriminator)로 구성
    <blockquote>
      <p>판별 모델 : 생성된 데이터를 입력으로 받아 실제 데이터인지 생성된 데이터인지를 출력</p>
    </blockquote>
  </li>
  <li>훈련 과정
    <ol>
      <li>임의의 초기 분포로부터 생성 모델이 데이터를 생성</li>
      <li>판별 모델이 분류; 판별 모델 갱신</li>
      <li>갱신된 판별 모델을 고정;생성 모델 갱신</li>
      <li>반복 과정을 거쳐 생성 모델은 판별 모델이 구별할 수 없는 수준의 데이터를 생성</li>
    </ol>
  </li>
  <li>목적 함수
    <ol>
      <li>생성 모델 : 실제와 유사한 데이터를 생성하여 판별자를 속여야 함(다음 목적 함수를 최소화함)</li>
      <li>판별 모델 : 실제와 생성된 데이터를 정확하게 구별해야 함(다음 목적 함수를 최대화함)</li>
    </ol>
  </li>
</ul>

<h1 id="조건부-생성-모델">조건부 생성 모델</h1>

<ul>
  <li>필요성 : 다양한 활용을 위해 생성 데이터의 의미 제어 방법이 필요함</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 조건을 입력 받아 원하는 의미를 갖는 데이터를 생성하는 생성 모델
- 범주(카테고리)부터 영상의 전체 구조(레이아웃)에 이르기까지 다양한 입력을 조건으로 받음
- 높은 다양성과 품질을 동시에 누릴 수 있으나 수집하기 더 까다로운 데이터를 필요로함
</code></pre></div></div>

<h3 id="이미지-대-이미지-image-to-image-translation">이미지 대 이미지 (Image-to-Image Translation)</h3>

<ol>
  <li><strong>Pix2pix</strong>
    <ul>
      <li>조건부 GAN (cGAN)을 기반으로 한 모델</li>
      <li>입력 이미지를 다른 스타일이나 도메인으로 변환</li>
      <li>지도 학습 방식으로 특정 입력-출력 쌍을 필요로 함</li>
      <li>예: 스케치를 컬러 이미지로 변환</li>
    </ul>
  </li>
  <li><strong>CycleGAN</strong>
    <ul>
      <li>지도 학습 없이 이미지 스타일 변환을 수행</li>
      <li>두 개의 생성자와 판별자를 사용하여 도메인 간 변환을 학습</li>
      <li>예: 말 ↔ 얼룩말, 여름 ↔ 겨울</li>
    </ul>
  </li>
  <li><strong>BiCycleGAN</strong>
    <ul>
      <li>Pix2pix 모델의 확장</li>
      <li>하나의 입력에 대해 다양한 출력을 생성할 수 있도록 개선</li>
      <li>잠재 공간에서 샘플링을 통해 다중 모드 출력 가능</li>
    </ul>
  </li>
  <li><strong>StarGAN</strong>
    <ul>
      <li>하나의 모델로 여러 개의 도메인 변환을 가능하게 함</li>
      <li>도메인 간 이미지 변환을 하나의 네트워크에서 수행</li>
      <li>예: 얼굴 사진에서 성별, 나이, 감정 변화 적용</li>
    </ul>
  </li>
  <li><strong>InstaGAN</strong>
    <ul>
      <li>개별 객체 단위의 변환이 가능한 모델</li>
      <li>배경과 객체를 분리하여 변환 적용</li>
      <li>예: 개별적인 얼굴 스타일 변경</li>
    </ul>
  </li>
  <li><strong>LostGAN</strong>
    <ul>
      <li>객체 수준의 이미지 변환 및 생성에 특화</li>
      <li>레이아웃 정보를 사용하여 장면을 구성</li>
      <li>예: 주어진 레이아웃에 따른 장면 이미지 생성</li>
    </ul>
  </li>
  <li><strong>SPADE (Spatially-Adaptive Denormalization)</strong>
    <ul>
      <li>레이아웃 정보를 반영한 고품질 이미지 생성</li>
      <li>세그먼트 맵을 활용하여 이미지 스타일 변환</li>
      <li>예: 스케치를 사실적인 장면으로 변환</li>
    </ul>
  </li>
</ol>

<hr />

<h3 id="텍스트-대-이미지-text-to-image-generation">텍스트 대 이미지 (Text-to-Image Generation)</h3>

<ol>
  <li><strong>GAN-CLS (GAN with Conditional Latent Space)</strong>
    <ul>
      <li>텍스트 설명을 조건으로 하는 이미지 생성 모델</li>
      <li>텍스트 임베딩을 GAN에 입력하여 관련된 이미지 생성</li>
      <li>예: “붉은 꽃이 핀 초록색 들판” → 해당 이미지 생성</li>
    </ul>
  </li>
  <li><strong>GigaGAN</strong>
    <ul>
      <li>초고해상도(High-Resolution) 텍스트 기반 이미지 생성 모델</li>
      <li>기존 GAN 기반 텍스트-이미지 모델보다 더 높은 품질과 해상도 제공</li>
      <li>예: 텍스트 입력만으로 사실적인 고해상도 이미지 생성</li>
    </ul>
  </li>
</ol>

<h1 id="확산-모델">확산 모델</h1>

<h2 id="확산-확률-모델diffusion-probabilistic-model-dpm">확산 확률 모델(Diffusion Probabilistic Model, DPM)</h2>

<ul>
  <li>DPM - 확률
    <ol>
      <li>확산 현상을 시간에 따라 확률적 모델링</li>
      <li>마르코프 체인 : 미래는 과거가 아닌 현재에만 의존</li>
    </ol>
  </li>
  <li>DPM 구조
    <ol>
      <li>정방향 확산 : 데이터 -&gt; 노이즈 ( 고정 )</li>
      <li>역방향 확산 : 데이터 &lt;- 노이즈 ( 학습 )
        <ul>
          <li>이미지 생성 과정 = 노이즈를 제거하는 과정은 계산 불가</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>VAE 와 다른 점
    <ol>
      <li>잠재 변수의 차원이 모두 데이터의 차원과 동일 + 여러 단계의 잠재 변수를 가짐</li>
      <li>디코더를 모든 시점에서 공유 + 인코더는 학습되지 않음</li>
    </ol>
  </li>
</ul>

<h2 id="디노이징-확산-확률-모델ddpm">디노이징 확산 확률 모델(DDPM)</h2>

<p>손실 함수를 간단한 형태로 정리함</p>

<ul>
  <li>구조
    <ol>
      <li>노이즈 예측 : U-net 구조</li>
      <li>t 시점 주입 : 사인 곡선적 포지션 임베딩(Sinusoidal Position Embedding)</li>
    </ol>
  </li>
  <li>생성 과정
    <ol>
      <li>노이즈 \(X_T\)를 표준정규분포에서 샘플링
        <ul>
          <li>t가 클 때 : 핵심적인 특징을 담고 있음</li>
          <li>t가 작을 때 : 세부적인 특징을 담고 있음</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>한계점
    <ol>
      <li>느린 생성 과정 : 5만개의 32x32 크기 이미지 생성 위해 20시간 필요</li>
      <li>조건부 생성 불가
        <ul>
          <li>DDPM은 Unconditional 모델 ( 조건 없는 모델 )</li>
          <li>품질 - 다양성 조절 불가</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h2 id="생성-과정-가속화">생성 과정 가속화</h2>

<h3 id="ddimdenoising-diffusion-implicit-model">DDIM(Denoising Diffusion Implicit Model)</h3>

<p>마르코프 체인 : 미래는 과거가 아닌 현재에만 의존함</p>

<ul>
  <li>샘플링 과정에서 마르코프 체인이 등장함
    <blockquote>
      <p>잡음이 조금 덜 낀 샘플을 더 심한 샘플과 모델이 이를 통해 구한 노이즈로 추정</p>
    </blockquote>
  </li>
  <li>DDPM 에서 DDIM으로
    <ol>
      <li>마르코프 체인을 가정하지 않는 확산 확률 모델을 정의 -&gt; 직전 시점에만 의존하지는 않음</li>
      <li>학습된 모델을 가지고 일부 시점만 거쳐 데이터 생성 가능 -&gt; 생성 속도 개선</li>
      <li>학습은 DDPM처럼 그러나 빠른 sampling을 원함(DDPM이 활용하는 중요 특성을 만족해야함)</li>
    </ol>
  </li>
  <li>생성과정
    <ol>
      <li>특성을 만족하는 정방향 함수를 정의함</li>
      <li>생성 시 노이즈를 추가하지 않는 방법 제안</li>
      <li>랜덤성은 오직 표준정규분포를 따르는 노이즈 \(X_T\) 에만 존재</li>
    </ol>
  </li>
</ul>

<h3 id="denoising-diffusion-gans">Denoising Diffusion GANS</h3>

<p>적은 시점( T &lt;= 8 )을 사용하기 위해 조건부 GAN을 활용하여 복잡한 디노이징 분포를 학습</p>

<h3 id="progressive-distillation">Progressive Distillation</h3>

<p>학습된 확산 확률 모델로부터 시점을 절반으로 줄이는 경량화된 모델을 반복적으로 학습<br />
이웃한 2개의 역방향 확산 과정을 하나로 합침</p>

<h3 id="consistency-model-and-distillation">Consistency Model and Distillation</h3>

<p>학습 시 모든 Time Step에 대해 동일한 결과를 내도록 학습을 진행함</p>

<h3 id="latent-consistency-model">Latent Consistency Model</h3>

<p>1 step to 8 step</p>

<h2 id="조건부-생성">조건부 생성</h2>

<ul>
  <li>조건부 DPM
    <ol>
      <li>정방향 확산은 고정</li>
      <li>역방향 확산(생성 과정)에서 조건 추가</li>
    </ol>
  </li>
</ul>

<h3 id="guided-diffusion">Guided Diffusion</h3>

<p>스케일을 조절하면 클래스를 유지하도록 생성 가능</p>

<ul>
  <li>단점 : 분류기를 추가 학습해야 함</li>
</ul>

<h3 id="classifier-free-diffusion-guidance">Classifier-free Diffusion Guidance</h3>

<p>분류기를 베이즈 정리를 활용하여 대체함<br />
확산 확률 모델과 조건부 확산 확률 모델을 함께 학습함(학습할 때 랜덤하게 클래스를 버림)<br />
생성할 때 두 모델에서 예측한 노이즈의 가중합 계산</p>

<h2 id="잠재-확산-모델">잠재 확산 모델</h2>

<h3 id="픽셀-공간에서-잠재-공간으로">픽셀 공간에서 잠재 공간으로</h3>

<ol>
  <li>기존의 확산 모델들은 고차원 이미지 공간에서 연산을 반복함</li>
  <li>이미지의 정보를 유지한 채로, 차원을 축소할 수 있다면 계산 복잡도를 감소시킬 수 있음</li>
  <li>영상 생성은 인지적 압축 과정과 의미적 압축으로 대략적 구분이 가능함
    <ul>
      <li>인지적 압축 : 과도한 세부 사항을 제거하며 핵심적인 특징을 잠재 표현으로 요약</li>
      <li>의미적 압축 : 데이터의 의미적, 구조적 특징을 학습</li>
    </ul>
  </li>
</ol>

<h3 id="잠재-확산-모델-1">잠재 확산 모델</h3>

<p>실제 이미지의 고차원 공간이 아닌 잠재 공간에서 노이즈 연산을 반복하도록 설계</p>

<ul>
  <li>확산 모델에서 주요한 연산들이 모두 잠재 공간에서 이루어지므로 훨씬 더 효율적인 공간에서 훈련 가능
    <ul>
      <li>기존 확산 모델보다 계산, 시간 자원 모두 효율적</li>
    </ul>
  </li>
  <li>사전 훈련 : 벡터 양자화 변분 오토 인코더
    <ul>
      <li>재건된 이미지를 가짜 이미지로 두어 실제 이미지와 판별하는 과정 포함</li>
    </ul>
  </li>
</ul>

<h2 id="조건부-생성-1">조건부 생성</h2>

<h3 id="text-to-image">Text-to-Image</h3>

<ul>
  <li>잠재 확산 모델은 확산 모델과 마찬가지로 다양한 조건부 생성이 가능(Ex. Text-to-Image, 초고해상도 이미징, 인페이팅)</li>
  <li>텍스트 입력이 추가되는 경우, 텍스트 인코더를 사전 훈련하는 단계가 필요함</li>
  <li>CLIP은 텍스트와 이미지 간의 대조 학습을 통해 훈련
    <ul>
      <li>텍스트-이미지 쌍에 대한 이해가 높은 모델</li>
    </ul>
  </li>
  <li>확산 모델과의 차이
    <ol>
      <li>텍스트 정보와 노이즈화된 잠재 표현의 관계성 학습을 위해 Cross-attention을 추가</li>
    </ol>
  </li>
  <li>Layout-to-Image</li>
  <li>Masked-to-Image</li>
</ul>

<h3 id="latent-diffusion-modelldm">Latent Diffusion Model(LDM)</h3>

<ul>
  <li>입력과 출력을 잠재 공간으로 매핑 후 학습해, 높은 품질의 영상을 효율성 높게 학습, 생성함</li>
  <li>일반 생성 외에도 텍스트, Layout, 가려진 이미지 등 다양한 조건 기반의 생성이 가능함</li>
  <li>현재 가장 활발히 활용, 연구되는 모델</li>
</ul>

<h1 id="확산-모델의-개인화">확산 모델의 개인화</h1>

<p>우리가 원하는 대상에 관한 사진이 일부 있을 때 피사체에 대한 새로운 이미지를 텍스트로부터 생성하는 방법</p>

<h2 id="텍스트-반전">텍스트 반전</h2>

<ul>
  <li>학습 데이터 : 우리가 원하는 대상에 관한 이미지 3~5개</li>
  <li>대상을 표현하는 새로운 단어를 임베딩 공간에서 찾기</li>
</ul>

<h2 id="dreambooth">DreamBooth</h2>

<p>사전학습된 text-to-image 확산 모델 중 U-net을 미세 조정</p>

<ul>
  <li>파인튜닝의 문제점
    <ol>
      <li>언어 드리프트 : 대상과 동일한 클래스의 이미지를 생성하는 방법을 천천히 잊어버림</li>
      <li>다양성 : 포즈와 각도 등이 새로운 학습 이미지에 과적합될 수 있음</li>
    </ol>
  </li>
</ul>

<h2 id="lora">LoRA</h2>

<p>기존 가중치는 유지하고 가중치 변화량을 학습함 -&gt; 기존 학습 결과를 잊지 않는 효과<br />
가중치 변화량을 행렬 2개의 곱으로 분해 -&gt; 학습 파라미터 수가 대폭 감소</p>

<h2 id="controlnet">ControlNet</h2>

<p>다양한 Structural input을 조건으로 주기 위한 방법<br />
추가적인 layer를 학습해, U-net은 유지하고 차이를 추가하는 방법</p>

<h2 id="face0">Face0</h2>

<p>사람 얼굴을 조건삼아, 동일한 얼굴의 다양한 결과를 생성하는 기법<br />
얼굴에서 특징을 추출, 텍스트 표현과 함께 모델에 제공함</p>

<h2 id="ip-adapter">IP-Adapter</h2>

<p>Text뿐만 아니라 이미지 feature를 함께 입력받아 그에 맞게 적절히 생성해낼 수 있게 함<br />
별도의 cross-attention layer를 두어 따로 학습함</p>

<h2 id="imagic">Imagic</h2>

<p>텍스트만을 활용한 의미적 편집 수행</p>

  </div>
  <div class="md-index">
    <h2>목차</h2>
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#생성-모델-generative-models">생성 모델 (Generative Models)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#특징">특징</a></li>
<li class="toc-entry toc-h3"><a href="#최대-가능도-추정법-maximum-likelihood-estimation-mle">최대 가능도 추정법 (Maximum Likelihood Estimation, MLE)</a></li>
<li class="toc-entry toc-h3"><a href="#평가-지표">평가 지표</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#판별-모델-discriminative-model">판별 모델 (Discriminative Model)</a></li>
<li class="toc-entry toc-h2"><a href="#생성-모델-vs-판별-모델">생성 모델 vs 판별 모델</a></li>
<li class="toc-entry toc-h2"><a href="#손실함수">손실함수</a></li>
<li class="toc-entry toc-h2"><a href="#디노이징denoising-오토-인코더">디노이징(Denoising) 오토 인코더</a></li>
<li class="toc-entry toc-h2"><a href="#활용">활용</a></li>
<li class="toc-entry toc-h2"><a href="#evidence-of-lower-boundelbo">Evidence of Lower BOund(ELBO)</a>
<ul>
<li class="toc-entry toc-h3"><a href="#이미지-대-이미지-image-to-image-translation">이미지 대 이미지 (Image-to-Image Translation)</a></li>
<li class="toc-entry toc-h3"><a href="#텍스트-대-이미지-text-to-image-generation">텍스트 대 이미지 (Text-to-Image Generation)</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#확산-확률-모델diffusion-probabilistic-model-dpm">확산 확률 모델(Diffusion Probabilistic Model, DPM)</a></li>
<li class="toc-entry toc-h2"><a href="#디노이징-확산-확률-모델ddpm">디노이징 확산 확률 모델(DDPM)</a></li>
<li class="toc-entry toc-h2"><a href="#생성-과정-가속화">생성 과정 가속화</a>
<ul>
<li class="toc-entry toc-h3"><a href="#ddimdenoising-diffusion-implicit-model">DDIM(Denoising Diffusion Implicit Model)</a></li>
<li class="toc-entry toc-h3"><a href="#denoising-diffusion-gans">Denoising Diffusion GANS</a></li>
<li class="toc-entry toc-h3"><a href="#progressive-distillation">Progressive Distillation</a></li>
<li class="toc-entry toc-h3"><a href="#consistency-model-and-distillation">Consistency Model and Distillation</a></li>
<li class="toc-entry toc-h3"><a href="#latent-consistency-model">Latent Consistency Model</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#조건부-생성">조건부 생성</a>
<ul>
<li class="toc-entry toc-h3"><a href="#guided-diffusion">Guided Diffusion</a></li>
<li class="toc-entry toc-h3"><a href="#classifier-free-diffusion-guidance">Classifier-free Diffusion Guidance</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#잠재-확산-모델">잠재 확산 모델</a>
<ul>
<li class="toc-entry toc-h3"><a href="#픽셀-공간에서-잠재-공간으로">픽셀 공간에서 잠재 공간으로</a></li>
<li class="toc-entry toc-h3"><a href="#잠재-확산-모델-1">잠재 확산 모델</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#조건부-생성-1">조건부 생성</a>
<ul>
<li class="toc-entry toc-h3"><a href="#text-to-image">Text-to-Image</a></li>
<li class="toc-entry toc-h3"><a href="#latent-diffusion-modelldm">Latent Diffusion Model(LDM)</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#텍스트-반전">텍스트 반전</a></li>
<li class="toc-entry toc-h2"><a href="#dreambooth">DreamBooth</a></li>
<li class="toc-entry toc-h2"><a href="#lora">LoRA</a></li>
<li class="toc-entry toc-h2"><a href="#controlnet">ControlNet</a></li>
<li class="toc-entry toc-h2"><a href="#face0">Face0</a></li>
<li class="toc-entry toc-h2"><a href="#ip-adapter">IP-Adapter</a></li>
<li class="toc-entry toc-h2"><a href="#imagic">Imagic</a></li>
</ul>
    <hr>
    <h2>관련 POST</h2>
    
      
        
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <p>
            <a href="/2025/03/10/Natural_Language_Processing-04.html">Natural Language Processing - 04 [ 2025년03월10일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/03/07/Natural_Language_Processing-03.html">Natural Language Processing - 03 [ 2025년03월07일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/03/06/Natural_Language_Processing-02.html">Natural Language Processing - 02 [ 2025년03월06일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/03/05/Natural_Language_Processing-01.html">Natural Language Processing - 01 [ 2025년03월05일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/02/27/generation.html">생성 모델과 판별 모델 [ 2025년02월27일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/02/14/Computer-Vision-02.html">Computer Vision - 02 [ 2025년02월14일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/02/11/Computer-Vision-01.html">Computer Vision - 01 [ 2025년02월11일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/02/08/Pytorch.html">Pytorch [ 2025년02월08일 ]</a>
          </p>
        
      
        
      
    
      
        
          <p>
            <a href="/2025/02/07/DEEPLEARNING-BASIC-01.html">DEEPLEARNING BASIC [ 2025년02월07일 ]</a>
          </p>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
        
      
    
      
        
      
    
      
        
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
        
      
    
      
        
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
      
        
      
        
      
    
  </div>
</div>

<script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 56,
  backgroundColor: 'rgb(135, 206, 250)',
  textColor: '#fff'
})</script>

<!-- Latex -->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

		</div>
	</div>
	<div class="footer">
  <p>Email : parkinsu9701@gmail.com</p>
  <a href="https://github.com/insu97">SITE : GITHUB_PAGE</a>
</div>
</body>

</html>
<script src="/assets/js/music-controls.js" defer></script>