[
  
  
    {
      "title": "Runpod",
      "tags": "RUNPOD",
      "desc": "Runpod - 2025년07월25일  tag : RUNPOD|",
      "content": "Runpod - 2025년07월25일  tag : RUNPOD|ERRORrunpod에 vscode로 접속 시install terminal quit with output: 프로세스에서 없는 파이프에 쓰려고 했습니다.  해결책 : C:\\Users\\{사용자이름}\\.ssh 접속 후 known_hosts 파일 열어서 전부 삭제OthersRUNPOD files → Local Downloads# powershellscp -r -P {포트번호} `  -i \"C:\\Users\\YourUser\\.ssh\\id_ed25519\" `  \"root@{서버주소}:{원격폴더경로}\" `  \"{로컬 저장 경로}\"",
      "url": "/2025/07/25/RUNPOD.html"
    },
  
    {
      "title": "Python",
      "tags": "PYTHON",
      "desc": "Python - 2025년07월25일  tag : PYTHON|",
      "content": "Python - 2025년07월25일  tag : PYTHON|ERRORRAGReranker  코드from langchain_community.cross_encoders import HuggingFaceCrossEncoderhf_reranker = HuggingFaceCrossEncoder(    model_name=\"BAAI/bge-reranker-v2-m3\")  error 문구Error rendering output item using 'jupyter-ipywidget-renderer'Cannot read properties of undefined (reading 'ipywidgetsKernel')  해결책pip install ipywidgets --upgradejupyter nbextension enable --py widgetsnbextension --sys-prefixjupyter nbextension install --py widgetsnbextension --sys-prefixOthers라이브러리 관리  방법1pip freeze &gt; requirements.txtpip install -r requirements.txt  방법2pip install pipreqspipreqs ./ --force# 폴더 내에서 환경이 존재하는 경우pipreqs . --force --ignore \"{가상환경}/Lib\"  방법3# 새로운 라이브러리를 설치 후 requirements.txt 에 적용할 때# Ex. cohere 설치했을경우pip freeze | grep cohere",
      "url": "/2025/07/25/PYTHON.html"
    },
  
    {
      "title": "Natural Language Processing - 04",
      "tags": "DL, NLP",
      "desc": "Natural Language Processing - 04 - 2025년03월10일  tag : DL|NLP|",
      "content": "Natural Language Processing - 04 - 2025년03월10일  tag : DL|NLP|딥러닝 기반 자연어처리  Sequence-to-sequence : 입력된 시퀀스(문장)을 다른 시퀀스로 변환하는 모델로, 인코더 RNN과 디코더 RNN로 구성  Encoder : 입력 시퀀스를 받아들여 고정된 길이의 벡터로 변환함.  Decoder : 문맥 벡털를 받아들여 출력 시퀀스를 생성RNN ( Recurrent Neural Networks )  장점          모든 길이의 시퀀스를 입력으로 처리 가능      시간에 따라 가중치를 공유하여, 입력 시퀀스가 길어져도 모델 크기가 증가하지 않음      과거 정보를 고려하여 다음 시간의 출력을 계산함        단점          매번 시간에 따라 출력을 계산하므로, 병렬 처리가 불가능하여 계산 속도가 느림      입력 혹은 출력 시퀀스가 길어지면 오래전 정보를 반영하기 어려움(Long-term dependecy)      현재 상태에 대한 미래 입력을 고려할 수 없음        Gradient vanishing / exploding          기존 RNN의 역전파 과정에서 그래디언트가 너무 작아져서(Gradient vanishing) 가중치 업데이트가 잘 안 되거나,그래디언트가 너무 커져서(Gradient exploding) 가중치 값이 엄청나게 커지는 문제가 발생      LSTM (Long Short-Term Memory)RNN에서 발생하는 Long-term dependecy problem 완화 방법으로 LSTM은 cell state와 gate라는 메커니즘을 도입  Forget gate  Input gate  Cell state  Output gateGRU (Gated Recurrent Unit)LSTM의 변형으로, 더 간단한 구조를 가짐Attention문맥에 따라 집중할 단어를 결정하는 방식으로 문맥을 최대한 고려할 수 있는 핵심 방식Transformer  Encoder의 구성요소          Self-Attention      Layer Normalization : 각 layer의 출력을 정규화하는 방법      Skip Connection      Feed-Forward Networks : 각 위치에서 독립적으로 동일하게 적용되는 두 개의 선형 변환으로 구성              이는 모델이 각 단어를 독립적으로 처리하면서도, 전체 문장의 문맥을 고려할 수 있게 함                  Multi-Head Attention                      Input Embeddings : 입력 데이터를 고차원 벡터로 변환하는 과정을 의미Positional Encoding : 단어의 위치 정보를 포함할 수 있음  Decoder의 구성요소          Auto-regressive      Teacher Forcing      Masked Self-Attention      Encoder-Decoder Attention : Decoder가 출력 시퀀스의 각 요소를 생성할 때, 입력 시퀀스의 모든 요소를 고려할 수 있게 함        Causal Attention : 입력된 토큰까지의 정보만 Attention에 반영하고, 미래의 입력은 반영하지 않는 것",
      "url": "/2025/03/10/Natural_Language_Processing-04.html"
    },
  
    {
      "title": "Natural Language Processing - 03",
      "tags": "DL, NLP",
      "desc": "Natural Language Processing - 03 - 2025년03월07일  tag : DL|NLP|",
      "content": "Natural Language Processing - 03 - 2025년03월07일  tag : DL|NLP|자연어처리의 역사규칙기반 및 통계기반 자연어처리  규칙 기반 NLP          Rule에 맞게 처리하는 시스템      Rule 생성을 위해서는 Task에 대한 전문 지식 필요      적은 양의 데이터로 일반화 기능      결론 도출의 논리적 추론 가능      학습에 필요한 데이터가 비교적 적게 필요      이를 제작한 전문가의 실력을 넘어서기 매우 어려움      해당 전문가의 오류를 동일하게 반복      규칙 구축에 많은 시간과 비용 소요      Toy task에 주로 적용되었음        통계 기반 NLP          대량의 텍스트 데이터로 통계를 내어 단어를 표현      “모두 (군중, 여러분)”가 “무의식적”으로 생산한 대량의 데이터(=빅데이터)를 활용      통계적 언어모델(SLM)                  이전 단어들로부터 다음 단어에 대한 확률을 구함 (확률기반)                      비교            Rule Based NLP      Statistical NLP                  Flexible      Easy to scale              Easy to debug      Learn by itself              Doesn’t require much training      Fast development              High precision      High coverage      기계학습 및 딥러닝기반 자연어처리  ML &amp; DL in NLP          “전문가” + “모두(군중, 여러분)” 공존의 시대      학습에 사용할 데이터의 질이 좋고 양이 많으면 인간의 실력을 넘어설 수 있음      인간이 생각하지 못한 새로운 방법을 사용할 수 있음      Data hungry      결과에 대한 해석의 어려움      논리적 추론이 아닌 귀납적 근사에 의한 결론 생성      모델 : Neural Machine Translation      뉴럴심볼릭기반 자연어처리전문가의 데이터를 전면 활용사전에 구축된 상식 정보를 지식 그래프 형태로 구축하여 딥러닝 모델에 주입            Symbolic Approaches      Neural Models                  기호를 통해 개념을 정의하고 일정한 논리적 규칙에 따라 추론 기능      대량의 데이터를 이용해 다층 구조로 이루어진 인공신경망을 통해 귀납적 추론 가능              일반화 능력이 우수하며, 결론에 대한 설명이 가능한 방법      미분 가능한 방식으로 학습이 가능하며 높은 정확도를 나타냄              불안전한 KB에 의존하며 논리적 규칙에 의해 정의된 지식과 새롬게 생성되는 지식 간의 연결이 어려움      학습을 위해 대량의 데이터가 요구됨              미분 가능한 방식으로 학습이 어려움      학습 도메인에 귀속되어 전이 능력 부족 현상              Toy Task 위주의 적용이 불가피함      결론 도출에 대한 설명력 부족 &amp; 외부지식을 활용하기 어려움        KB : Knowledge Base  KGBERT  Common Sense Knowledge Graph          인간의 상식이나 지식에 기초해 작은 학습데이터로 많은 추론을 이끔      상식을 entity, relation을 활용한 지식 베이스 그래프 형태로 표현        Multi-hop Question Answering          질문과 함께 거대한 지식 코퍼스가 주어졌을 때 답을 찾기 위해 말뭉치에 다중추론 점프(홈)를 수행하여 질문에 답하는 것      Entity 중심의 relation graph를 활용해 Multi-hop reasoning이 필요한 Question에 대한 응답을 추출      Pretrain-Finetuning 기반 자연어처리  Language Model : 대중이 만든 데이터(pre-train) + 전문가가 만든 데이터(Fine-tune)  Pretraining : 내가 원하는 task 이외의 다른 task의 데이터를 이용하여 주어진 모델을 먼저 학습하는 과정  Finetuning : 사전학습된 모델을 원하는 task에 해당하는 데이터, 학습 방식으로 다시한번 재학습 시키는 과정  Language Model          Seq2Seq -&gt; Attention + Seq2Seq -&gt; Transformer -&gt; GPT-1 -&gt; BERT -&gt; GPT-2      XLNet -&gt; RoBERTa -&gt; MASS -&gt; BART -&gt; MT - DNN -&gt; T5      LLM기반 자연어처리데이터 양보다 모델 사이즈가 성능에 일관적으로 더 큰 영향을 미치며, 큰 모델이 좋음을 증명  Foundation Models  In-Context Few-Shot Learning &amp; Prompt Learning  예시          OpenAI의 GPT3      Google의 PaLM(Scaling Language Modeling with Pathways)      Meta의 LLaMA &amp; LLaMA2      Open AI의 DALL-e      Kakao Brain 의 KoGPT      Kakao Brain 의 Min DALL-E      Naver AI Lab 의 HyperCLOVA      LG AI Research 의 EXAONE        ChatGPT          Supervised Fine-tuning 과 Reinforce Learning with Human Feedback 으로 학습      사람의 지시에 잘 따르며 수많은 task들을 잘 수행하는, 사람이 원하는 방향의 응답을 생성하는 AI      ",
      "url": "/2025/03/07/Natural_Language_Processing-03.html"
    },
  
    {
      "title": "Natural Language Processing - 02",
      "tags": "DL, NLP",
      "desc": "Natural Language Processing - 02 - 2025년03월06일  tag : DL|NLP|",
      "content": "Natural Language Processing - 02 - 2025년03월06일  tag : DL|NLP|텍스트 전처리  방법          HTML 태그, 특수문자, 이모티콘정규표현식불용어 : 분석에 큰 의미가 없는 단어로 코퍼스 내에 빈번하게 등장하나, 실질적으로 의미를 갖고 있지 않은 용어어간추출  : 어형이 변형된 단어로부터 접사 등을 제거하고 그 단어의 어간을 분리해내는 것 &lt; 포터 스태머 알고리즘 &gt;표제어추출(Lemmatization) : 품사 정보가 보존된 형태의 기본형으로 변환        라이브러리 : KoNLPy, NLTK, SentencePiece토큰화(Tokenization)  주어진 데이터를 토큰이라 불리는 단위로 나누는 작업  토큰이 되는 기준은 다를 수 있음문장 토큰화문장 분리단어 토큰화구두점 분리, 단어 분리편집거리(Edit distance)  Levenshtein distance          한 string s1 을 s2로 변환하는 최소 횟수를 두 string 간의 거리. 거리가 낮을수록 유사한 문자열로 판단함      정규표현식(Regex)특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어고려사항  구두점이나 특수 문자를 단순 제외  줄임말과 단어 내 띄어쓰기  문장 토큰화 : 단순 마침표를 기준으로 자를 수 있음텍스트 정제코퍼스 내에서 토큰화 작업에 방해가 되거나 의미가 없는 부분의 텍스트, 노이즈를 제거하는 작업자연어처리의 다양한 응용시스템자연어이해 기반 하위분야  형태소 분석          어떠한 문자열이 주어졌을 때, 그 문자열을 이루고 있는 형태소를 비롯한 어근, 접두사, 접미사, 품사 등 다양한 언어적 속성의 구조를 파악하는 것        품사 태깅          형태소 분석을 한 결과의 각 형태소에 품사 태그를 할당하는 과정        형태소 분석기          HMM : Hidden Markov Model              통계적 마르코프 모델의 하나, 어떠한 결과를 야기하는 원인은 은닉 상태인 이전의 여러 연속된 사건들이라고 보는 모델      바로 직전의 단계에서만 직접적인 영향을 받고, 이전의 상태들은 연속적이며 내재적으로 담겨있음                  CRF : Conditional Random Field                    시퀀스 라벨링(어떠한 배열을 입력으로 받으면 그와 같은 길이의 결과 반환)에 많이 사용      특징 함수(Feature function)을 정의                  Charater-Lavel Bidirectional LSTM-CRF                    띄어쓰기 오류 등의 문제로 한국어 형태소를 처리할 때는 음절 단위를 입력으로 받아 형태소를 분석하는 모델이 좋은 성능을 보임        개체명 인식 - 의학분야 개체명 인식 시스템, 태깅 시스템, 한국어 NER 데이터셋  정보추출 : 비구조적인 triple를 추출하는 태스크          triple이란? 두 개체 간의 관계를 &lt;주어, 관계, 목적어&gt;으로 나타낸 구조              구조                  입력된 문서를 문장단위로 분할          각 문장을 토큰화          품사 태깅을 통해 각 단어의 품사를 파악          품사를 기준으로 엔티티를 추출          술어, 주어, 객체에 대한 관계파악을 위해 텍스트에서 서로 가까이 있는 엔티티쌍의 특정 패턴을 추출                      규칙 기반 접근 : 문장에서 문법적 속성에 대한 규칙 세트를 정의한 다음 규칙을 사용하여 정보를 추출  기계학습 기반 접근 : 다량의 데이터로부터 기계학습 알고리즘이 직접 패턴을 발견해 학습자연어생성 기반 하위분야  기계번역          규칙 기반 기계 번역      통계 기반 기계 번역      신경망 기반 기계 번역(NMT : Neural Machine Translation) : Sequence to Sequence        질의응답          질문처리 : 질문유형 분류 및 정답 유형 분류      문서처리 : 정답을 포함, 관련성이 높은 문서 혹은 문장을 검색      정답처리 : 검색된 문서 혹은 문장에서 정답 후보에 해당하는 개체, 어휘, 구 등을 추출                  IR + QA [ 정보검색 , 질의응답 ]          대화형 질의응답          Visual Question Answering          Large Vision-Language Model          New VQA Task                      대화 시스템          목적 지향 대화 시스템 : 특정한 목적 또는 작업을 수행하는 것이 목표                  파이프라인 방식          자연어 이해 : 도메인 확인, 의도 파악, 슬롯 채우기          대화 상태 추적(DST, Dialog State Tracking) : 발화자 의도, 목표와 요청을 정확하게 추적 하는 것          자연어 생성 : 발화정보로부터 자연어 문장 생성          음성 합성 : 자연어 문장의 음성 생성          종단 간 학습                    일상 대화 시스템                  검색 기반 방식          생성 기반 방식          검색-생성 혼합 방식                      문서 요약          추출 요약      추상적 요약      Multi documents summarization : 복수개의 문서를 요약하는 작업      Long documents summarization : 길이가 매우 긴 문서를 요약하는 작업으로 다양한 접근 방식      Unsupervised summarization : 상대적 중요도를 측정하는 중요도 점수를 기반으로 주어진 문장에서 중요한 부분을 추출      자연어처리의 특이한 분야  Hate Speech Detection : 인터넷 상에서 발생하는 혐오 발언 및 공격적 표현을 자동으로 탐지하고 분류하는 기술          대표 데이터셋 : HateXplain        Counter Speech Generation          혐오 및 허위정보가 내제, 외재된 대화 또는 문장들에 대해 모델이 신뢰성 있는 근거가 내포된 문장을 생성함으로써      적절하게 대응할 수 있도록 하는 Task      대표 방법론 : Author-Reviewer framework, Generate, Prune, Select      대표 데이터셋 : CONAN, ProsocialDialog        Sarcasm Detection : 텍스트 또는 음성 데이터에서 풍자적 의미나 반어법적 말을 감지하고 인식하는 Task          대표 데이터셋 : iSarcasm        Fake News Detection          인터넷 상에서 유표되는 정보 중에서, 사실과 다른 정보, 혹은 과장된 정보를 식별하고 분류하는 Task      대표 데이터셋 : LIAR        Fact Checking          미디어나 인터넷 상에서 유포되는 정보의 진실성을 확인하는 Task      대표 데이터셋 : FEVER        Machine Translation          WMT(Workshop on Machine Translation)      Quality Estimation : 기계 번역된 문장이 얼마나 잘 번역을 하고있는지의 품질을 예측하는 Task                  대표 데이터셋 : QUAK                    Automatic Post Editing : 기계 번역의 출력물에서 번역 오류, 문법적 오류 등을 자동으로 수정하는 Task                  대표 데이터셋 : SubEdits                    Word-Level AutoCompletion : 소스 문장, 번역 컨텍스트 및 사람이 입력한 문자 시퀀스가 주어지면 대상 단어를 예측하는 Task      Chat Translation : 채팅, 일상대화 분야의 구어체에 대해 기계번역을 수행하는 Task        Dialogue          Persona-grounded Dialogue : 개별 사용자가 갖는 여러 개인적 특성을 고려해 personalized 된 대화를 생성하는 Task                  대표 데이터셋 : PersonaChat, BSBT, FoCus                    Persuasive Dialogue : 상대방을 설득하기 위한 목적의 대화, 모델이 상대방을 설득하고 자신의 주장을 전달하기 위해 응답 발화를 생성하는 Task                  대표 데이터셋 : Persuasion for Good                    Dialogue Summarization : 대화 기록이나 대화 데이터를 기반으로 중심 정보들을 재구성하여 요약하는 Task                  대표 데이터셋 : DialogSum &amp; SAMSum                    Knowledge-grounded Dialogue                  대화 시 외부정보가 필요한 경우, Pre-train model 외에 외부 지식을 별도로 활용하여          자연스럽고 전문적인 정보를 제공할 수 있는 대화를 생성하는 Task          대표 데이터셋 : Wizard Of Wikipedia &amp; Wizard Of Internet                      기타          Question Generation : 주어진 지문으로 부터 도출될 수 있는 질문들을 생성하는 Task                  대표 데이터셋 : FairytaleQA                    Document-level Relation Extraction : 문서 전체에서 개체에 대한 속성과 관계를 예측하는 Task                  대표 데이터셋 : DocRED                    Instruction Tuning : 사람이 원하는 방식의 대답을 이끌어내기 위한 instruction을 통해 대규모 언어 모델(LLM)을 미세 조정하는 데 사용되는 방법                  대표 데이터셋 : Super Natural Instructions          대표 방법론 : InstructGPT, Alpaca                    LLM Evaluation : LLM의 유창성, 일관성, 관련성, 정확성 등 모델 성능의 다양한 측면을 평가해 동작에 대한 인사이트를 얻고 개선점을 파악하고자하는 분야      Huggingface Open LLM : 사용자가 다양한 작업에서 다양한 대규모 언어 모델의 성능을 평가하고 비교할 수 있도록 해주는 Huggingface Platform                  대표 데이터셋 : AI2 Reasoning Challenge, HellaSwag, MMLU, TruthfulQA                      한국어 관련 Task          고전어 데이터셋      케어콜 데이터셋      혐오 발언 탐지 데이터셋      쓰기 평가 데이터셋      문법 교정 데이터셋      ",
      "url": "/2025/03/06/Natural_Language_Processing-02.html"
    },
  
    {
      "title": "Natural Language Processing - 01",
      "tags": "DL, NLP",
      "desc": "Natural Language Processing - 01 - 2025년03월05일  tag : DL|NLP|",
      "content": "Natural Language Processing - 01 - 2025년03월05일  tag : DL|NLP|개요  자연어          사람들이 일상생활에서 자연스럼게 사용하는 언어        자연어 처리          컴퓨터가 자연어의 의미를 분석하여 이해하고 생성할 수 있도록 만들어주는 기술        자연어 처리가 어려운 이유          표현의 중의성      고유 명사 처리      사전 미등록어 처리      문맥에 따른 모호성      규칙의 예외성        한국어에서 자연어 처리가 더 어려운 이유          교착어 : 어간에 접사가 붙어 단어를 이루고 의미와 문법적 기능이 정해짐      단어 순서 및 주어 생략      띄어쓰기      평서문과 의문문        활용          문법 교정      음성 인식      기계 번역      Real-Time Translation      정보 추출 - 검색      질의응답      문서 요약      AI Chat-Bot      AI X Creation      자동완성      검색엔진      정보 요약      언어학의 하위분야언어의 구성요소  언어의 구성요소          형태 : 실체인 의미를 물리적으로 표현할 수 있는 방법              음운론 : 말소리 연구      형태론 : 형태소, 단어 연구      통사론 : 문장 연구                  내용 : 언어가 의미하는 실제의미                    의미론 : 단어, 문장의 의미 연구                  사용 : 언어를 사용하는 상황                    화용론 : 상황에 따라 달라지는 단어나 문장의 의미 연구      형태론  형태론          언어에서 의미를 갖는 가장 기본단위인 형태소를 분석      형태소 간의 상관관계를 규명하는 학문        형태소          의미를 갖는 언어 단위 중 가장 작은 단위      의미 혹은 문법적 기능의 최소단위        이형태 : 한 형태소에 대한 여러 개의 변이 형태를 가질 수 있음통사론단어가 결합하여 구와 문장을 형성하는 규칙/방법을 연구하는 학문  문법 규칙          언어의 올바른 어순을 결정함      단어 그룹의 의미와 단어의 배열사이의 관계를 정의함      주저와 목적어 같은 문장 내의 문법적 관계를 명시함      문장이나 구문이 난해할 때 단어의 결합이 의미와 어떻게 연관되는지를 설명함        심층구조 : 화자가 문장에 대해 갖는 추상적인 정보를 담은 구조      표층구조 : 실생활에서 사용하는 단어들의 규칙적인 구조    구조적 모호성          두개의 서로 다른 심층 구조      구조적 모호성이 있는 표층구조        반복          문법규칙은 반복이라는 중요한 속성을 가짐      문장 속으로 다른 문장을 넣을 수 있음        구 구조규칙          특정 구의 구조가 하나 또는 특정된 순서로 늘어선 여러 개의 구성요소들로 이루어진다는 점을 표현        어휘규칙          구 구조규칙은 구조를 생성함      어휘를 구 구조규칙의 기본 단위인 품사기호로 변환하는 규칙이 필요함      의미론의미론은 단어, 구, 그리고 문장의 의미를 연구하는 분야  개념적 의미 : 단어가 사용될 때 전달되는 기본적, 본질적 의미 성분      연상적 의미 : 연상 또는 함축    의미자질(semantic features)          단어의 의미를 자질들의 나열로 표현하는 방법      단어의 의미를 차별화하기 위한 기본적인 구성요소      단어 의미의 일부분을 자질의 이름과 (+), (-) 기호를 조합하여 표현        의미역(semantic roles)          개별 단어가 가지는 의미자질 분석 이외에, 문장에서 각 단어의 의미적 역할을 분석행위자 : 특정 행위를 하는 주체대상자 : 특정 행위에 포함되거나 영향을 받는 개체        관계          동의 관계 vs 반의 관계      상하 관계      동음이철어 : 서로 다른 단어가 동일한 발음을 가지는 경우      동음이의어 : 동일한 형태(발음)의 단어가 전혀 관련성이 없는 서로 다른 의미를 가지는 경우      다의어      연어(collocation) : 문장, 문서에서 두 단어가 같이 출현하는 경우가 많은 경우      화용론  “보이지 않는” 의미 또는 실제로 말하거나 쓰지 않았을지라도 화자가 의미하는 바에 대한 연구  화용 원칙          대화할 때 “보이지 않는” 의미를 인식하기 위하여, 화자는 반드시 다수의 가정과 기대에 의지하여야 함        문맥          물리적인 문맥 vs 언어적인 문맥        직시 표현 : 화자의 문잭(특히 물리적 문맥)을 알아야 해석할 수 있는 표현  지시 : 화자가 청자로 하여금 무언가를 알아채도록 언어를 사용하는 하나의 행위  추론 : 발화된 내용과 그것의 의미를 연결시키기 위해 청자가 부가적인 정보를 이용하여 해석하는 과정  대용어 : 이미 소개된 실체에 뒤따르는 지시를 대용어라고 함  전제 : 화자가 가정하는 것이 진리이거나 청자(또는 독자)가 알고 있는 사실이라는 것을 전제라고 표현  화행 : 언어를 통해서 이루어지는 행위, 화자의 발화화 함께 취해지는 행위          직접 화행      간접 화행      담화론텍스트와 대화 속에 나타난 언어를 연구하는 학문  결속(Cohesion)  일관성(Coherence)  대화 분석  차례 얻기  협조의 원칙          양의 격률      질의 격률      상관성의 격률      방법의 격률        함의",
      "url": "/2025/03/05/Natural_Language_Processing-01.html"
    },
  
    {
      "title": "생성 모델과 판별 모델",
      "tags": "DL, COMPUTER_VISION",
      "desc": "생성 모델과 판별 모델 - 2025년02월27일  tag : DL|COMPUTER_VISION|",
      "content": "생성 모델과 판별 모델 - 2025년02월27일  tag : DL|COMPUTER_VISION|생성 모델과 판별 모델생성 모델 (Generative Models)생성 모델은 데이터 X와 특성 Y의 결합 분포 p(X, Y) 또는 조건부 분포 p(X|Y)를 추정합니다. Y가 없는 경우, 데이터의 주변 분포 p(X)를 추정합니다.가정: 데이터는 저차원의 필수적인 정보로부터 생성 가능하다고 가정합니다.예시: 가우시안 혼합 모델 (Gaussian Mixture Model, GMM)특징  어려움:          고차원 데이터 모델링: 복잡한 모든 특성의 분포를 알아야 함      평가 지표: 생성된 데이터에 대한 정량적 평가가 어려움        활용:          이미지 품질 개선      맥락에 맞는 이미지 자동 완성      최대 가능도 추정법 (Maximum Likelihood Estimation, MLE)가능도를 최대화하는 파라미터 값을 찾는 방법입니다. 일반적으로 가능도 함수의 미분을 통해 계산합니다.  Kullback-Leibler Divergence: 쿨백-라이블러 발산 최소화 = 로그가능도 최대화평가 지표  Inception Score(IS)          예리함과 다양성 두 가지를 주요하게 고려      IS = Sharpness(S) * Diversity(D)      한계점              분류기 모델의 훈련 데이터 셋과 다른 데이터를 생성하는 경우 제대로 평가하기 어려움      IS가 높은 데이터를 생성하면 계속 같은 데이터를 생성(Mode Collapse)      기울기 기반(Gradient Based) 공격, 리플레이(Replay) 공격을 통해 점수 조작 가능        Frechet Inception Distance(FID)          생성된 데이터의 특징 벡터를 이용하여 훈련 데이터와의 거리를 계산      훈련 데이터와 생성 데이터를 모두 활용      훈련 데이터와 생성 데이터의 각 분포를 정규 분포로 가정하고, 두 분포의 거리를 Frechet 거리로 계산      한계점                  FID 점수는 Fidelity와 Diversity를 각각 평가할 수 없음                      개선된 정밀도, 재현율(Improved Precision &amp; Recall)          Precision : 생성된 데이터 중에서, 실제 데이터 분포에 아주 가까운 데이터 = (TP) / (TP + FP)      Recall : 실제 데이터 중에서, 생성된 데이터 분포에 아주 가까운 데이터 = (TP) / (TP + FN)      한계점                  이상치에 민감          실제 데이터와 생성된 데이터의 분포가 동일하더라도 샘플링에 따라 점수가 낮을 수 있음                    문제 완화                  Density : 반경의 합집합이 아닌 가중 합집합으로 계산하여 이상치에 대해 상대적으로 덜 민감          Coverage : 생성된 데이터에 대해 매번 계산하지 않고 실제 데이터 집합으로 미리 계산하여 안정적이고 계산량 감소                      조건부 정확도(Conditional Accuracy)  Learned Perceptual Image Patch Similarity(LPIPS)          모델 특징 비교를 통한 영상간 유사도 측정        CLIP-Score          Text와 Image 간의 유사도 측정      판별 모델 (Discriminative Model)판별 모델은 데이터 X가 주어졌을 때, 특성 Y가 나타날 조건부 확률 p(Y\\|X)를 직접적으로 반환합니다.  판별 모델은 정답(Ground Truth, GT)가 존재하므로 모델의 출력을 정답과 비교하기 용이  분류, 회귀 문제로 나눌 수 있음특징: 주어진 데이터를 통해 데이터 사이의 경계를 예측합니다.예시: 로지스틱 회귀 분석생성 모델 vs 판별 모델            특성      생성 모델      판별 모델                  추정      p(X, Y) 또는 p(X|Y)      p(Y|X)              접근 방식      데이터 생성 과정 모델링      클래스 간 경계 학습              복잡도      상대적으로 높음      상대적으로 낮음              데이터 요구량      더 많은 데이터 필요      적은 데이터로도 가능              유연성      새로운 클래스 추가 용이      새로운 클래스 추가 어려움      오토 인코더입력 데이터의 패턴을 학습하여 데이터를 재건하는 모델 -&gt; 비선형 차원 축소 기법으로 활용 가능  인코더 : 데이터를 저차원 잠재 표현으로 요약  디코더 : 저차원 잠재 표현으로부터 데이터를 재구성(Reconstruction)손실함수잠재 표현으로부터 복구한 데이터와 입력 데이터의 평균제곱오차(MSE)디노이징(Denoising) 오토 인코더입력 데이터에 랜덤 노이즈를 주입하거나 Dropout 레이어를 적용  노이즈가 없는 원래 데이터로 재구성  원리 : 노이즈에 강건한 잠재 표현(미세하게 변형된 데이터도 같은 잠재 벡터로 표현되도록)활용  특징 추출기          잠재 벡터로부터 분류, 클러스터링 문제 해결        이상치 탐지(Anomaly Detection)          이상치는 재구성 했을 때 평균제곱오차가 크게 나올 것!      특정 임계값을 넘으면 이상치로 판단      변분 오토 인코더(Variation Autoencoder, VAE)오토 인코더와 동일한 구조(Encoder + Decoder)를 가지는 생성 모델잠재 변수 모델 : 데이터는 저차원의 잠재 변수로부터 생성됨  잠재 벡터의 분포 : 표준정규분포  장점          q(z|x)로부터 데이터를 요약하는 유용한 잠재 표현을 찾을 수 있음        단점          가능도가 아닌 가능도의 하한을 최대화      흐릿한 이미지를 생성      Evidence of Lower BOund(ELBO)현재 모델이 우리가 가진 현상을 얼마나 잘 설명하는가 = 가능도(Likelihood)직접 계산이 어려우니, 간점적으로 계산하여 최대화함VQVAE(Vector Quantized Variational Autoencoder)유한한 잠재 표현을 활용하는 변분 오토 인코더  이산(Discrete) 잠재 변수          범주 : K개의 D차원 임베딩(Embedding) 벡터      적대적 생성 신경망 (Generative Adverarial Networks, GANs)적대적으로 학습하는 신경망들로 구성되며, 생성 모댈로써 활용함  생성된 데이터와 실제 데이터를 판별하고 속이는 과정을 거치며 생성 모델을 개선  데이터를 생성하는 생성 모델과 데이터의 진위를 구별하는 판별 모델(Discriminator)로 구성          판별 모델 : 생성된 데이터를 입력으로 받아 실제 데이터인지 생성된 데이터인지를 출력        훈련 과정          임의의 초기 분포로부터 생성 모델이 데이터를 생성      판별 모델이 분류; 판별 모델 갱신      갱신된 판별 모델을 고정;생성 모델 갱신      반복 과정을 거쳐 생성 모델은 판별 모델이 구별할 수 없는 수준의 데이터를 생성        목적 함수          생성 모델 : 실제와 유사한 데이터를 생성하여 판별자를 속여야 함(다음 목적 함수를 최소화함)      판별 모델 : 실제와 생성된 데이터를 정확하게 구별해야 함(다음 목적 함수를 최대화함)      조건부 생성 모델  필요성 : 다양한 활용을 위해 생성 데이터의 의미 제어 방법이 필요함- 조건을 입력 받아 원하는 의미를 갖는 데이터를 생성하는 생성 모델- 범주(카테고리)부터 영상의 전체 구조(레이아웃)에 이르기까지 다양한 입력을 조건으로 받음- 높은 다양성과 품질을 동시에 누릴 수 있으나 수집하기 더 까다로운 데이터를 필요로함이미지 대 이미지 (Image-to-Image Translation)  Pix2pix          조건부 GAN (cGAN)을 기반으로 한 모델      입력 이미지를 다른 스타일이나 도메인으로 변환      지도 학습 방식으로 특정 입력-출력 쌍을 필요로 함      예: 스케치를 컬러 이미지로 변환        CycleGAN          지도 학습 없이 이미지 스타일 변환을 수행      두 개의 생성자와 판별자를 사용하여 도메인 간 변환을 학습      예: 말 ↔ 얼룩말, 여름 ↔ 겨울        BiCycleGAN          Pix2pix 모델의 확장      하나의 입력에 대해 다양한 출력을 생성할 수 있도록 개선      잠재 공간에서 샘플링을 통해 다중 모드 출력 가능        StarGAN          하나의 모델로 여러 개의 도메인 변환을 가능하게 함      도메인 간 이미지 변환을 하나의 네트워크에서 수행      예: 얼굴 사진에서 성별, 나이, 감정 변화 적용        InstaGAN          개별 객체 단위의 변환이 가능한 모델      배경과 객체를 분리하여 변환 적용      예: 개별적인 얼굴 스타일 변경        LostGAN          객체 수준의 이미지 변환 및 생성에 특화      레이아웃 정보를 사용하여 장면을 구성      예: 주어진 레이아웃에 따른 장면 이미지 생성        SPADE (Spatially-Adaptive Denormalization)          레이아웃 정보를 반영한 고품질 이미지 생성      세그먼트 맵을 활용하여 이미지 스타일 변환      예: 스케치를 사실적인 장면으로 변환      텍스트 대 이미지 (Text-to-Image Generation)  GAN-CLS (GAN with Conditional Latent Space)          텍스트 설명을 조건으로 하는 이미지 생성 모델      텍스트 임베딩을 GAN에 입력하여 관련된 이미지 생성      예: “붉은 꽃이 핀 초록색 들판” → 해당 이미지 생성        GigaGAN          초고해상도(High-Resolution) 텍스트 기반 이미지 생성 모델      기존 GAN 기반 텍스트-이미지 모델보다 더 높은 품질과 해상도 제공      예: 텍스트 입력만으로 사실적인 고해상도 이미지 생성      확산 모델확산 확률 모델(Diffusion Probabilistic Model, DPM)  DPM - 확률          확산 현상을 시간에 따라 확률적 모델링      마르코프 체인 : 미래는 과거가 아닌 현재에만 의존        DPM 구조          정방향 확산 : 데이터 -&gt; 노이즈 ( 고정 )      역방향 확산 : 데이터 &lt;- 노이즈 ( 학습 )                  이미지 생성 과정 = 노이즈를 제거하는 과정은 계산 불가                      VAE 와 다른 점          잠재 변수의 차원이 모두 데이터의 차원과 동일 + 여러 단계의 잠재 변수를 가짐      디코더를 모든 시점에서 공유 + 인코더는 학습되지 않음      디노이징 확산 확률 모델(DDPM)손실 함수를 간단한 형태로 정리함  구조          노이즈 예측 : U-net 구조      t 시점 주입 : 사인 곡선적 포지션 임베딩(Sinusoidal Position Embedding)        생성 과정          노이즈 \\(X_T\\)를 표준정규분포에서 샘플링                  t가 클 때 : 핵심적인 특징을 담고 있음          t가 작을 때 : 세부적인 특징을 담고 있음                      한계점          느린 생성 과정 : 5만개의 32x32 크기 이미지 생성 위해 20시간 필요      조건부 생성 불가                  DDPM은 Unconditional 모델 ( 조건 없는 모델 )          품질 - 다양성 조절 불가                    생성 과정 가속화DDIM(Denoising Diffusion Implicit Model)마르코프 체인 : 미래는 과거가 아닌 현재에만 의존함  샘플링 과정에서 마르코프 체인이 등장함          잡음이 조금 덜 낀 샘플을 더 심한 샘플과 모델이 이를 통해 구한 노이즈로 추정        DDPM 에서 DDIM으로          마르코프 체인을 가정하지 않는 확산 확률 모델을 정의 -&gt; 직전 시점에만 의존하지는 않음      학습된 모델을 가지고 일부 시점만 거쳐 데이터 생성 가능 -&gt; 생성 속도 개선      학습은 DDPM처럼 그러나 빠른 sampling을 원함(DDPM이 활용하는 중요 특성을 만족해야함)        생성과정          특성을 만족하는 정방향 함수를 정의함      생성 시 노이즈를 추가하지 않는 방법 제안      랜덤성은 오직 표준정규분포를 따르는 노이즈 \\(X_T\\) 에만 존재      Denoising Diffusion GANS적은 시점( T &lt;= 8 )을 사용하기 위해 조건부 GAN을 활용하여 복잡한 디노이징 분포를 학습Progressive Distillation학습된 확산 확률 모델로부터 시점을 절반으로 줄이는 경량화된 모델을 반복적으로 학습이웃한 2개의 역방향 확산 과정을 하나로 합침Consistency Model and Distillation학습 시 모든 Time Step에 대해 동일한 결과를 내도록 학습을 진행함Latent Consistency Model1 step to 8 step조건부 생성  조건부 DPM          정방향 확산은 고정      역방향 확산(생성 과정)에서 조건 추가      Guided Diffusion스케일을 조절하면 클래스를 유지하도록 생성 가능  단점 : 분류기를 추가 학습해야 함Classifier-free Diffusion Guidance분류기를 베이즈 정리를 활용하여 대체함확산 확률 모델과 조건부 확산 확률 모델을 함께 학습함(학습할 때 랜덤하게 클래스를 버림)생성할 때 두 모델에서 예측한 노이즈의 가중합 계산잠재 확산 모델픽셀 공간에서 잠재 공간으로  기존의 확산 모델들은 고차원 이미지 공간에서 연산을 반복함  이미지의 정보를 유지한 채로, 차원을 축소할 수 있다면 계산 복잡도를 감소시킬 수 있음  영상 생성은 인지적 압축 과정과 의미적 압축으로 대략적 구분이 가능함          인지적 압축 : 과도한 세부 사항을 제거하며 핵심적인 특징을 잠재 표현으로 요약      의미적 압축 : 데이터의 의미적, 구조적 특징을 학습      잠재 확산 모델실제 이미지의 고차원 공간이 아닌 잠재 공간에서 노이즈 연산을 반복하도록 설계  확산 모델에서 주요한 연산들이 모두 잠재 공간에서 이루어지므로 훨씬 더 효율적인 공간에서 훈련 가능          기존 확산 모델보다 계산, 시간 자원 모두 효율적        사전 훈련 : 벡터 양자화 변분 오토 인코더          재건된 이미지를 가짜 이미지로 두어 실제 이미지와 판별하는 과정 포함      조건부 생성Text-to-Image  잠재 확산 모델은 확산 모델과 마찬가지로 다양한 조건부 생성이 가능(Ex. Text-to-Image, 초고해상도 이미징, 인페이팅)  텍스트 입력이 추가되는 경우, 텍스트 인코더를 사전 훈련하는 단계가 필요함  CLIP은 텍스트와 이미지 간의 대조 학습을 통해 훈련          텍스트-이미지 쌍에 대한 이해가 높은 모델        확산 모델과의 차이          텍스트 정보와 노이즈화된 잠재 표현의 관계성 학습을 위해 Cross-attention을 추가        Layout-to-Image  Masked-to-ImageLatent Diffusion Model(LDM)  입력과 출력을 잠재 공간으로 매핑 후 학습해, 높은 품질의 영상을 효율성 높게 학습, 생성함  일반 생성 외에도 텍스트, Layout, 가려진 이미지 등 다양한 조건 기반의 생성이 가능함  현재 가장 활발히 활용, 연구되는 모델확산 모델의 개인화우리가 원하는 대상에 관한 사진이 일부 있을 때 피사체에 대한 새로운 이미지를 텍스트로부터 생성하는 방법텍스트 반전  학습 데이터 : 우리가 원하는 대상에 관한 이미지 3~5개  대상을 표현하는 새로운 단어를 임베딩 공간에서 찾기DreamBooth사전학습된 text-to-image 확산 모델 중 U-net을 미세 조정  파인튜닝의 문제점          언어 드리프트 : 대상과 동일한 클래스의 이미지를 생성하는 방법을 천천히 잊어버림      다양성 : 포즈와 각도 등이 새로운 학습 이미지에 과적합될 수 있음      LoRA기존 가중치는 유지하고 가중치 변화량을 학습함 -&gt; 기존 학습 결과를 잊지 않는 효과가중치 변화량을 행렬 2개의 곱으로 분해 -&gt; 학습 파라미터 수가 대폭 감소ControlNet다양한 Structural input을 조건으로 주기 위한 방법추가적인 layer를 학습해, U-net은 유지하고 차이를 추가하는 방법Face0사람 얼굴을 조건삼아, 동일한 얼굴의 다양한 결과를 생성하는 기법얼굴에서 특징을 추출, 텍스트 표현과 함께 모델에 제공함IP-AdapterText뿐만 아니라 이미지 feature를 함께 입력받아 그에 맞게 적절히 생성해낼 수 있게 함별도의 cross-attention layer를 두어 따로 학습함Imagic텍스트만을 활용한 의미적 편집 수행",
      "url": "/2025/02/27/generation.html"
    },
  
    {
      "title": "Computer Vision - 02",
      "tags": "DL, COMPUTER_VISION",
      "desc": "Computer Vision - 02 - 2025년02월14일  tag : DL|COMPUTER_VISION|",
      "content": "Computer Vision - 02 - 2025년02월14일  tag : DL|COMPUTER_VISION|Computer VisionCV metrics  Classification 을 위한 평가 지표          Accuracy      Precision / Recall      Sensitivity / Specificity      F1 Score      AUC - ROC        Object Detection 을 위한 평가 지표          IoU : Bounding Box의 겹치는 영역의 크기를 통해 정확도를 평가하는 방법      Precision / Recall      AP      mAP        Semantic Segmentation 을 위한 평가 지표          PA      MPA      IoU      Dice coefficient      BackboneMulti Layer Perceptron  MLP는 input layer -&gt; hidden layer -&gt; output Layer      이미지를 MLP의 input layer에 입력하려면 이미지를 flatten 해야함    이미지의 Locality 특성          Spatial Locality : 같은 물체라도 이미지마다 크기가 다름      Positional invariance : 같은 물체라도 다른 위치에 있을 수 있음        Convolution filter          이미지를 flatten 하지 않고 사용할 수 있음      Convolution Neural Network  이미지의 Locality 특성을 고려하여 학습할 수 있도록 설계된 neural network  한계점          이미지 안에서 멀리 떨어진 객체끼리 관련성을 파악하기가 어려움      이미지의 각 파트가 이미지 이해에서 얼마나 중요한지, 얼마나 서로 관련이 있는지 평가할 수 없음      ImageNet Large Scale Visual Recognition Challenge (IKSVRC)AlexNet  CNN을 사용한 최초의 모델이자 GPU 사용을 고려한 딥러닝 모델VGG  구도가 단순하지만, 깊은 네트쿼그의 중요성을 알림ResNet  네트워크를 깊게 쌓을 경우 생기는 문제점을 해결, 기존보다 더 깊은 네트워크를 제안(최대 152 Layer)EfficientNet  기존 네트워크들이 가진 trade-off를 분석Transformer  Natural Language Processing (NLP) task 를 위해 고안VIT &amp; Swin  Transformer 구조를 computer vision 에 적용하여 해결한 VIT(Vision Transformer)CNN 특성을 ViT 에 다시 적용한 Swin TransformerVGGResNet  Residual block(skip connection)          각 block의 input을 x, Convolution 연산을 F라고 가정F(x) + x 를 output으로 사용      EfficientNetWidth ScalingDepth ScalingResolution ScalingCompound ScalingTransformer",
      "url": "/2025/02/14/Computer-Vision-02.html"
    },
  
    {
      "title": "Computer Vision - 01",
      "tags": "DL, COMPUTER_VISION",
      "desc": "Computer Vision - 01 - 2025년02월11일  tag : DL|COMPUTER_VISION|",
      "content": "Computer Vision - 01 - 2025년02월11일  tag : DL|COMPUTER_VISION|Computer Vision  AI의 한 종류로, vision 데이터들에서 의미 있는 정보를 추출하고 이를 이해한 것을 바탕으로 여러가지 작업을 수행하는 것고전 컴퓨터 비전  규칙 기반의 이미지 처리 알고리즘 ( Ex. OpenCV )Morphological Transform  이미지에 기반한 연산이며, 흑백 이미지에서 일반적으로 수행종류  Erosion          물체의 경계를 침식      이미지의 특징을 축소할 때도 사용 가능        Dilation          Erosion과는 정반대로 동작      사물의 크기를 팽창할 때도 사용 가능        Opening          Erosion 커널과 Dilation 커널 순서대로 동작되는 연산        Closing          Dilation -&gt; Erosion 동작하는 연산        Morphological gradient  Top hatContour Detection  Contour : 같은 색깔 및 intensity를 가지는 연속적인 경계점들로 이루어진 curve고전 컴퓨터 비전을 활용하여 raw image에서 객체의 contour를 추출중요성  딥러닝 모델 사용 X -&gt; 딥러닝 모델 학습을 위한 데이터 가공 시 활용 가능과정  Edge Detection          Canny Edge Detector : 정확도 높음, 실행 시간 느림, 구현 복잡함              과정 : 노이즈 제거 -&gt; 이미지 내의 높은 미분값 찾기 -&gt; 최대값이 아닌 픽셀 값 0으로 치환 -&gt; 하이퍼파라미터 조정을 통한 세밀한 엣지 검출        Dilation(optional)  Contour detection          Raw image를 binary image로 변환 -&gt; OpenCV 의 findContours() 함수 이용      컴퓨터 비전 모델 구조Vision Feature  컴퓨터 비전의 태스크를 해결할 때 필요한 이미지의 특성을 담고 있는 정보들을 지칭함.Backbone  이미지에서 중요한 Feature를 추출(extract)할 수 있도록 훈련됨역할 : 주어진 비전 태스크를 잘 수행할 수 있는 압축된 Visual Feature를 산출하는 것구조 : low-level -&gt; mid-level -&gt; high-levelDecoder  압축된 Feature를 목표하는 태스크의 출력 형태로 만드는 과정을 수행CNN  Convolution Layer : 네트워크가 비전 테스크를 수행하는 데에 유용한 Feature들을 학습할 수 있도록 함  Activation Function : 네트워크에 비선형성을 가해주는 역할을 함  Pooling Layer : Feature Map에 Spatial Aggregation을 시켜줌AlexNet  구조 : Convolution - Pooling - Batch Normalization  Lateral Inhibition : 강하게 활성화된 뉴런이 다른 뉴런의 값을 억제하는 현상Overfitting  방지          Data Augmentation : 학습 데이터에 변형(Augmentation)을 가해서 좀 더 다양성을 지닌 데이터로 학습될 수 있도록 하는 방법      Dropout : 뉴런 중 일부를 일정 비율로 생략하면서 학습을 진행하는 방법, 학습 시에만 행해지고, 테스트를 할 때는 모든 뉴런을 사용함      ResNetEfficientNetClass Activation Mapping ( CAM )  모델이 어디를 보고 있는지 시각화 할 수 있는 방법을 제시Gradient-weighted Class Activation Map ( Glad-CAM )Object Detection  사물 각각의 Bounding Box 위치와 Category를 예측2-stage Detector  Region Proposals 및 Feature Extractor를 거치며 object detection 수행  Region Proposals : 다양한 크기와 모양의 Bounding Box 로 물체의 위치를 제안  Feature Extractor : 제안한 Region (Bounding Box)에 대하여 물체의 특성 추출1-stage Detector  Region Proposals 없이, Feature Extractor만을 이용한 object detection 수행성능 평가 방법Intersection of Union ( IOU )  우리가 예측한 Bounding Box 와 실제 Bounding Box 값의 차이를 계산IOU = Area of intersection / Area of UnionAverage Precision ( AP )R-CNNRegion Proposals  Selective Search 기법으로 2000개의 Roi(Region of Interests)를 추출  Selective Search : 인접한 영역끼리 유사성을 측정해 큰 영역으로 차례대로 통합Warped Region  각 Roi 에 대하여 동일한 크기의 이미지로 변환Backbone  Region Proposals 마다 각각의 Backbone (CNN)에 넣어서 결과를 계산Output  SVM : 각 Region Proposals 마다 SVM으로 Class 분류 결과 예측  Bbox Regression : Backbone의 Feature를 Regression으로 Bounding Box 위치 예측Limitation  CPU 기반의 Selective Search 기법으로 인해 많은 시간이 필요  2000개의 Roi로 인하여, 2000번의 CNN연산이 필요하며 많은 시간이 필요Fast R-CNNRoi Pooling  각 Roi 영역에 대하여 Max Pooling을 이용고정된 크기의 Vector 생성Limitation  CPU기반의 Selective Search 기법으로 인해 처리 속도 느림  Roi Pooling 정확도 떨어짐Faster R-CNNRegion Proposal Network (RPN)  Feature Map을 기반으로 물체의 위치 예측K개의 Anchor Box를 이용Sliding Window 방식으로 Feature Map에 적용Non-Maximum-Suppression (NMS)  중복된 경계 Bbox 를 제거하여 최종 객체 감지 결과를 정리하고 정확도를 높이는 기술Limitation  2-stage Detector로 연산량이 많아, 실시간 사용에 부적합  1-stage Detector인 YOLO는 실시간 사용 가능1-stage DetectorYOLO- Single Shot Architecture : YOLO는 객체 감지를 위한 단일 신경망 아키텍처를 사용  - 이미지를 그리드로 나누고, 그리드 셀 별로 Bounding Box 와 해당 객체의 클래스 확률 예측Semantic Segmentation  Pixel-wise 로 각각의 class 를 예측하여 물체 Category 별로 분할성능 평가 방법 - Intersection-over-Union ( IoU )  IoU = Area of Intersection / Area of UnionIoU = TP / ( TP + FP + FN )Sliding Window  문제점          픽셀 주변의 정보밖에 고려하지 못함      많은 패치가 중복되는 영역을 처리하기 때문에 계산 비용 증가      Size Preserving Convolutional Layers  문제점          Receptive field 가 여전히 제한적      Downsampling + Upsampling  장점          큰 Receptive field를 가짐      Fully Convolutional Networks ( FCN )  Convolution(Downsampling)          Backbone를 통해 Features 추출        Deconvolution(Upsampling)          Feature Map 을 확장하여 입력 이미지와 동일한 크기의 Segmentation Map 생성        Skip Connection          Upsampling 의 결과와 Backbone의 중간 Layer에서 나오는 Feature Map과 결합      U-NetEncoder  입력 이미지를 Downsampling이미지의 공간 정보를 계층적으로 추상화 및 중요한 Feature 추출Decoder  Encoder 가 추출한 Feature를 UpsamplingUpsampling하여 Segmentation Map 생성Skip Connection  Encoder와 Decoder 간의 정보 전달Decoder에서 Upsampling된 Feature와 동일한 해상도의 Encoder Feature를 결합",
      "url": "/2025/02/11/Computer-Vision-01.html"
    },
  
    {
      "title": "Pytorch",
      "tags": "DL, PYTORCH",
      "desc": "Pytorch - 2025년02월08일  tag : DL|PYTORCH|",
      "content": "Pytorch - 2025년02월08일  tag : DL|PYTORCH|Pytorch텐서  데이터 배열(array)를 의미Broadcasting  차원이 다른 두 텐서 혹은 텐서와 스클라간 연산을 가능하게 해주는 기능Dense Tensor  배열의 모든 위치에 값을 가지는 텐서Sparse Tensor  0이 아닌 원소와 그 위치를 저장하는 텐서딥러닝에서 Pytorch 작동 방식  Data          DataLoader      Dataset        Model          torch.nn.Module        Output  Loss          torch.nn      torch.nn.functional        Optimization          torch.optim      전이학습  사전 학습된 모델의 지식을 다른 task에 활용하는 것Fine-Tuning  Pretrained Model 을 그대로 혹은 layers를 추가한 후 새로운 작업에 맞는 데이터로 모델을 추가로 더 훈련시키는 방법Domain Adaptation  A 라는 도메인에서 학습한 모델을 B라는 도메인으로 전이하여 도메인 간의 차이를 극복하는 것이 목적Multi-task learning  하나의 모델을 사용하여 여러 개의 관련된 작업을 동시에 학습하면서 공통으로 사용되는 특징을 공유하는 학습 방식Zero-shot learning  기존에 학습되지 않은 새로운 클래스나 작업에 대해 예측을 수행하는 기술One/few-shot learning  하나 또는 몇 개의 훈련 예시를 기반으로 결과를 예측한느 학습 방식Pytorch Lightning  Pytorch에 대한 high-level 인터페이스를 제공하는 오픈소스 라이브러리Hydra  파라미터가 복잡해지면서 코드를 구조화하거나 관리하기 어려운 문제를 해결하기 위해, 별도의 설정 파일을 작성하여 관리하는데 사용하는 오픈소스 프레임워크",
      "url": "/2025/02/08/Pytorch.html"
    },
  
    {
      "title": "DEEPLEARNING BASIC",
      "tags": "DL",
      "desc": "DEEPLEARNING BASIC - 2025년02월07일  tag : DL|",
      "content": "DEEPLEARNING BASIC - 2025년02월07일  tag : DL|딥러닝딥러닝의 발전 5단계  Rule based programming          목표 달성에 필요한 연산 방법을 사람이 전부 고안하는 방법        Conventional machine learning          특징값을 뽑는 방식은 기존처럼 하되, 특징값들로 판별하는 로직은 기계가 스스로 고안      동작 방법-1. 학습 데이터 준비 [ 이미지 수집 -&gt; 특징 정의 -&gt; 학습 데이터 생성 ]-2. 모델 학습 [ 예측 및 오차 계산 ]        Deep Learning          출력을 계산 하기 위해 모든 연산들을 기계가 고안        Pre-training &amp; Fine-tuning          기존 문제점[분류 대상 or 태스크가 바뀔 때마다 다른 모델이 필요]      이를 해결하기 위해 Pre-training 으로 미리 대량의 데이터를 학습 후 Fine-tuning으로 특정한 Task에 맞게 추가 학습        Big Model &amp; zero/few shot          Multimodal 학습 가능 [ 텍스트, 이미지, 음성, 영상 등 다양한 입력을 동시에 학습하는 방법 ]      Zero-shot Learning : 한 번도 본 적 없는 Task나 데이터에 대해 학습 없이 정답을 예측하는 방법      Few-shot Learning : 적은 양의 예제 데이터를 제공한 후 학습하는 방법      딥러닝의 구성 요소  DATA  MODEL          Activation Function : 입력 신호의 총합을 출력신호로 변환하는 함수 [ Ex. Sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU ]      경사하강법-1. 확률적 경사하강법 : 모든 데이터를 사용해서 갱신하는 것 대신, 데이터 일부만을 사용해서 여러 번 갱신하는 방법      역전파 [ 밑바닥부터 시작하는 딥러닝1 참고 ]        Loss Function : 실제 값과 예측 값 사이의 차이, 오차를 수치화하는 함수          Mean Squared Error[= L2 Loss] : 실제 값과 모델의 예측 값 사이의 차이를 제곱하여 평균낸 값[보통 회귀문제에서 사용]                  단점 : 이상치에 민감                    Mean Absolute Error[= L1 Loss] : 실제 값과 모델의 예측 값 사이의 차이에 대한 절대값을 평균낸 값                  단점 : 기울기가 일정하여 점핑이 일어날 수 있음 [ 지역 최소점에 도달하지 못 하는 문제]                    Huber Loss : 오차가 일정 수준 이하일 때는 MSE, 그렇지 않을 때는 MAE를 사용하여 두 손실 함수의 장점을 결합한 방법      Cross Entropy : 주어진 확률 변수 또는 사건 집합에 대한 두 확률 분포 간의 차이를 측정하는 함수[보통 분류문제에서 사용]                                                                              일반적인 경우                  \\(CE = -\\sum{Q(x)*log(P(X))}\\)                                                                                                                          이진분류                  \\(BCE = -\\sum{ylog(p) + (1-y)log(1-p)}\\)                                                                        Hinge Loss : 모델이 만드는 decision boundary 와 데이터 사이의 margin 을 최대화하는 것을 목표로 하는 함수                  \\[Hinge = max(0, 1-y * \\hat{y})\\]                                Optimization Algorithm : Loss Function 이 최소값을 가지도록 모델의 파라미터를 최적화하는 알고리즘  성능 향상을 위한 기타 알고리즘성능 고도화 방법과적합, 과소적합  과적합과 과소적합이 아닌 적합한 상태를 ‘Robust’하다 라고 표현한다.  Bias 와 Variance 는 반비례 관계 -&gt; 모델은 두 가지를 동시에 최소화하는 방향으로 Trade-off를 고려해야 한다.지역 최소값, 전역 최소값  지역 최소값에 빠지지 않도록 조심해야 한다.네트워크 안정화 기법Dropout  모델 학습 시 임의의 가중치 노드를 일정 확률로 비활성화시키는 방법 [ 앙상블 방법 ]정규화배치 정규화  Batch 단위의 데이터를 기준으로 평균과 분산을 계산하여 활성화 레이어 이후 출력을 정규화하는 방법[활성화 함수를 적용하기 전에 적용하는 것을 추천]  \\[z = \\frac{x - \\mu}{\\sigma}\\]  레이어 정규화  배치 정규화의 단점을 보완하는 방법  배치 정규화는 입력 데이터의 단위가 일정해야 사용가능 -&gt; 이 점 보완인스턴스 정규화  이미지 변환 시 사용하는 정규화그룹 정규화  인스턴스 정규화의 확장 버전가중치 초기화  왜 가중치 초기화가 필요한가?          가중치 초기화를 진행하지 않으면 모델의 층이 깊어질수록 활성화 함수 이후 데이터의 분포가 한 쪽으로 쏠릴 수 있다.              Xavier 초기화 방법 : Sigmoid나 tanh 같은 선형 활성화 함수 또는 근사를 사용할 때 효과적이다.He 초기화 방법 : Xavier는 ReLU 활성화 함수에서는 여전히 문제점을 해결하지 못함으로 He 초기화 방법 사용            과적합 방지를 위한 규제화 및 학습률 조정가중치 감쇠 : 큰 가중치에 대한 패널티를 부과학습 조기종료학습 스케줄러  Constant : 초기에 설정한 학습률을 학습 과정 전체에 걸쳐 변경하지 않음  Step Decay : 일정한 주기 마다 학습률을 일정 비율로 감소  Exponential Decay : 학습률을 지수적으로 감소  Cosine Anneealing : 코사인 함수를 따라 학습률이 감소하도록 설정          학습률이 안정적인 감소를 보이게 하며, 일정 주기로 다시 재시작할 수 있음        One-cycle Policy : 학습률이 먼저 증가한 다음 감소하도록 설정          빠르게 수렴하고, 끝부분에서는 학습률을 감소시켜 안정적인 학습을 도움      옵티마이저  Momentum          \\(v \\leftarrow \\alpha{v} - \\eta{\\frac{\\partial{L}}{\\partial{W}}} | v : 속도, \\alpha : 일반적으로 0.9, \\eta : 학습률\\)\\(W \\leftarrow W + v\\)        Nesterov Accelerated Gradient(NAG)          \\(v \\leftarrow \\alpha{v} - \\eta{\\frac{\\partial{L}}{\\partial{W}}}(W + \\alpha{v})\\)\\(W \\leftarrow W + v\\)다음 위치에서의 기울기를 미리 계산하여 보다 더욱 정확하게 업데이트를 가능하게 한다.        AdaGrad          \\(h \\leftarrow h + \\frac{\\partial{L}}{\\partial{W}} \\bigodot \\frac{\\partial{L}}{\\partial{W}}\\)\\(W \\leftarrow W - \\eta{\\frac{1}{\\sqrt{h}}\\frac{\\partial{L}}{\\partial{W}}}\\)        RMSProp          \\(h \\leftarrow \\beta{h} + (1 - \\beta)(\\frac{\\partial{h}}{\\partial{W}} \\bigodot \\frac{\\partial{L}}{\\partial{W}})\\)\\(W \\leftarrow W - \\eta{\\frac{1}{\\sqrt{h+\\varepsilon}} \\bigodot \\frac{\\partial{L}}{\\partial{W}}}\\)        Ada          Momentum and RMSProp 의 장점을 모두 결합한 방법\\(m \\leftarrow \\beta_{1}{m} + (1 - \\beta_{1})\\frac{\\partial{L}}{\\partial{W}}\\)\\(v \\leftarrow \\beta_{2}{v} + (1 - \\beta_{2})(\\frac{\\partial{L}}{\\partial{W}})^2\\)\\(W \\leftarrow W - \\eta{\\frac{m}{\\sqrt{v} + \\varepsilon}}\\)      데이터 증강 기법  이미지          Cutout : 일부 영역 가리기      Mixup : 두 이미지의 픽셀 값을 선형적으로 조합      CutMix : 하나의 이미지에서 잘라낸 영역을 다른 이미지에 붙여넣는 방식        텍스트          동의어 대체(Synonym Replacement)      무작위 삽입(Random Insertion)      무작위 교체(유의하면서 사용)      무작위 삭제(유의하면서 사용)–&gt; 의미 유지 방법 : 능동태에서 수동태로, 직접화에서 간접화로, 역번역, 사전학습된 언어모델 사용        자기지도학습(Self-Supervised Learning) : 레이블이 명시적으로 제공되지 않아도 모델이 스스로 학습할 수 있도록 하는 방식          생성 학습(Generative Laerning)      대체 작업 학습(Proxy Task Learning)      대조 학습(Contrastive Learning)      CNN  입력 데이터의 크기가 주로 고정되어 있음StridePaddingPooling  Max Pooling  Average PoolingRNN  길이가 고정되어 있지 않은 데이터를 다룰 때 사용  시퀀스가 길어질수록 앞부분의 정보를 잊어버림LSTMGRU딥러닝의 역사  LeNet-5 (1998) - 최초의 CNN 구조 중 하나, Yann LeCun이 개발  ImageNet Large Scale Visual Recognition Challenge (2010~2017) - 대규모 이미지 분류 대회, 딥러닝 발전 촉진  AlexNet (2012) - ImageNet 대회 우승, 딥러닝 붐을 일으킨 CNN  VGG (2014) - 깊은 CNN 구조(VGG-16, VGG-19)로 이미지 분류 성능 향상  GoogLeNet (2014) - Inception 모듈 도입으로 연산 효율성 증가  Generative Adversarial Networks (2014) - Ian Goodfellow가 제안한 생성 모델 (GAN)  ResNet (2015) - 잔차 연결(Residual Connection) 도입, 매우 깊은 네트워크 학습 가능  Sequence-to-Sequence (2014) - 기계 번역 등 NLP 작업에 활용된 RNN 기반 모델  Transformer (2017) - “Attention is All You Need” 논문에서 제안, NLP 혁신  Bidirectional Encoder Representations from Transformers (BERT, 2018) - 사전 학습된 양방향 Transformer 기반 모델  Generative Pre-trained Transformer (GPT, 2018~현재) - OpenAI의 GPT 시리즈, 언어 모델 발전 주도  EfficientNet (2019) - 모델 크기 조정(scale) 최적화 CNN  Vision Transformer (ViT, 2020) - Transformer를 이미지 처리에 적용  ChatGPT (2022) - GPT 시리즈 기반 대화형 AI",
      "url": "/2025/02/07/DEEPLEARNING-BASIC-01.html"
    },
  
    {
      "title": "Database - 02",
      "tags": "Computer_Engineering",
      "desc": "Database - 02 - 2024년12월31일  tag : Computer_Engineering|",
      "content": "Database - 02 - 2024년12월31일  tag : Computer_Engineering|DATABASE데이터 입력INSERT INTO 테이블이름 (컬럼1, 컬럼2, ...) VALUES (값1, 값2, ...)-- EXINSERT INTO users VALUES (1, 'user1', 'user1@example.com', '1999-01-01', '2024-12-31 00:00:00');데이터 조회SELECT 컬럼FROM 테이블WHERE 조건GROUP BY 그룹화할_컬럼HAVING 필터_조건ORDER BY 정렬할_컬럼-- EXSELECT DISTINCT name -- 중복 제거     , priceFROM productsWHERE category_id = 2ORDER BY name DESC; -- 내림차순 정렬상세 조회  연산 및 집계 함수          COUNT                  SELECT COUNT(*) FROM products;                    SUM                  SELECT SUM(price) FROM products;                    AVG                  SELECT AVG(price) FROM products;                    MAX                  SELECT MAX(price) FROM products;                    MIN                  SELECT MIN(price) FROM products;                      LIKE를 이용한 패턴 검색          % : 임의의 문자열                  SELECT name FROM products WHERE name LIKE ‘%monitor%’                    _ : 임의의 문자(딱 하나)                  SELECT name FROM products WHERE name LIKE ‘_onitor’;                      GROUP BY &amp; HAVINGSELECT user_id     , SUM(price * amount)FROM mytableGROUP BY user_idHAVING SUM(price * amount) &gt; 500;  서브 쿼리 : 쿼리 속의 쿼리(쿼리 속의 select)SELECT user_id, product_idFROM mytableWHERE (user_id, amount) IN (  SELECT user_id, MAX(amount)  FROM mytable  GROUP BY user_id);데이터 수정 - UPDATE  UPDATE 테이블 SET 열=값, 열=값 [WHERE 조건식];  on update          cascade : 참조 데이터 업데이트 시 상대방 데이터도 함께 업데이트      set null : 참조 데이터 업데이트 시 상대방 테이블의 참조 컬럼을 NULL로 업데이트      set default : 참조 데이터 업데이트 시 상대방 테이블의 참조 컬럼을 Default 값으로 업데이트      restrict : 참조하고 있을 경우, 업데이트 불가      no Action : Restrict 와 동일, 옵션을 지정하지 않았을 경우 자동으로 선택됨      -- EXUPDATE products SET registration_date = '2024-12-31 00:00:00' WHERE name = 'monitor';데이터 삭제 - DELETE  조건에 맞는 데이터 삭제          DELETE FROM 테이블명 WHERE 조건식;        테이블의 모든 데이터 삭제          DELETE FROM 테이블명;        on delete          cascade : 참조 데이터 업데이트 시 상대방 데이터도 함께 삭제      set null : 참조 데이터 업데이트 시 상대방 테이블의 참조 컬럼을 NULL로 업데이트      set default : 참조 데이터 업데이트 시 상대방 테이블의 참조 컬럼을 Default 값으로 업데이트      restrict : 참조하고 있을 경우, 삭제 불가      no Action : Restrict 와 동일, 옵션을 지정하지 않았을 경우 자동으로 선택됨      ",
      "url": "/2024/12/31/Database-02.html"
    },
  
    {
      "title": "Database - 01",
      "tags": "Computer_Engineering",
      "desc": "Database - 01 - 2024년12월30일  tag : Computer_Engineering|",
      "content": "Database - 01 - 2024년12월30일  tag : Computer_Engineering|Database기본개념  데이터베이스란 여러 사람이 공유하여 사용할 목적으로 체계화해 통합, 관리하는 데이터의 집합  DBMS(Database Management System)          데이터베이스를 관리하는 시스템종류 : SQL Server, MongoDB, Maria DB, MySQL, ORACLE, PostgreSQL, SQLite        관계형 DBMS          데이터를 테이블 형태로 저장SQL를 사용해서 관계형 DBMS를 다룬다.        SQL의 3종류          DML : Data Manipulation Language                  SELECT : 데이터 조회INSERT : 데이터 삽입UPDATE : 데이터 갱신DELETE : 데이터 삭제                    DDL : Data Definition Language                  CREATE : 테이블 생성ALTER : 테이블 변경(구조 변경 등)DROP : 테이블 삭제RENAME : 테이블 이름 변경TRUNCATE : 테이블 구조를 유지한 채 초기화                    DCL : Data Control Language                  GRANT : 유저에게 권한 부여REVOKE : 유저로부터 권한 회수COMMIT : 데이터베이스에 작업 반영ROLLBACK : 작업 이전의 상태로 되돌림SAVEPOINT : 롤백의 기준점                      NoSQL : 비관계형 데이터베이스          구조를 정의할 필요 없이 데이터를 저장 및 검색확장성, 가용성이 장점        Transaction : 데이터베이스 상호작용의 단위          지켜야 할 성질 ACID              Atomicity(원자성): 트랜잭션은 하나의 논리적인 작업 단위로 간주Consistency : 데이터베이스를 일관된 형태로 유지Isolation : 동시에 여러 트랜잭션이 실행될 때 각 트랜잭션은 서로 간섭하지 않고 격리되어야 함Durability : 트랜잭션이 성공적으로 완료되면 그 결과는 영구적으로 저장되어야 함            데이터베이스 생성과 관리기본키와 외래 키  key : 조건에 맞는 데이터를 찾기 위한 식별자          기본키(Primary Key) : 행을 특정 지을 수 있는 단 하나의 데이터고유키(Unique Key) : 기본키와 유사하나 Null로 지정 가능외래키(Foreign Key) : 다른 테이블을 연결(참조)하기 위한 키      데이터베이스와 테이블 생성하기# 데이터 베이스 생성하기CREATE DATABASE example;# 데이터베이스 조회SHOW DATABASES;# 데이터베이스 사용USE example;# 데이터베이스 삭제DROP DATABASE example;# 테이블 생성CREATE TABLE 테이블이름 (  열이름1 자료형1, [DEFAULT 기본값] [NULL | NOT NULL],  ...)## EXCREATE TABLE users (\tuser_id INT,\tusername VARCHAR(50),    email VARCHAR(50),    birthdate DATE,    registration_date TIMESTAMP);CREATE TABLE users (\tuser_id INT PRIMARY KEY AUTO_INCREMENT,    username VARCHAR(50) NOT NULL,    email VARCHAR(100) UNIQUE,    birthdate DATE,    registration_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP);CREATE TABLE categories(\tid INT NOT NULL AUTO_INCREMENT,    name VARCHAR(50) NOT NULL,    PRIMARY KEY (id));CREATE TABLE products ( id INT NOT NULL AUTO_INCREMENT,   name VARCHAR(255) NOT NULL,   category_id INT NOT NULL,   PRIMARY KEY (id),   FOREIGN KEY (category_id) REFERENCES categories (id));# 데이터베이스 내 테이블 전부 조회SHOW TABLES;SHOW TABLES FROM world; # world라는 데이터베이스 내에 있는 테이블들 조회# 테이블 상세 조회DESC users;# 테이블 - 이름 변경RENAME TABLE old_name TO new_name;# 테이블 변경 - ALTER TABLEALTER TABLE tbl_name alter_option[열 추가, 열 삭제, 열 변경, 제약 추가/삭제]1. ALTER TABLE members ADD COLUMN age INT; # 열 추가2. ALTER TABLE members DROP COLUMN birth_date; # 열 삭제3. ALTER TABLE members MODIFY age VARCHAR(255); # 열 타입, 이름 변경4. ALTER TABLE members CHANGE age birth_date Date; # 이름 바꾸고 타입도 바꾸기5. ALTER TABLE members ADD PRIMARY KEY (id); # 기본 키 제약 추가6. ALTER TABLE order_items ADD FOREIGN KEY (product_id) REFERENCES products(product_id); # 외래키 추가7. ALTER TABLE members MODIFY id VARCHAR(30) NOT NULL; # NOT NULL 제약 추가8. ALTER TABLE members MODIFY id VARCHAR(30); # NOT NULL 제약 삭제9. ALTER TABLE members DROP PRIMARY KEY; # 기본키 제약 삭제(제약 이름으로도 삭제 가능)# 테이블 삭제DROP TABLE TABLENAME; # 테이블 구조까지 삭제TRUNCATE TABLE TABLENAME; # 테이블 구조는 유지, 행만 삭제  숫자형          TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT, FLOAT, DOUBLE, DECIMAL        문자형          CHAR : 고정 길이VARCHAR : 가변 길이BLOB : 바이너리 데이터TEXT : 텍스트 데이터        날짜/시간형          DATE : 날짜TIME : 시간DATETIME : 날짜와 시간TIMESTAMP : 날짜와 시간        기타          ENUM : 정해진 값들 중 하나만 저장      SET : 정해진 값들 중 여러 개를 저장      GEOMETRY : 지리 정보 저장      XML : XML 데이터 저장      JSON : JSON 데이터 저장      ",
      "url": "/2024/12/30/Database-01.html"
    },
  
    {
      "title": "System Programming - 03",
      "tags": "Computer_Engineering",
      "desc": "System Programming - 03 - 2024년12월29일  tag : Computer_Engineering|",
      "content": "System Programming - 03 - 2024년12월29일  tag : Computer_Engineering|소켓  socket : 네트워크를 경유하는 프로세스 간 통신의 종착점&gt; 송신지 프로세스는 메세지를 소켓으로 보내고 수신지 프로세스는 메세지를 소켓에서 읽는다.&gt; 소켓은 프로세스 간 통신 기법(IPC)의 일종  구성요소          TCP + IP 혹은 UDP + IP      출발지 IP 주소      출발지 포트 번호      목적지 IP 주소      목적지 포트 번호      TCP 소켓 다루기  socket : 소켓을 생성          $ int socket(int domain, int type, int protocol)domain : 통신 도메인 지정 ( 프로토콜 집단 )type : 통신 방법 지정protocol : 특정 프로토콜 지정        bind : 호스트의 IP 주소를 소켓에 연결          $ int bind(int socketfd, struct sockaddr *my_addr, socklen_t addrlen)socketfd : 생성된 소켓 디스크립터my_addr : 호스트의 주소addrlen : my_addr의 바이트 길이        listen : LISTEN 상태로 전환          $ int listen(int socketfd, int backlog)socketfd : 소켓 디스크립터backlog : 연결 요청을 담을 큐        accept : 연결 요청 수락          $ int accept(int socketfd, struct sockaddr *addr, socklen_t *addrlen)socketfd : 소켓 디스크립터addr : 연결 요청한 상대에 대한 정보addrlen : addr의 바이트 단위 길이        connect : 연결 요청          $ int connect(int sockfd, const struct sockaddr *serv_addr, socklen_t addrlen)sockfd : 소켓 디스크립터serv_addr : 통신을 연결할 상대에 대한 정보addrlen : serv_addr의 길이        recv / send : 소켓으로 송수신          $ int send(int sockfd, const void *msg, size_t msg_len, int flags)$ int recv(int sockfd, void *msg, size_t msg_len, int flags)        close : 소켓 닫기          $ int close(int fd)      UDP 소켓 다루기  socket : 소켓을 생성  bind : 호스트의 IP 주소를 소켓에 연결  sendto : 메세지 송신          $ int sendto(int sockfd, const void *msg, size_t len, int flags, const struct sockaddr *to, socklen_t tolen)socketfd : 소켓 디스크립터msg : 송신할 메세지를 저장할 공간len : 전송할 메세지의 길이flags : 송신 방식 결정 -&gt; 일반적으로 0to : 메세지를 송신할 상대 호스트의 정보tolen : to의 바이트 단위 크기        recvfrom : 메세지 수신          $ int recvfrom(int sockfd, void *buf size_t, int flags, struct sockaddr *from socklen_t *fromlen)buf : 수신할 메세지를 저장할 공간len : buf의 크기flogs : 수신 방식 결정 -&gt; 일반적으로 0        from : 메세지를 송신한 호스트의 정보          fromlen : from의 바이트 단위 크기        close : 소켓 닫기",
      "url": "/2024/12/29/System_Programming-03.html"
    },
  
    {
      "title": "System Programming - 02",
      "tags": "Computer_Engineering",
      "desc": "System Programming - 02 - 2024년12월28일  tag : Computer_Engineering|",
      "content": "System Programming - 02 - 2024년12월28일  tag : Computer_Engineering|프로세스와 스레드 다루기프로세스  프로세스 : 실행 중인 프로그램  프로세스 제어 블록(PCB)          PID(PPID), 레지스터, 스케줄링 정보, 메모리 정보, 사용한 파일 정보, 입출력장치 정보        리눅스 프로세스(태스크) 상태 확인          R(Running) : 실행 상태      S(Sleeping) : 대기 상태      W(Waiting) : 준비 상태      S(Stopped) : 종료 상태      Z(Zombie) : 프로세스 종료 후 자원이 반환되었지만 커널 영역에 프로세스가 남아있는 상태      &gt; 자식 프로세스가 종료되었을 때 부모 프로세스가 종료를 처리해주지 않으면-&gt; 자식 프로세스는 Zombie Process가 된다.스레드  스레드 : 프로세스를 구성하는 실행 흐름의 단위각기 다른 스레드 ID, 프로그램 카운터, 레지스터, 스택  스레드와 프로세스 차이          프로세스 간에는 기본적으로 자원을 공유하지 않고, 스레드 간에는 프로세스의 자원을 공유함            프로세스 간에도 자원 공유가 가능함 : IPC(Inter-Process Communication)    스레드 조인 : 스레드 실행이 종료될 때까지 대기(스레드 종료 처리)  스레드 떼어내기 : 종료 시 자동으로 자원 해제뮤텍스와 세마포      뮤텍스 초기화 / (락/언락) / 락 여부 확인 / 삭제        세마포          unnamed semaphore                  공유 메모리에 대한 동기화 수행 시 활용공유 메모리의 일부를 세마포로 간주 접근                    named semaphore                  일반적인 자원에 대한 동기화 수행 시 활용(세마포에 ID를 부여) 세마포를 파일과 같이 간주, 접근                    공유 메모리 기반 IPC  공유 메모리          다수의 프로세스가 공유 가능한 메모리 영역공유하는 메모리를 읽고 씀으로써 프로세스간 통신이 가능      파이프 기반 IPC  pipe          단방향 IPC 도구한 쪽에서는 파이프를 읽고, 한 쪽에서는 파이프에 쓴다.주로 Stream 형태의 데이터를 주고받을 때 사용파이프는 일종의 파일 : file descriptor를 기반으로 읽고 쓰기 수행      &gt; 파이프에 더 이상 쓸 공간이 없을 때 쓰면 block  &gt; 파이프가 비어 있을 때 읽기를 시도하면 block  &gt; 지나치게 큰 값(리눅스의 경우 4kb 이상)쓰기 지양시그널 다루기  시그널 : 소프트웨어 인터럽트          프로세스에게 특정 이벤트가 발생했음을 알리는 수단시그널마다 기본 동작이 정해져 있고, 처리가 가능한 것들이 있다.        시그널 집합 관리 : 시그널 마스킹",
      "url": "/2024/12/28/System_Programming-02.html"
    },
  
    {
      "title": "System Programming - 01",
      "tags": "Computer_Engineering",
      "desc": "System Programming - 01 - 2024년12월27일  tag : Computer_Engineering|",
      "content": "System Programming - 01 - 2024년12월27일  tag : Computer_Engineering|파일 다루기파일 디스크립터와 파일 포인터  File Descripter : 저수준에서 파일을 식별하는 정보          고정된 파일 디스크립터 값              0번 : 표준 입력        1번 : 표준 출력        2번 : 표준 에러              Parameter          pathname : 파일 경로flag : 접근 플래그mode : 파일 생성시 모드fd : 닫고자 하는 파일 디스크립터stream : 파일 포인터        권환          읽기 4쓰기 2실행 1        파일 포인터 : FILE*타입          파일을 스트림처럼 읽고 쓰기 위한 자료형 -&gt; 편리한 함수 기능 많음파일 디스크립터가 저수준에서 파일을 식별하는 정보라면 파일 포인터는 고수준 파일 식별 정보      파일 입력  read : 파일 읽기          stream : 파일 포인터  format : 입력 포멧s : 문자열 버퍼size : 버퍼 크기ptr : 저장 결과를 담을 주소(저장 결과 포인터)nmemb : 입력할 item 개수      파일 출력  파일 쓰기 : write          c : 출력할 문자offset : whence값 기준 떨어져있는 위치whence : 기준점(SEEK_SET : 파일의 시작 기준, SEEK_END : 파일의 끝 기준, SEEK_CUR : 현재 파일 위치 기준)      디렉터리 다루기  생성 : mkdir  비어있는 디렉터리 삭제하기 : rmdir  열기 : *opendir  열린 디렉터리 닫기 : clodesir  디렉터리 읽기 : dirent *readdir하드 링크와 심볼릭 링크  하드 링크 파일 : 하드 링크 생성 시 같은 아이노드를 공유하는 하드 링크 파일이 생성됨  심볼릭 링크 파일 : 심볼릭 링크 생성 시 원본 파일을 가리키는 새로운 아이노드를 만든다.1. 하드링크는 같은 아이노드 번호를 가지는 파일 생성2. 심볼릭 링크는 원본 파일을 가리키는 파일 생성1. 하드링크는 같은 아이노드 번호를 가지는 파일 생성2. 심볼릭 링크는 원본 파일을 가리키는 파일 생성- 하드링크 생성하기$ ln 원본파일이름 하드링크파일이름- 심볼릭 링크 생성하기$ ln -s 원본파일이름 심볼릭링크파일이름파일 속성 다루기  파일 : 보조기억장치의 의미있는 정보의 집합          구성요소 : 이름, 실행하기 위한 정보, 부가 정보접근 단위 : block[섹터 단위가 아님]      파일 메모리 매핑  파일 메모리 매핑          프로세스 메모리 영역에 파일의 내용 일부에 대응시키는 것디스크에 있는 파일에 읽고 쓰는 것(파일 입출력)이 아니라 프로세스 메모리 영역에 읽고 쓰기두 개 이상의 프로세스가 같은 영역을 매핑할 경우 다른 프로세스와의 통신 가능        mmap          addr : 매핑할 메모리 주소에 대한 힌트(NULL일 경우 임의의 위치에 매핑)Length : 매핑할 바이트 단위 길이prot : 메모리 보호 모드flags : 매핑 형태와 동작 방식fd : 파일 디스크립터offset : 매핑을 시작할 위치      ",
      "url": "/2024/12/27/System_Programming-01.html"
    },
  
    {
      "title": "Network - 05",
      "tags": "Computer_Engineering",
      "desc": "Network - 05 - 2024년12월26일  tag : Computer_Engineering|",
      "content": "Network - 05 - 2024년12월26일  tag : Computer_Engineering|응용 계층DNS  네트워크 상에서 호스트를 특정 지을 수 있는 주소 : MAC 주소, IP 주소  DNS 레코드(자원 레코드)          A 레코드      AAAA 레코드      CNAME 레코드      NS 레코드      SOA 레코드      자원과 자원의 식별  네트워크 상에서 자원의 식별자 : Uniform Resource Identifier  URL : 위치 기반 자원 식별(Locator)      URN : 이름 기반 자원 식별(Name)    URL 구성요소          scheme : 일반적으로 프로토콜 이름 명시      authority      path : 자원이 있는 경로      query : key:value 형태로 서버에 전달할 문자 형태의 파라미터      fragment : 자원의 조각(fragment)를 가리키는 데에 사용      웹 서버와 웹 어플리케이션 서버  서버          웹 서버      웹 어플리케이션 서버(WAS)        서버가 응답해야 하는 자원          정적인 자원      동적인 자원      HTTP의 특성  요청-응답 기반 클라이언트-서버 구조 프로토콜  미디어-독립적 프로토콜 : 어떤 형태의 데이터도 HTTP 메세지로 보낼 수 있음  비연결성 프로토콜 : 다수의 클라이언트가 연결을 시도할 경우 연결을 유지하는 동안 서버의 자원소모가 큼  스테이트리스 프로토콜 : 서버는 클라이언트의 상태를 기억하지 않는다.  지속 연결 프로토콜 : 하나의 연결을 사용해 여러 개의 HTTP 요청/응답 주고받기  HTTP 버전별 특성          HTTP 0.9 : 단일한 요청 방법(GET 메서드), 비지속 연결, 별다른 기능 X      HTTP 1.0 : 다양한 요청 방법과 헤더 추가      HTTP 1.1 : 지속 연결 기능 추가      HTTP 2.0 : 요청 순서대로 응답을 반환할 필요 없음, 헤더 압축      HTTP 3.0 : UDP 기반 프로토콜인 QUIC로 변경      HTTP 메세지 개관  HTTP 메서드 : 서버에게 요청할 동작          GET : 자원 조회      POST : 요청할 데이터 처리      PUT : 자원 덮어쓰기      PATCH : 자원 부분 변경      DELETE : 자원 삭제        멱등성 : 여러 번 동일한 요청을 보내도 첫 요청 결과와 같은가캐시 가능성 : 응답 결과를 캐시해서 사용할 수 있는가  응답 코드          2XX : 성공      3XX : 리다이렉션      4XX : 클라이언트 오류      5XX : 서버 오류      HTTP 헤더  header-field = field name “:” field-value  대표적인 헤더 정보          HOST : 요청 호스트에 대한 호스트명 + 포트 정보      Date : 메세지 생성 시간      Referer : 직전에 머물렀던 URL      User-Agent : 클라이언트 소프트웨어, 브라우저 명칭과 정보      Server : 서버 소프트웨어 명칭과 정보      Connection : keep-alive 일 경우 킵 얼라이브      Location : 리다이렉트시 이동할 경로      Content-Type : HTTP 요청 및 응답에서 사용될 컨텐츠의 유형      Content-Encoding : 데이터의 인코딩/압축 방식      Content-Length : 데이터의 바이트 단위 길이      Content-Language : 데이터의 언어      캐시  서버의 지연을 줄이기 위해 웹 페이지, 이미지 등의 자원 사본을 임시 저장하는 웹 기술      Last-Modified : 해당 자원이 언제 마지막으로 변경되었는지를 알려주는 헤더        캐시된 자원의 변경  &gt; 클라이언트는 요청시 if-modified-since 헤더로 특정 시점 이후 자원 변경 여부를 묻는다.  &gt; 만일 변경되었다면 다시 다운로드, 아니면 서버는 304 Not Modified 응답을 보낸다.&gt; Etag를 통해 자원의 변경 여부 감지 기능  &gt; 서버는 캐시된 자원이 Etag라는 식별 문자를 붙이고,  &gt; 클라이언트는 If-None-Match 헤더를 통해 해당 Etag가 변경되었는지를 물어본다쿠키  서버로부터 받은 정보를 클라이언트 측(웹 브라우저)에 임시 저장되는 이름=값 형태의 데이터유효 기간이 있음쿠키를 전송할 도메인과 경로가 정해져 있음  cookie 의 보안기능          Secure : Https인 경우에만 전송      HTTPOnly : 자바스크립트에서 접근 불가      컨텐츠 협상  클라이언트가 원하는 컨텐츠를 받을 수 있도록 서버에게 부탁하는 기능",
      "url": "/2024/12/26/Network-05.html"
    },
  
    {
      "title": "Network - 04",
      "tags": "Computer_Engineering",
      "desc": "Network - 04 - 2024년12월25일  tag : Computer_Engineering|",
      "content": "Network - 04 - 2024년12월25일  tag : Computer_Engineering|전송 계층  전송 계층의 역할          응용 계층의 어플리케이션 프로세스 식별      네트워크 계층의 신뢰성/연결성 확립      포트  포트 번호는 16비트로 표현 가능      포트 범위 : 0번부터 65535번까지    서버 : 일반적으로 잘 알려진 포트와 등록된 포트로 동작      클라이언트 : 일반적으로 동적 포트로 동작    NAT : 공인 IP주소와 사설 IP 주소 간의 변환 가능          하나의 공인 IP 주소를 여러 사설 IP 주소가 공유 가능IP 주소 부족 문제 해결      TCP와 UDP      MSS : TCP 세그먼트로 보낼 수 있는 최대 크기    TCP 세그먼트 구조          출발지 포트      목적지 포트      순서 번호 : 송수신되는 세그먼트 데이터 첫 바이트에 부여되는 번호      확인 응답 번호 : 순서 번호에 대한 응답 (다음으로 수신받길 기대하는 바이트 번호)      제어 비트                  ACK : 세그먼트 승인을 나타내는 비트SYN : 연결 수립을 위한 비트FIN : 연결을 끝내기 위한 비트RST : 연결을 리셋하기 위한 비트                    윈도우 : 수신지 윈도우 크기. 한 번에 수신 받고자 하는 양(흐름 제어에서 언급)        UDP는 IP 패킷을 감싸는 껍데기일 뿐          비연결성/비신뢰성 프로토콜TCP의 재전송/흐름 제어/혼잡 제어 등의 기능 없음      TCP 연결  TCP는 연결형 프로토콜          연결 설정[ Three-way handshake ]      데이터 송수신      연결 종료      TCP 상태  TCP의 연결형 프로토콜    &gt; CLOSED : 아무런 연결이 없는 상태&gt; LISTEN / SYN-SENT / SYN-RECEIVED : 연결 수립 도중 사용되는 상태&gt; ESTABLISHED : 연결되어 있는 상태  &gt; FIN-WAIT-1 / FIN-WAIT-2 / CLOSE-WAIT / CLOSING / LAST-ACK / TIME-WAIT : 연결 해제시 사용되는 상태      TCP 재전송 기능  TCP는 신뢰성 프로토콜  무엇인가를 확실히 전송했다는 보장이 있으려면      재전송 기반의 오류 제어 : 잘못 전송된 경우 재전송    흐름 제어 : 받을 수 있을 만큼만 받기    혼잡 제어 : 보낼 수 있는 상황에서만 보내기    언제 잘못 되었음을 인지할까?      중복된 ACK 세그먼트를 수신했을 때    타임아웃이 발생했을 때    TCP 오류 제어          TCP는 재전송 기반의 오류 제어를 수행재전송을 기반으로 잘못된 전송을 바로잡는 것 : ARQ(자동 재전송 요구)        ARQ          Stop-and-Wait ARQ      Go-Back-N ARQ      Selective Repeat ARQ      TCP의 혼잡 제어와 흐름제어  송신 버퍼 : 어플리케이션 계층에서 전송할 데이터 임시 저장  수신 버퍼 : 네트워크 계층에서 수신할 데이터 임시 저장      버퍼 오버플로우 : 일부 데이터가 처리되지 않을 수 있음    윈도우 : 파이프라이닝 가능한 순서번호 범위  윈도우 크기 : 확인 응답 받지 않고도 한번에 보낼 수 있는 최대 양      RTT(Round Trip Time) : 메세지를 전송한 뒤 그에 대한 답변을 받는 시간    TCP 혼잡 제어          기본 동작 형태 : AIMD(Additive Increase Multicative Decrease)느린 시작 : 특정 임계치(sshresh)값과 같아지면 혼잡 회피 수행혼잡 회피 : 세 번의 중복 세그먼트가 발생했을 경우 빠른 회복 수행빠른 회복 : 세 번의 중복 ACK 세그먼트가 수신되었을 때 느린 시작을 건너뛰고 혼잡 회피를 수행하는 알고리즘      ",
      "url": "/2024/12/25/Network-04.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 010",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 010 - 2024년12월25일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 010 - 2024년12월25일  tag : Discovering_Statistics_Using_R|R|여러 평균의 비교: 분산분석(GLM 1)분산분석에 깔린 이론상승된 오류율  조건이 셋 이상인 상황에서 t 검정을 수행하면 안되는 이유    &gt; if 실험 조건이 세 가지이고, 모든 그룹 쌍에 대해 각각 t 검정을 수행한다면, 총 3번의 검정이 필요하다.&gt; 세 t 검정 모두 .05 수준으로 유의성을 판정한다고 하면, 각 검정에서 제1종 오류를 범하지 않을 확률은 .95이다.&gt; 만일 세 검정이 독립이라고 하면 제1종 오류를 범하지 않을 전체 확률은 .95^3 ==.857  이다.&gt; 이 결과로 제1종 오류를 범할 확률은 5% 에서 14.3%로 증가했다.        집단별 오류 = \\(1 - (0.95)^n\\) [ n : 자료에 대해 수행한 검정들의 수 ]F 값의 해석  분산분석은 F 통계량 또는 F-ratio라는 값을 산출한다.          F는 모형과 오차의 비        분산분석은 총괄검정(omnibus test)에 해당한다.          즉, 분산분석은 어떤 그룹이 영향을 받았는지에 대한 구체적인 정보를 제공하지 않는다.      총제곱합(\\(SS_T\\))  \\(SS_T\\) : 관측자료와 Y의 평균의 차이들을 사용  관측된 각 자료점과 총평균의 차이를 계산하고 그 차이들을 제곱해서 모두 합한 것\\(SS_T = \\sum_{i=1}^{N}{(x_i - \\bar{x_총}^2)}\\)분산과 제곱합의 관계가 \\(s^2 = \\frac{SS}{N-1}\\) 이므로 \\(SS = s^2(N-1)\\) 이다.모형제곱합(\\(SS_M\\))      \\(SS_M\\) = Y의 평균과 모형(그룹 평균)의 차이들을 사용        과정          그룹마다 그룹 평균과 총평균의 차이를 계산한다.      그룹마다 그 차이를 제곱한다.      그룹마다 차이 제곱에 그룹의 참가자 수를 곱한다.      각 그룹의 결과를 모두 더한다.      \\[SS_M = \\sum_{n=1}^{k}{(\\bar{x}_k - \\bar{x}_총)^2}\\]잔차제곱합(\\(SS_R\\))  \\(SS_R\\) = 관측자료와 모형(그룹 평균)의 차이들을 사용  \\[SS_R = SS_T - SS_M\\]    \\[SS_R = \\sum_{i=1}^{n}{(x_{ik} - \\bar{x_k}^2)}\\]    \\[SS_R = \\sum{s_k^2(n_k - 1)}\\]  평균제곱  \\(MS_M = \\frac{SS_M}{df_M}\\) # 모형이 설명하는 변동의 평균량  \\(MS_R = \\frac{SS_R}{df_R}\\) # 가외 변수들이 설명하는 변동의 평균량F 비  모형이 설명하는 변동과 비체계적 요인들이 설명하는 변동의 비\\(F = \\frac{MS_M}{MS_R}\\)  F 비가 1보다 작다는 것은 해당 모형의 효과는 유의하지 않다는 점          \\(MS_R &gt; MS_M\\) 이는 곧 비체계적 변동이 체계적 변동보다 많다는 뜻이기 때문확신을 가지려면 관측된 F비를 그룹 평균들이 같을 전적으로 우연히 얻을 수 있는 최대 F비와 비교해 봐야 한다.      분산분석의 가정들  F 통계량의 신뢰성을 위한 가정          실험 조건의 분산들이 서로 상당히 비슷해야하고      관측들이 서로 독립이어야 하며      종속변수가 적어도 구간 척도에서 측정된 것이어야 한다.      분산의 동질성  t 검정에서처럼, 분산분석은 그룹 분산들이 같다고 가정 -&gt; 이 가정은 레빈 검정으로 검사할 수 있다.이 레빈 검정은 관측값과 그 관측값이 속한 표본의 평균 또는 중앙값의 차이의 절댓값에 대한 분산분석 검정이다.If 레빈 검정의 결과가 유희하다면 분산들이 유의하게 서로 다르다고 말할 수 있으며, 분산분석의 여러 가정 중 하나가 깨진 것이므로문제를 해결한 후 분석을 진행해야 한다.  분산분석의 가정들이 깨졌을 때 몇 가지 해결책          웰치의 F      분포의 정규성 가정이 깨졌을 때는, 윌콕스의 강건한 방법들을 사용      계획된 대비  계획된 대비(planned contrast) : 모형이 설명하는 변동을 더 작은 부분들로 나누는 것  사후검정(post hoc test) : 모든 그룹을 비교하되 집단별 오류율이 .05보다 커져서는 안 된다는 좀 더 엄격한 허용 기준을 적용하는 것수행할 대비의 선택  계획된 대비의 3가지 규칙          실험에 대조군이 존재한다면, 보통의 경우 그 대조군은 다른 실험군들과의 비교를 위한 것이다.      각 대비는 반드시 변동의 두 ‘조각’만 비교해야 한다.      한 대비에서 어떤 한 그룹을 특정해서 선택했다면(=single out), 그 다음의 대비들에서 그 그룹을 다시 사용해서는 안 된다.      가중치를 이용한 대비 정의  합당한 비교를 선택해야 한다.          한 대비에서 오직 두 개의 변동 조각만 비교해야 한다는 점과, 한 비교에서 특정하게 선택된 그룹은 이후의 대비에서 사용하지 말아야 한다        양의 가중치가 배정된 그룹들이 음의 가중치가 배정된 그룹들과 비교된다.          한 변동 조각에는 양의 가중치를, 다른 변동 조각에는 음의 가중치를 배정해야 한다.        한 비교에서 가중치들의 합은 0이어야 한다.          한 대비에서 비교하는 모든 그룹의 가중치를 합하면 0이 되어야 한다.        비교에 포함되지 않는 그룹에는 무조건 가중치 0을 배정해야 한다.          가중치가 0인 그룹은 모든 계산에서 제외된다.        주어진 한 대비에서, 한 변동 조작의 그룹(들)에 배정된 가중치들의 크기(절댓값)는 다른 변동 조각에 있는 그룹의 개수와 같아야 한다.  비직교 대비  표준 대비  다항 대비: 추세분석",
      "url": "/2024/12/25/Discovering-Statistics-Using-R-010.html"
    },
  
    {
      "title": "Network - 03",
      "tags": "Computer_Engineering",
      "desc": "Network - 03 - 2024년12월24일  tag : Computer_Engineering|",
      "content": "Network - 03 - 2024년12월24일  tag : Computer_Engineering|네트워크 계층IP  네트워크 계층          물리 계층과 데이터링크 계층 -&gt; LAN에 국한된 통신LAN을 넘어서기 위한 계층네트워크 간 통신이 가능한 계층 -&gt; 라우팅단편화가 이루어지는 계층        MAC 주소 : 물리주소      IP 주소 : 논리주소    단편화 : 패킷의 크기를 MTU(Maximum Transmission Unit) 이하로 유지          MTU 크기 이하로 단편화된 패킷들은 목적지에서 재조합        IPv4 헤더          식별자 : 패킷에 할당된 번호(재조합 시 사용)플래그 : 부가 정보(미사용, Don’t Fragment, More Fragment 비트)단편화 오프셋 : 단편화되기 전 데이터가 얼마나 떨어져 있는가TTL(Time To Live) : 패킷의 수명, 라우터를 거칠 때마다 1감소프로토콜 : 상위 계층의 프로토콜        IPv4 주소          4바이트(32비트)로 표현가능한 옥텟은 0 ~ 255 범위의 네 개의 십진수로 표기이론적으로 할당 가능한 IPv4 주소 개수 == 2^32개IP주소 부족 문제        IPv6 주소          16바이트(128비트)로 표현 가능이론적으로 할당 가능한 IPv4 주소 개수 == 2^128개      ARP  기본적으로 MAC 주소 이전에 IP주소를 사용  ARP : IP 주소를 통해 MAC 주소를 알아내기 위한 프로토콜          과정 : 요청 -&gt; 응답 -&gt; ARP 테이블(ARP 캐시) 갱신      ICMP  IP의 한계      비신뢰성 : 패킷이 목적지까지 제대로 전송한다는 보장이 없는 특성    비연결형 : 호스트 간의 사전 연결 수립이 없는 특성    ICMP : IP의 비신뢰성과 비연결형 특성을 보완하기 위한 네트워크 계층 프로토콜          IP 패킷의 전송 과정에 대한 피드백 메세지 제공[오류 보고, 네트워크 진단 정보]IP의 한계를 보완할 뿐 완전히 해결하는 것은 아니다.      IP 주소  IP주소의 구성 : 네트워크 주소, 호스트 주소      MAC 주소의 구성 : 제조사 번호, 일련번호    클래스리스 주소체계          클래스풀 주소 체계보다 더 정교히 네트워크를 나누는 방법오늘날 주로 사용하는 방식네트워크와 호스트를 구분하기 위해 서브넷 마스크 이용        CIDR 표기          서브넷 마스크 상의 1의 개수를 IP주소/숫자로 표기      IP주소의 분류  공인 IP 주소 : 인터넷 사용할 때 사용하는 고유한 주소      사설 IP 주소 : 사설 네트워크 내에서 사용하는 고유하지 않은 주소    NAT : 공인 IP 주소와 사설 IP 주소 간의 변환가능          하나의 공인 IP 주소를 여러 사설 IP주소가 공유 가능IP주소 부족 문제 해결        정적IP주소 : 정적 할당된 IP 주소(고정)      동적IP주소 : 동적 할당된 IP 주소(유동적)    DHCP : 동적 IP주소를 할당하기 위한 프로토콜          DHCP 서버에 의해 동적으로 IP주소 할당(==IP 주소 임대)      라우팅  라우팅 : 패킷이 이동할 최적 경로 설정, 해당 경로로 패킷 이동  라우팅 프로토콜 : 라우팅을 수행하는 방법      홉 : 라우터와 라우터 간의 패킷 이동 과정    라우팅 테이블 : 특정 목적지까지 도달하기 위한 정보를 명시하는 표와 같은 정보          대표적인 정보 : 목적지 주소, 서브넷 마스크, 게이트웨이, 인터페이스, 메트릭롱기스트 프리픽스 매치 : 여러 라우팅 테이블 항목과 일치할 경우 가장 길게 일치하는 항목 선택 후 패킷 전송        라우팅 테이블이 만들어지는 과정          정적 라우팅 : 수동으로 라우팅 테이블 항목 채워넣기      동적 라우팅 : 자동으로 라우팅 테이블 항목 채워넣기(라우팅 프로토콜)      ",
      "url": "/2024/12/24/Network-03.html"
    },
  
    {
      "title": "Network - 02",
      "tags": "Computer_Engineering",
      "desc": "Network - 02 - 2024년12월23일  tag : Computer_Engineering|",
      "content": "Network - 02 - 2024년12월23일  tag : Computer_Engineering|네트워크 엑세스 계층이더넷  WAN : 인터넷  LAN : 이더넷          현대 (유선) LAN에서 가장 대중적으로 사용되는 기술물리 계층, 데이터 링크 계층(네트워크 엑세스 계층) 스펙/프로토콜 정의        이더넷 기술          물리 계층 : 이더넷으로 통신이 가능한 테이블데이터 링크 : 이더넷 프레임        이더넷 프레임 : 이더넷 네트워크에서 주고받는 데이터 형식          프리앰블 : 이더넷 프레임의 시작을 알리는 비트열, 송수신간의 동기화      목적지/송신지 MAC주소 : 물리적 주소, 네트워크 장치(NIC)마다 할당된 고유한 주소                  NIC(네트워크 인터페이스) : 연결 매체를 통해 받은 신호를 컴퓨터에게 전달, 네트워크에 연결하기 위한 하드웨어                    이더타입/길이      페이로드 : 운반할 데이터      FCS : 오류 검출을 위한 CRC 값이 명시되는 필드      허브와 CSMA/CD  허브 : 물리 계층의 장비          MAC 주소를 사용하지 않는다.반이중 통신 : 송신 혹은 수신이 한 번에 한 번만 이루어지는 통신전이중 통신 : 송신과 수신이 동시에 이루어지는 통신        반이중 이더넷의 충돌을 해결하기 위한 CSMA/CD          CS : Carrier Sense                  캐리어(반송파) 감지 : 메세지 전송 전 현재 전송 중인 것이 있는지 확인                    MA : Multiple Access                  다중 접근 : 두 개 이상의 호스트가 동시에 네트워크에 접근(충돌 발생)                    CD : Collision Detection                  충돌 감지 : 잼 신호(jam signal)를 보낸 뒤 임의의 시간 동안 대기 후 재전송                      허브의 특성 정리          전달 받은 신호를 모든 포트로 내보냄      연결된 모든 호스트가 충돌 도메인      반이중 모드로 통신      스위치와 VLAN  스위치의 특성전달 받은 신호를 목적지 포트로만 포트로 내보내고목적지 호스트가 연결된 곳만 충돌 도메인에 속해 있으며전이중 모드로 통신하는 데이터 링크 계층의 장비  MAC 주소 학습 기능 : 포트에 연결된 호스트와 MAC 주소의 관계를 기억하는 스위치 기능          학습과정              플러딩 : 허브와 같이 모든 포트에 프레임 전송포워딩과 플터링 : 어떤 포트로 내보낼지 내보내지 않을지 결정에이징 : 특정 시간이 지나면 MAC 주소 테이블 항목 삭제              VLAN : 스위치 기능, 가상의 LAN, 물리적 위치에 관계 없이 특정 LAN에 속할 수 있음",
      "url": "/2024/12/23/Network-02.html"
    },
  
    {
      "title": "Network - 01",
      "tags": "Computer_Engineering",
      "desc": "Network - 01 - 2024년12월22일  tag : Computer_Engineering|",
      "content": "Network - 01 - 2024년12월22일  tag : Computer_Engineering|Network  인터넷 : 네트워크의 네트워크  컴퓨터 네트워크 : 여러 장치들이 서로 정보를 주고받을 수 있는 통신망          구성요소 : 노드, 메세지, 간선(통신 링크)              노드 : 종단 시스템, 호스트, 메세지를 최초로 송신, 생성하는 대상, 주소를 통해 위치 특정[유니캐스트, 브로드캐스트, 멀티캐스트]              클라이언트 : 요청을 보내는 호스트      서버 : 응답을 보내는 호스트    간선(통신 링크)          유선 케이블(트위스티트 페어 케이블, 광케이블)무선(와이파이)        메세지 : 주고받은 정보  WAN : 원거리를 연결한 네트워크  LAN : 근거리를 연결한 네트워크프로토콜과 캡슐화  패킷 교환 네트워크          주고받는 정보를 패킷 단위로 주고받는 네트워크패킷이란 패킷 교환 네트워크에서 주고받는 데이터 단위        회선 교환 네트워크          정해진 회선(circuit)으로만 통신하는 네트워크사전에 연결 수립 작업, 다른 호스트는 도중에 끼어들 수 없음장점 : 전송률 보장단점 : 회선 이용률 저하        패킷 구성 요소          header : 패킷에 붙일 부가 정보      payload : 패킷에 보낼 정보      trailer : 패킷 뒤에 붙일 부가정보        프로토콜          장비 간 정보를 주고받을 규칙이나 방법호스트 간에 합의된 의사소통 규칙        OSI 모델          계층 : [물리, 데이터링크, 네트워크, 전송, 세션, 표현, 응용 계층]        TCP/IP 모델          계층 : [네트워크 엑세스, 인터넷, 전송, 응용 계층]        캡슐화 : 상위 계층으로부터 내려받은 패킷을 payload로 삼아, 상위 계층으로부터 받은 정보에 프로토콜에 걸맞는 헤더를 덧붙이는 것  역캡슐화 : 캡슐화 과정에서 붙인 헤더를 각 계층에서 제거하는 것네트워크의 성능  트래픽 : 특정 시간 동안 네트워크 내 정보 흐름  대역폭(bandwidth) : 네트워크 트래픽을 수용할 수 있는 용량, 송수신 가능한 최대 데이터 양  패킷 손실 : 얼마나 많은 패킷이 송수신 과정에서 손실되었는가?",
      "url": "/2024/12/22/Network-01.html"
    },
  
    {
      "title": "Operating System-05",
      "tags": "Computer_Engineering",
      "desc": "Operating System-05 - 2024년12월21일  tag : Computer_Engineering|",
      "content": "Operating System-05 - 2024년12월21일  tag : Computer_Engineering|파일 시스템  파일과 디렉터리(폴더)를 관리하는 커널의 한 부분다양한 파일 시스템이 있고, 여러 파일 시스템을 동시에 사용할 수 있음파일과 디렉터리  파일 : 보조기억장치의 의미있는 정보의 집합      구성요소 : 이름, 실행하기 위한 정보, 부가 정보    파일(+ 디렉터리) 접근 단위 : 블록(block) [ 섹터 단위로 접근하지 않음 ]    디렉터리 : 많은 운영체제는 디렉터리를 파일과 동일하게 간주한다.      구성요소 : 파일 이름, 위치를 유추할 수 있는 정보, (파일 속성)  파일 시스템이 만들어지기까지  파티셔닝 : 보조기억장치의 영역을 구획하는 작업  파티션 : 보조기억장치에서 구획된 영역  formatting : 파일 시스템을 만드는 작업  마운트 : 파일 시스템에 접근할 경로 설정, 파일 시스템을 다른 파일 시스템에 편입파일 시스템 종류와 속성  FAT 기반 파일 시스템 : FAT(File Allocation Table)를 활용하는 파일 시스템  i-node 기반 파일 시스템 : i-node라는 색인 블록을 활용한 파일 시스템          i-node : 사실상 파일의 모든 정보를 담고 있다.        NFTS : 윈도우 운영체제에서 주로 사용되는 파일 시스템  APFS : macOS, ios, watchOS, tvOS 에서 주로 사용되는 파일 시스템      ext2, ext3, ext4, xfs : 리눅스 운영체제에서 주로 사용되는 파일 시스템    journaling file system          파일 시스템에 크래쉬가 발생했을 때 빠르게 복구하기 위한 방법              작업 직전 파티션의 로그 영역에 로그를 남긴다.        로그를 남긴 후 작업을 수행        작업이 끝났다면 로그를 삭제한다.            ",
      "url": "/2024/12/21/Operating_System-05.html"
    },
  
    {
      "title": "Operating System-04",
      "tags": "Computer_Engineering",
      "desc": "Operating System-04 - 2024년12월20일  tag : Computer_Engineering|",
      "content": "Operating System-04 - 2024년12월20일  tag : Computer_Engineering|가상 메모리 관리페이징과 페이지 테이블  swapping : 프로세스를 보조기억장치의 일부 영역으로 쫓아내고 당장 필요한 프로세스를 적재하는 메모리 관리 기법  swap-out : 프로세스를 보조기억장치의 일부 영역으로 쫓아내는 것  swap-in : swap-out 된 프로세스를 메모리에 적재하는 것      swap 영역 : swap-out된 프로세스가 적재되는 보조기억장치 영역    연속 메모리 할당 : 프로세스를 메모리에 연속적으로 배치하는 방식          부작용 : 외부 단편화 [ 프로세스들이 실행되고 종료되길 반복하며 빈 공간이 생기는 메모리 낭비 현상 ]        페이징          물리 메모리를 프레임이라는 일정한 크기로 나누고프로세스를 페이지라는 일정한 크기로 나눈 뒤페이지를 프레임에 매핑하는 메모리 관리 방식        가상 메모리          프로세스의 일부만을 적재하여 실제 물리 메모리보다 큰 프로세스를 실행하는 기술페이징은 현대 운영체제에서 가장 대중적으로 사용되는 가상 메모리 관리 기법        페이지 테이블 : 프레임과 페이지의 매핑 정보를 담고 있는 표 형태의 데이터          페이지 테이블 베이스 레지스터(PTBR) : 각 프로세스의 페이지 테이블 위치를 가리키는 레지스터TLB(Translation Lock-aside Buffer) : 페이지 테이블의 캐시 메모리유효 비트(valid bit) : 페이지 테이블 내 정보              페이지 폴트(page fault) : 접근하려는 페이지가 보조기억장치에 있을 경우        작업 내역 백업  페이지 폴트 루틴 실행 - 접근하려는 페이지 적재유효 비트 1로 변경접근하려는 페이지 접근                보호 비트(protection bit) : 접근하려는 페이지의 권한참조 비트(reference bit) : 접근한 적 있는 페이지인가?수정 비트(modify bit / dirty bit) : 쓰기 작업을 한 적 있는 페이지인가?            요구 페이징, 스래싱  요구 페이징          처음부터 모든 페이지를 적재하지 않고 페이지 폴트가 발생하면 그 때 페이지를 적재한다.        순수 요구 페이징          아무 페이지를 적재하지 않고 실행첫 명령어 실행부터 페이지 폴트 발생적당한 페이지가 적재된 이후부터 페이지 폴트 감소        스래싱          프로세스 실행 시간보다 페이징에 더 많은 시간이 소요되는 문제      페이지 교체 알고리즘  매모리에 적재된 페이지 중 페이지-아웃시킬 페이지를 선정하는 방법좋은 페이지 교체 알고리즘은 페이지 폴트를 적게 일으키는 알고리즘  종류          FIFO 페이지 교체 알고리즘      2차 기회 FIFO 페이지 교체 알고리즘      최적 페이지 교체 알고리즘      LRU 페이지 교체 알고리즘      ",
      "url": "/2024/12/20/Operating_System-04.html"
    },
  
    {
      "title": "Operating System-03",
      "tags": "Computer_Engineering",
      "desc": "Operating System-03 - 2024년12월19일  tag : Computer_Engineering|",
      "content": "Operating System-03 - 2024년12월19일  tag : Computer_Engineering|동기화와 교착상태프로세스 동기화  동기화 : 동시 다발적으로 실행되는 프로세스(&amp; 스래드)          실행 순서와 자원의 일관성을 보장해야 한다.실행 순서 제어 : 프로세스를 올바른 순서로 실행하기상호 배제 : 동시에 접근해서는 안되는 자원에 하나만 접근하기        공유 자원 : 공동의 자원      임계 구역 : 동시에 접근하면 문제가 발생할 수 있는 공유 자원에 접근하는 코드    레이스 컨디션 : 임계 구역을 동시에 실행하여 자원의 일관성이 깨지는 현상뮤텍스와 세마포  동기화 해결의 세 가지 원칙          상호 배제 : 한 프로세스가 임계 구역에 진입했다면 다른 프로세스는 대기해야 함      진행 : 어떤 프로세스도 임계 구역에 진입하지 않았다면 진입이 가능해야 함      유한 대기 : 한 프로세스가 임계 구역 진입을 위해 대기하고 있다면 언젠간 진입이 가능해야 함        뮤텍스 락 : 상호 배제를 위한 동기화 도구  세마포 : 상호 배제 &amp; 실행 순서 제어를 위한 동기화 도구조건변수와 모니터  모니터          공유 자원에 접근하기 위한 인터페이스인터페이스를 통해서만 접근(상호 배제)      교착상태와 해결 방법      교착상태 : 일어나지 않을 사건을 기다리며 무한히 대기하는 현상    조건          상호 배제 : 동시에 자원 사용이 불가능한 경우      점유와 대기 : 자원을 할당받은 채 다른 자원의 할당을 기다리는 경우      비선점 : 강제로 자원을 빼앗을 수 없는 경우      원형 대기 : 자원을 원형으로 대기할 경우        교착상태를 예방하는 방법          교착상태 조건 중 하나 이상을 해결하는 방법      발생 배경 원천 차단      교착상태가 발생하지 않음을 보장할 수 있지만, 여러 부작용이 따르는 방식      ",
      "url": "/2024/12/19/Operating_System-03.html"
    },
  
    {
      "title": "Operating System-02",
      "tags": "Computer_Engineering",
      "desc": "Operating System-02 - 2024년12월18일  tag : Computer_Engineering|",
      "content": "Operating System-02 - 2024년12월18일  tag : Computer_Engineering|프로세스와 스레드커널 영역과 사용자 영역의 프로세스      프로세스 : 실행 중인 프로그램    프로세스 제어 블록(PCB)          PID(PPID), 레지스터, 스케줄링 정보, 메모리 정보, 사용한 파일 정보, 입출력장치 정보        문맥 : 실행을 재개하기 위한 기억해야 할 정보      문맥 교환 : 여러 프로세스들이 번갈아가며 실행되는 원리    코드 영역(텍스트 영역) : 실행 가능한 코드; 기계어로 이루어진 명령어  데이터 영역 : 프로그램이 실행되는 동안 유지할 데이터; BSS 영역 : 프로그램 실행 동안 유지할 데이터 중 초기값 없는 데이터  힙 영역 : 사용자(개발자)가 직접 할당 가능한 영역; 메모리 영역을 할당 했다면 해제하기[직접, 자동으로 설정] -&gt; 안했다면 메모리 누수 발생          낮은 주소에서 높은 주소로 할당        스택 영역 : 임시로 저장되는 영역; 높은 주소에서 낮은 주소로 할당(주소 중복 방지)프로세스 생성과 상태  대표적인 프로세스 상태          생성 상태(new)      준비 상태(ready) : W(Waiting)      실행 상태(running) : R(Running)      대기 상태(blocked) : S(Sleeping)      종료 상태(terminated) : S(Stopped)                  Z : Zombie = 프로세스 종료 후 자원이 반환되었지만 커널 영역에 프로세스가 남아있는 상태                      fork-exec : 계층적 구조로 프로세스가 생성되는 원리          fork : 복사본 만들기exec : 새로운 코드로 대체(덮어쓰기)      스레드  프로세스를 구성하는 실행 흐름의 단위각기 다른 스레드 ID, 프로그램 카운터, 레지스터, 스택  프로세스 간에는 기본적으로 자원을 공유하지 않음  스레드 간에는 프로세스의 자원을 공유CPU 스케줄링프로세스 우선순위와 스케줄링 큐  CPU 스케줄링 ; 운영체제가 공정하고 합리적으로 CPU를 배분하는 방법  대표적인 스케줄링 큐          준비 큐 : CPU 이용을 기다리는 프로세스들의 큐      대기 큐 : 대기 상태 프로세스들의 큐 ( 입출력 요청 )        선점형 스케줄링          프로세스에 자원을 고루 할당 가능문맥 교환 과정의 오버헤드        비선점형 스케줄링          고르지 않은 자원 분배 문맥 교환 과정에서의 오버헤드 적음      CPU 스케줄링 알고리즘  선입 선처리 스케줄링(FIFO) : CPU를 먼저 요청한 프로세스부터 CPU할당, 준비 큐에 삽입된 순서대로 실행되는 비선점형 스케줄링  최단 작업 우선 스케줄링 : 준비 큐 프로세스 중 CPU 이용 시간이 짧은 프로세스부터 실행, 호위효과 방지  라운드 로빈 스케줄링 : 선입 선처리 스케줄링 + 타임 슬라이스, 준비 큐에 삽입된 순서로 실행하되, 타임 슬라이스만큼 실행, 선점형 스케줄링  최소 잔여 시간 우선 스케줄링(SRT) : 최단 작업 우선 스케줄링 + 라운드 로빈 스케줄링, 작업 시간 짧은 프로세스부터 처리하되 타임 슬라이스만큼 돌아감  우선순위 스케줄링 : 프로세스마다 우선순위 부여, 우선순위 높은 순으로 스케줄링          아사(starvation) 현상 : 우선순위 낮은 프로세스의 실행이 계속 연기되는 현상  해결책 : 에이징(aging) = 대기 시간이 길어지면 점차 우선순위를 높이는 방식        다단계 큐 스케줄링 : 우선순위 별로 준비 큐를 여러 개 사용하는 스케줄링  다단계 피드백 큐 스케줄링 : 높은 우선순위 큐에 삽입, 실행이 끝나지 않을 경우 낮은 우선순위 큐에 삽입리눅스의 스케줄링  실시간 정책 스케줄링[우선순위 높음]          SCHED_FIFOSCHED_RR        일반 정책 스케줄링[우선순위 낮음]          SCHED_OTHER/SCHED NORMALSCHED_BATCHSCHED_IDLE        CFS(Completely Fair Scheduler)          비실시간 프로세스를 대상으로 하는 스케줄링 방식vruntime(virtual runtime) : 프로세스가 그 동안 실행한 시간을 정규화한 정보vruntime 이 작은 프로세스를 다음 실행할 프로세스로 삼음      ",
      "url": "/2024/12/18/Operating_System-02.html"
    },
  
    {
      "title": "Operating System-01",
      "tags": "Computer_Engineering",
      "desc": "Operating System-01 - 2024년12월17일  tag : Computer_Engineering|",
      "content": "Operating System-01 - 2024년12월17일  tag : Computer_Engineering|운영체제의 역할  리소스 : 실행에 마땅히 필요한 요소      운영체제 : 자원을 관리하고 할당하는 특별한 프로그램    커널영역 : 운영체제 서비스를 제공받을 수 있는 모드(입출력 가능)  사용자영역 : 운영체제 서비스를 제공받을 수 없는 모드(입출력 불가능)      시스템 콜 : 운영체제 서비스를 제공받기 위해 커널 모드로 전환하는 것[소프트웨어 인터럽트라고 불리기도]    운영체제 핵심 서비스          프로세스 관리      자원 관리 접근 및 할당[Ex. CPU, 메모리, 보조기억장치 &amp; 입출력장치]      파일 시스템 관리 - 파일 시스템      리눅스 설치  Virtualbox 설치  Ubuntu 22.04 설치  virtualbox 실행  새로만들기 -&gt; 이름 설정 -&gt; ISO 이미지 ubuntu에서 받은 .iso파일 지정 -&gt; 무인 설치 건너뛰기 체크[= Skip Unattended Installation] -&gt; 다음  기본메모리, 프로세서 설정은 환경에 맞게[Ex. 2048MB, 2개]  지금 새 가상 하드 디스크 만들기 선택[= Create a Virtual Hard Disk Now, 25GB] -&gt; 다음 -&gt; 완료  시작 버튼 클릭 -&gt; Try or Install Ubuntu 엔터 -&gt; 언어 English 선택 -&gt; Install Ubuntu 선택  English, English -&gt; Continue -&gt; Minimal installation , Download updates while installing Ubuntu -&gt; Continue  Erase disk and install Ubuntu -&gt; install now -&gt; Continue -&gt; seoul 선택 -&gt; Continue  이름과 비밀번호 설정 -&gt; Continue -&gt; Restart Now  환경설정          ubuntu에서 terminal 실행 후 아래 명령어 입력      $ sudo apt install gcc -y$ gcc --version$ sudo apt install git -y$ git --version$ sudo apt-get install vim -y              종료          현재 시스템 상태 저장하기 클릭 후 확인      strace  시스템 콜을 추적하기 위한 도구  설치    $ apt-get update$ apt-get install strace        깊게 공부하고 싶다면 다음 명령어 출력 페이지 공부하기    $ man strace        사용법          만약 ls 라는 명령어에 관한 시스템 콜을 확인하고 싶다면      $ strace ls              Tip    $ ps   # 현재 실행되고 있는 Process 확인$ strace -o output.txt ls   # ls 에 관한 시스템 콜을 output.txt 파일로 출력물을 저장$ strace -t ls   # 타임 스탬프$ strace -tt ls   # 밀리세컨드 타임스탬프$ strace -T ls   # 각 시스템 호출 소요 시간$ strace -c ls   # 요약 결과 출력$ strace -e trace=open,read ls   # 실행 파일의 시스템 호출 결과 필터링      시스템 콜  종류          open/closeread/writefork/exec      &gt; fork : 프로세스 복제하여 자식 프로세스 생성, 프로세스들이 계층적으로 구성되는 원리&gt; exec : 현재 프로세스의 주소 공간을 새로운 프로세스로 덮어쓰기, 자식 프로세스로 하여금 다른 코드를 실행토록 하기            getpid/getppid      &gt; getpid : PID를 반환하는 시스템 콜&gt; getppid : 부모 프로세스 PID를 반환하는 시스템 콜            syslog : 시스템 로그 남기기exit : 실행 중인 프로그램 종료      ",
      "url": "/2024/12/17/Operating_System-01.html"
    },
  
    {
      "title": "Github - 002",
      "tags": "Github",
      "desc": "Github - 002 - 2024년12월16일  tag : Github|",
      "content": "Github - 002 - 2024년12월16일  tag : Github|Merge Conflict 해결방법  Solve Merge Conflict [ 일반적인 상황에서 사용 ]-&gt; Merge 상황에서 해결  Solve Merge Conflict [ 특수한 상황에서 사용 ]-&gt; rebase 방법으로 해결Branch Models  git flow          가장 전통적이고 많이쓰이는 모델각 단계가 명확히 구분되어 배포주기가 주기적인 서비스에 유리. 하지만 복잡        github flow          브랜치 모델의 단순화.CI 의존성이 높고, pull request가 없으면 실수에 대처가 힘듬      $ git branch feature/{새로운 branch 이름}$ git switch feature/{새로운 branch 이름}$ vi {파일명.확장자} # 여기서 main branch 와 다르게 수정$ git add {파일명.확장자}$ git commit$ git push -u origin feature/refactor-fb # u : 새로운 branch에서 push 할 때 한번만 사용하면 OK# 깃허브 리퍼지스토리에 들어가기# Pull requests에 접속 -&gt; Create pull request# feature/{새로운 branch 이름} 선택# Create pull request 선택 후 title, description 잘 짓기 (+ Assignees(담당자) 선택) -&gt; Create pull request# Merge pull request 선택 -&gt; Confirm merge# 작업이 다 끝났으므로# 왼쪽 위 main 클릭 -&gt; View all branches 클릭 -&gt; feature/{새로운 branch 이름} 삭제# git bash에 돌아와서# feature/{새로운 branch 이름} 삭제$ git branch -D feature/{새로운 branch 이름}# github cloud에 있는 최종 데이터를 로컬에 받기$ git pull origin main  gitlab flow          deploy, issue에 대응을 하기 쉽도록 한 모델      stash  작업중인 변경사항 잠시 미뤄두기 [ 새로운 branch 에서 사용 ]  $ git stash$ git stash list     # stash 목록 보기$ git stash pop {번호} # stash 다시 불러오기 {번호}는 특정한 stash 불러올 때만  undo  변경사항 취소하기  $ git restore {파일명.파일확장자}  Unstaging  Stage의 변경사항(blob) Working Directory로 내리기  $ git add {파일명.파일확장자}$ git reset HEAD {파일명.파일확장자}    # HEAD : 마지막의, 최신의  Edit commit message  직전 commit message 수정하기git add -&gt; commit -&gt;  $ git commit --amend  Revert commit  잘못을 인정하고 특정시점으로 되돌리기git add -&gt; commit 인 상황에서  $ git revert --no-commit HEAD~{되돌리고 싶은 commit 숫자}..$ git status   # 삭제 이력 나와있음$ git commit    # 왜 사용했는지 이유를 적어야 함  Issue  github 프로젝트의 다양한 이슈를 관리하기 위한 기능할 일, 버그, 질문 등을 관리하기 위함Label, 상태 관리 등의 업데이트가 잘 이루어져야 원할한 작업 가능template 존재[ 깃허브 repotory -&gt; settings -&gt; issue template ]Issue Labels      Issue의 상태와 종류, 긴급도 등을 표시하기 위함Milestone : 작성된 Issue가 프로젝트의 어떤 주기에서 해결되어야 하는지를 표기Milestone 작성을 통해 해당 Sprint의 달성률과 남은 Issue 파악이 쉬워짐  Projects  github에서 repo issue 기반의 task managementScrum board와 table의 방식 존재.팀의 Admin만 관리 가능(관리자가 관리하는 것이 맞음!!)commit, pull request 등을 통해 자동으로 움직이도록 관리할 수 있음Wiki  repository에서 README.md 에서 더 자세히 설명할 부분이 있을 경우 작성따로 사이트를 만들지 않더라도 해당 프로젝트에 대한 FAQ, Docs 처리 가능프로젝트 중 문서화가 필요한 모든 것들을 담아놓는 공간Daily Scrum, Sprint Retrospection(Liked, Learned, Lacked), Code Convention, Technical Issuespull request [깃허브 상에서]  명령어 지원close, resolve, fix(복수형, 과거형)ex. resolve #1팀 Project REMOTE 과정[main branch 에서 실행]$ git remote -v    # remote 확인보기$ git remote add upstream {팀 repository 주소}   # 관습적으로 팀 이름은 upstream 이라고 부름$ git fetch upstream main   # FETCH_HEAD 라는 공간에 쌓임$ git merge FETCH_HEAD$ git push origin main해야할 것!  직접 프로젝트를 생성하고 협업하는 과정을 처음부터 끝까지 해보기!",
      "url": "/2024/12/16/Github-002.html"
    },
  
    {
      "title": "Computer Structure",
      "tags": "Computer_Engineering",
      "desc": "Computer Structure - 2024년12월15일  tag : Computer_Engineering|",
      "content": "Computer Structure - 2024년12월15일  tag : Computer_Engineering|컴퓨터 구조  컴퓨터의 네 가지 핵심 부품          CPU : 명령어 해석, 실행 담당하는 장치                  레지스터 : 명령어 처리 전후로 값을 임시 저장하는 장치ALU(산술논리연산장치) : 연산을 수행하는 장치 (계산을 담당하는 회로)제어장치 : 명령어를 해석하고 제어 신호를 내보내는 장치                    메인 메모리 &amp; 캐시 메모리 : 실행 중인 프로그램 저장하는 장치 [전원이 꺼지면 삭제]      보조기억장치 : 보관할 프로그램 저장하는 장치 [전원이 꺼져도 저장 유지]      입출력장치 : 컴퓨터 내외부와 정보를 주고받는 장치        (시스템)버스 : 부품 간 정보를 주고받을 통로명령어  소스 코드 : 사람(개발자)가 이해하기 편한 언어 [ 고급 언어 ]          컴파일 : 소스 코드 전체가 컴파일러에 의해 검사, 목적 코드로 변환인터프리트 : 소스 코드 한 줄씩 인터프리터에 의해 검사, 목적 코드로 변환        명령어와 데이터 : 컴퓨터가 이해하기 편한 언어 [ 저급 언어 ]          기계어어셈블리어        오퍼랜드로 연산 코드를 수행해라.          오퍼랜드(operand) : 명령어를 수행할 대상, 대상(데이터)이 직접 명시되기도 하고, 대상의 위치가 명시되기도 함연산 코드(op-code) : 오퍼랜드로 수행할 동작        유효 주소 : 연산 코드에 사용할 데이터가 저장된 위치, 즉 연산의 대상이 되는 데이터가 저장된 위치  주소 지정 : 유효 주소를 찾는 방법, CPU마다 차이가 있음          즉시 주소 지정 : 연산에 사용할 데이터를 오퍼랜드 필드에 직접 명서, 가장 빠른 주소 지정 + 데이터 크기에 제한      직접 주소 지정 : 오퍼랜드 필드에 유효 주소 명시, 오퍼랜드 필드로 표현 가능한 메모리 주소 크기에 제한      간접 주소 지정 : 오퍼랜드 필드에 유효 주소의 주소 명시, 유효 주소 크기에 제한은 없으나, 속도가 비교적 느림      레지스터 주소 지정 : 연산에 사용할 데이터를 저장한 레지스터를 오퍼랜드 필드에 직접 명서, 레지스터 접근은 메모리보다 빠르다!      레지스터 간접 주소 지정 : 연산에 사용할 데이터를 메모리에 저장하고, 그 주소(유효 주소)를 저장한 레지스터를 오퍼랜드 필드에 명시, 메모리 접근은 한 번      데이터  인코딩 : 문자를 0과 1로 이루어진 문자 코드로 변환  디코딩 : 0과 1로 이루어진 문자 코드를 문자로 변환메인메모리 &amp; 캐시 메모리  RAM(Random Access Memory) : 휘발성 저장장치          DRAM : 시간이 지나면 점차 저장된 데이터가 사라지는 RAMSRAM : 시간이 지나도 저장된 데이터가 사라지지 않는 RAMSDRAM : 클럭과 동기화된 RAMDDRSDRAM : 대역폭을 넓혀 속도를 높인 RAM        ROM(Read Only Memory)          Mask ROM : 가장 기본적인 형태의 ROMPROM : 데이터를 한 번 새길 수 있는 ROMEPROM : 지우고 다시 저장 가능한 PROM플래시 메모리 : EEPROM의 발전된, 저렴한 형태, 반도체 기반의 저장장치        엔디안 : 연속해서 저장해야 하는 바이트를 저장하는 순서          빅 엔디안 : 상위바이트부터 저장하는 방식리틀 엔디안 :하위바이트부터 저장하는 방식        물리 주소 : 실제 메모리의 하드웨서 상의 주소  논리 주소 : CPU와 실행 중인 프로그램이 사용하는 주소  논리 주소와 물리 주소 간의 변환을 담당하는 장치 : MMU(Memory Management Unit)보조기억장치와 입출력장치  하드디스크          플래터 : 하드 디스크 상에서 실질적으로 데이터가 저장되는 부분스핀돌 : 플래터를 회전시키는 부분헤드 : 플래터의 데이터를 읽고 쓰는 부분디스크 암 : 헤드를 옮기는 부분        하드디스크의 데이터 단위          트랙, 섹터, 실린더, 블록        하드 디스크의 지연 시간          탐색 시간, 회전 지연, 전송 시간        플래시 메모리 : 반도체 기반의 저장 장치, 매우 범용성 넓은 저장 장치          NAND 플래시 메모리, NOR 플래시 메모리 존재셀(cell) : 플래시 메모리의 가장 작은 저장 단위      ",
      "url": "/2024/12/15/Computer-Structure.html"
    },
  
    {
      "title": "Github - 001",
      "tags": "Github",
      "desc": "Github - 001 - 2024년12월14일  tag : Github|",
      "content": "Github - 001 - 2024년12월14일  tag : Github|Github  git bash 에서 아래 나오는 코드 실행하기환경설정  정보 확인하기$ git config --list  위 출력에서 없는 것을 확인한 후 추가$ git config --global uesr.name \"{username}\"$ git config --global user.email \"{emaliaddr}\"$ git config --global core.editor \"vim\"$ git config --global core.pager \"cat\"  Tip  만약 core.editor 값이 vim, notepad 두 개가 있을 경우  $ vi ~/.gitconfig    위 명령어 입력 후 notepad부분을 지우고 저장 후 다시 $ git config –list 로 확인하기깃허브 생성  단계          깃허브 접속      create repository -&gt; public 선택, Lisence : MIT 로 선택      주소 복사 후[HTML] git bash 에서 다음 명령어 입력        git clone \"주소\"                    깃허브 상태 확인 후 commit 까지$ git status # 자주 확인해주는 것이 좋다.$ git add {파일명.확장자}$ git commit$ git remote      # origin$ git remote -v$ git push origin main # origin[git remote으로 확인한 값]에 main branch로 올린다.  비밀 키 받기          오른쪽 맨 위 프로필 아이콘 클릭 -&gt; Settings 클릭      왼쪽 사이드 바에서 Developer Settings 클릭      왼쪽 사이드 바에서 Personal access tokens 아래에 있는 Tokens(Classic) 클릭      오른쪽 위에 Generate new token 선택 후 Generate new token(classic) 선택      pre-commit  commit 수행 전 체크해야 할 것들을 자동 수행하도록 도와주는 도구참고 사이트$ pip install pre-commit (or $ pip3 install pre-commit) # 설치$ pre-commit --version # 잘 설치되었는지 확인$ pre-commit sample-config &gt; .pre-commit-config.yaml # .pre-commit-config.yaml 파일 생성$ cat .pre-commit-config.yaml # 내용보기  사용방법          commit 하기 전에 사용      $ pre-commit run$ pre-commit run -a # 이것을 주로 사용하는 것 같다.[개인적인 의견]            Branch  분기점을 생성하여 독립적으로 코드를 변경할 수 있도록 도아주는 모델  branch 확인하기$ git branch$ git branch -r$ git branch -a  branch 생성$ git branch {branch 이름} # branch 생성$ git switch {branch 이름} # branch 전환... { 여기서 branch에서 작업 add -&gt; commit }$ git switch main # main branch 로 전환$ git merge {branch 이름} # main이 아닌 다른 branch 에서 작업한 것을 합치기$ git branch -D {branch 이름} # branch 삭제그 외  보기좋은 로그 출력하기    $ git config --global alias.lg \"log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit\"    commit 메세지 작성하는 방법          참고사이트        github repository 생성 사이트          참고사이트        gitignore 파일 만들기          참고사이트1참고사이트2        github에서 jupyter notebook 변경사항을 보고 싶을 때          오른쪽 위 프로필 사진 클릭 -&gt; Feature preview 클릭 -&gt; Rich Jupyter Notebook Diffs 활성화        git 을 쓰면서 습관들이면 안되는 것    &gt; $ git add . : 현재 디렉토리 하위의 모든 파일을 staging할 때 사용.&gt; but, 현재 commit단위에 들어가면 안되는 파일까지 휩쓸릴 수 있으므로 습관적으로 사용하면 안됩니다.&gt; $ git commit -m \"Message\" : 쉘에서 바로 메시지를 쓰면서 커밋할 때 사용.&gt; but, revert commit, merge commit 등의 상황에서 메시지를 덮어쓸 우려가 있으므로 습관적으로 사용하면 안됩니다.&gt; 저장소 안에 저장소 clone 하기 : 프로젝트 단위는 항상 독립적으로 존재해야 합니다.&gt; 의미없는 commit message 남기기(ex. a, 1, ..) : commit message는 제목만으로 해당 작업단위에 대한 설명이 가능해야 나중에 고생하지 않습니다.&gt; Conventional commit 잘 지키기!!      ",
      "url": "/2024/12/14/Github-001.html"
    },
  
    {
      "title": "DataBase System",
      "tags": "Computer_Science",
      "desc": "DataBase System - 2024년12월13일  tag : Computer_Science|",
      "content": "DataBase System - 2024년12월13일  tag : Computer_Science|DataBase  데이터베이스는 데이터를 저장하고 관리하기 위한 체계적인 방법을 제공하는 컴퓨터 시스템Types of database  관계형 데이터베이스(Relational database)          테이블 형태로 데이터를 저장하고 SQL(Structured Query Language)을 사용하여 데이터를 검색하고 조작하는 데이터베이스 유형대규모 데이터를 처리하는데 적합하며, 데이터의 일관성과 무결성을 보장한다.데이터베이스 내의 데이터는 중복을 최소화하여 일관성을 유지하며, 데이터를 업데이트하거나 삭제할 때 다른 테이블 간의 연관성을 유지하여 무결성을 보장Ex. Oracle, MySQL, Microsoft SQL Server        NoSQL 데이터베이스(Not only SQL)          관계형 데이터베이스와는 다른 형태의 데이터 모델을 사용하는 비관계형 데이터베이스 유형대량의 비정형 데이터를 다루는 데에 적합하며, 수평적 확장성을 강조데이터를 컬렉션(collection), 문서(document), 그래프(graph) 등의 형태로 저장수평적 확장이 가능하도록 설계되어 있다.Ex. MongoDB, Cassandra, Couchbase, Redis, Amazon DynamoDB대규모 분산 시스템에서 사용되며, 대량의 비정형 데이터를 저장하고 처리하는 데에 적합        메모리 데이터베이스(In-memory database)          디스크 대신 메모리(RAM)를 사용하여 데이터를 저장하고 처리하는 데이터베이스주로 실시간 데이터 처리나 대규모 데이터 처리 등의 분야에서 사용하지만, 메모리 데이터베이스는 메모리 크기에 제한을 받기 때문에 대용량 데이터 처리에는 적합하지 않다.또한, 메모리에 데이터를 저장하기 때문에 시스템에 장애가 발생하면 데이터가 손실될 가능성이 있다.Ex. Redis, Memcached, Apache Ignites        분산 데이터베이스(Distributed database)          여러 대의 컴퓨터에 데이터를 분산하여 저장하고, 이를 중앙 집중식으로 관리하는 데이터베이스분산 데이터베이스는 대규모 데이터 처리를 위해 설계되어 있으며, 여러 대의 서버를 사용하여 데이터를 처리하므로 단일 서버에 비해 처리량이 향상될 수 있다.분산된 데이터를 병렬적으로 처리할 수 있기 때문에 빠른 데이터 처리 속도를 보장할 수 있다.분산 데이터베이스는 서버 간 통신을 필요로 하므로 네트워크 대역폭의 한계나 네트워크 장애 등이 발생할 경우 데이터 접근 속도가 저하될 수 있다데이터 일관성 등의 문제를 해결하기 위한 추가적인 구현이 필요하다Ex.  Oracle RAC, Apache Cassandra, MongoDB      Database schema design  데이터베이스의 구조를 설계하는 과정스키마 디자인은 데이터베이스의 정확성, 일관성, 유지보수성 등을 결정하는 중요한 요소테이블의 속성(Column)을 적절하게 설계하고, 테이블 간의 관계(Relationship)를 정의하여 데이터를 효율적으로 저장하고 관리할 수 있도록 한다.  ERD(Entity-Relationship Diagram)          개체와 그들 간의 관계를 시각적으로 나타내는 모델링 도구데이터베이스의 구조와 관계를 쉽게 이해할 수 있으며, 효율적인 데이터 저장과 검색이 가능한 스키마를 설계할 수 있다.      유형1. 일대일(One-to-One): 한 테이블의 레코드가 다른 테이블의 레코드와 하나씩 대응하는 관계2. 일대다(One-to-Many): 한 테이블의 레코드가 다른 테이블의 여러 레코드와 대응하는 관계3. 다대다(Many-to-Many): 한 테이블의 레코드가 다른 테이블의 여러 레코드와 대응하고, 다른 테이블의 레코드도 한 테이블의 여러 레코드와 대응하는 관계            ",
      "url": "/2024/12/13/Database_System.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 009",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 009 - 2024년12월12일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 009 - 2024년12월12일  tag : Discovering_Statistics_Using_R|R|두 평균의 비교  평균을 비교하는 방법          서로 다른 여러 그룹에 각각 다른 실험적 조작을 가하는 것[=그룹간 설계, 독립설계]      하나의 그룹에 서로 다른 시점에서 여러 실험적 조작을 가하는 것[=반복측정 설계]      차이 살펴보기Data&gt; head(spiderLong, n= 3)    Group Anxiety1 Picture      302 Picture      353 Picture      45&gt; head(spiderWide, n= 3)  picture real1      30   402      35   353      45   50ggplot() +    stat_summary(data = spiderWide, aes(x = \"Spider Picture\", y = picture),                 fun = mean, geom = \"bar\", fill = \"white\", colour = \"black\") +    stat_summary(data = spiderWide, aes(x = \"Spider Picture\", y = picture),                 fun.data = mean_cl_normal, geom = \"pointrange\") +    stat_summary(data = spiderWide, aes(x = \"Spider Real\", y = real),                 fun = mean, geom = \"bar\", fill = \"white\", colour = \"black\") +    stat_summary(data = spiderWide, aes(x = \"Spider Real\", y = real),                 fun.data = mean_cl_normal, geom = \"pointrange\") +    labs(x = \"Type of Stimulus\", y = \"Anxiety\") +    theme_minimal()평균계산&gt; spiderWide$pMean&lt;-(spiderWide$picture + spiderWide$real)/2 # 각 실험 참가자의 평균 계산&gt; head(spiderWide, n=3)  picture real pMean1      30   40  35.02      35   35  35.03      45   50  47.5&gt; grandMean&lt;-mean(c(spiderWide$picture, spiderWide$real)) # 총 평균 계산&gt; grandMean[1] 43.5조정인자 계산&gt; spiderWide$adj&lt;-grandMean-spiderWide$pMean # 조정인자 계산&gt; head(spiderWide, n=3)  picture real pMean  adj1      30   40  35.0  8.52      35   35  35.0  8.53      45   50  47.5 -4.0조정인자로 각 값을 조정&gt; spiderWide$picture_adj&lt;-spiderWide$picture + spiderWide$adj&gt; spiderWide$real_adj&lt;-spiderWide$real + spiderWide$adj&gt; head(spiderWide, n=3)  picture real pMean  adj picture_adj real_adj1      30   40  35.0  8.5        38.5     48.52      35   35  35.0  8.5        43.5     43.53      45   50  47.5 -4.0        41.0     46.0&gt; spiderWide$pMean2&lt;-(spiderWide$picture_adj + spiderWide$real_adj)/2 # pMean2값이 전부 같은데 이는 평균의 개체 간 변동이 사라졌다는 증거&gt; head(spiderWide, n=3)  picture real pMean  adj picture_adj real_adj pMean21      30   40  35.0  8.5        38.5     48.5   43.52      35   35  35.0  8.5        43.5     43.5   43.53      45   50  47.5 -4.0        41.0     46.0   43.5  맨 처음 그래프와 비교      두 조건의 평균이 바뀌지 않았다.    오차 막대들은 이전보다 짧아졌으며, 겹치는 현상이 없어졌다.  T 검정일반선형모형으로서의 t검정&gt; t.test.GLM&lt;-lm(Anxiety ~ Group, data = spiderLong)&gt; summary(t.test.GLM)Call:lm(formula = Anxiety ~ Group, data = spiderLong)Residuals:   Min     1Q Median     3Q    Max -17.0   -8.5    1.5    8.0   18.0Coefficients:                 Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)        40.000      2.944  13.587 3.53e-12 ***GroupReal Spider    7.000      4.163   1.681    0.107    ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 10.2 on 22 degrees of freedomMultiple R-squared:  0.1139,\tAdjusted R-squared:  0.07359F-statistic: 2.827 on 1 and 22 DF,  p-value: 0.1068  상수(\\(b_0\\))값이 평균과 같은 40이라는 점회귀계수(\\(b_1\\))의 값이 두 그룹 평균의 차이(47 - 40 = 7)랑 같다.\\(b_1\\)이 0과 유의하게 다른지를 검사한 결과인 t 통계량을 확인해보면 유의하지 않다.따라서, \\(b_1\\)(두 그룹 평균의 차이)은 0과 유의하게 다르지 않다.독립 t검정  절차          자료를 입력한다.      자료를 탐색한다.[그래프 -&gt; 기술통계량 확인 -&gt; 분포의 가정들도 점검]      검정통계량을 계산한다.      효과크기를 계산한다.      &gt; by(spiderLong$Anxiety, spiderLong$Group, stat.desc, basic = FALSE, norm = TRUE) # 기술 통계량 확인spiderLong$Group: Picture      median         mean      SE.mean CI.mean.0.95          var      std.dev     coef.var     skewness     skew.2SE  40.0000000   40.0000000    2.6827168    5.9046200   86.3636364    9.2932038    0.2323301    0.0000000    0.0000000    kurtosis     kurt.2SE   normtest.W   normtest.p  -1.3939289   -0.5656047    0.9650165    0.8522870------------------------------------------------------------------------------------------spiderLong$Group: Real Spider       median          mean       SE.mean  CI.mean.0.95           var       std.dev      coef.var      skewness 50.000000000  47.000000000   3.183765638   7.007420922 121.636363636  11.028887688   0.234657185  -0.005590699     skew.2SE      kurtosis      kurt.2SE    normtest.W    normtest.p -0.004386224  -1.459758279  -0.592315868   0.948872904   0.620569431  정규성 결과는 둘 다 유의하지 않다.&gt; ind.t.test&lt;-t.test(Anxiety ~ Group, data = spiderLong)&gt; ind.t.test\tWelch Two Sample t-testdata:  Anxiety by Groupt = -1.6813, df = 21.385, p-value = 0.1072alternative hypothesis: true difference in means between group Picture and group Real Spider is not equal to 095 percent confidence interval: -15.648641   1.648641sample estimates:    mean in group Picture mean in group Real Spider                       40                        47  p값이 0.05보다 크므로 그룹 평균들이 다르지 않다는 귀무가설을 기각할 수 없다.신뢰구간의 하계와 상계는 95%의 경우에서 평균들의 진짜 차이가 포함되리라고 기대하는 차이 범위를 정의독립 평균들의 강건한 비교 방법&gt; yuen(spiderWide$real, spiderWide$picture, tr=.2, alpha=.05) # tr : 평균을 깎는(절사: trimming) 비율, alpha : 검정의 유의 수준(알파)을 설정&gt; yuenbt(spiderWide$real, spiderWide$picture, tr=.2, alpha=.05, nboot = 2000) # nboot : 사용할 부트스트랩 표본들의 개수&gt; pb2gen(spiderWide$real, spiderWide$picture, alpha=.05, nboot=2000, est=mom) # 부트스트랩과 M 추정량(절사평균이 아니라)으로 검정을 수행효과크기 계산&gt; t&lt;-ind.t.test$statistic[[1]] # t 검정의 t값&gt; t[1] 1.681346&gt; df&lt;-ind.t.test$parameter[[1]] # 자유도&gt; df[1] 21.38502&gt; r &lt;- sqrt(t^2/(t^2+df)) # 효과크기 계산&gt; round(r, 3)[1] 0.342  r = 0.342 &gt; 0.3 –&gt; 중간 효과결과평균적으로 참가자들은 거미 사진을 보았을 때 (M = 40, SE = 2.68) 보다 진짜 거미를 보았을 때 더 많이 불안해했다.(M=47, SE=3.18).그 차이는 유의하지 않다(t(21.39) = -1.68, p &gt; 0.05).그렇긴 하지만, 그 차이는 중간 크기의 효과를 대표한다(r = 0.34)종속 t검정  통계량 확인&gt; stat.desc(spiderWide, basic = FALSE, norm = TRUE) # 기술 통계량 확인                picture          realmedian       40.0000000  50.000000000mean         40.0000000  47.000000000SE.mean       2.6827168   3.183765638CI.mean.0.95  5.9046200   7.007420922var          86.3636364 121.636363636std.dev       9.2932038  11.028887688coef.var      0.2323301   0.234657185skewness      0.0000000  -0.005590699skew.2SE      0.0000000  -0.004386224kurtosis     -1.3939289  -1.459758279kurt.2SE     -0.5656047  -0.592315868normtest.W    0.9650165   0.948872904normtest.p    0.8522870   0.620569431  t 검정&gt; dep.t.test&lt;-t.test(spiderWide$real, spiderWide$picture, paired = TRUE) # paired = TRUE : 주어진 점수들이 종속적&gt; dep.t.test\tPaired t-testdata:  spiderWide$real and spiderWide$picturet = 2.4725, df = 11, p-value = 0.03098alternative hypothesis: true mean difference is not equal to 095 percent confidence interval:  0.7687815 13.2312185sample estimates:mean difference              7  t = 2.47, df = N - 1 = 11거미 공포증 자료의 경우 그 확률은 아주 낮다(p = 0.031) -&gt; 즉, 귀무가설이 참일 때 t가 2.47일 가능성은 3.1%밖에 되지 않는다.t값은 유의하며, t값이 양수라는 뜻은 첫 조건(real)의 평균이 둘째 조건(picture)의 평균보다 크다는 뜻따라서 거미 사진보다 진짜 거미가 참가자들을 더 불안하게 했다고 말할 수 있다.결론 : 사진을 제시했을 때보다 진짜 거미를 제시했을 때 거미 공포증 환자들이 유의하게 더 높은 수준의 불안을 보였다.  종속평균들을 비교하는 강건한 방법&gt; yuend(spiderWide$real, spiderWide$picture, tr=.2, alpha=.05)&gt; ydbt(spiderWide$real, spiderWide$picture, tr=.2, alpha=.05, nboot = 2000)&gt; bootdpci(spiderWide$real, spiderWide$picture, est=tmean, nboot=2000)  효과크기 계산&gt; t&lt;-dep.t.test$statistic[[1]]&gt; t[1] 2.472533&gt; df&lt;-dep.t.test$parameter[[1]]&gt; df[1] 11&gt; r &lt;- sqrt(t^2/(t^2+df))&gt; round(r, 3)[1] 0.598  보고&gt; 평균적으로 참가자들은 거미 사진을 보았을 때보다 진짜 거미를 보았을 때 유의하게 더 많이 불안해했다.",
      "url": "/2024/12/12/Discovering-Statistics-Using-R-009.html"
    },
  
    {
      "title": "Operating System Concepts",
      "tags": "Computer_Science",
      "desc": "Operating System Concepts - 2024년12월11일  tag : Computer_Science|",
      "content": "Operating System Concepts - 2024년12월11일  tag : Computer_Science|Introduction to Operaing SystemsOperating System 이란?  유저가 컴퓨터를 편하게 사용할 수 있게 모든 하드웨어를 관리해주는 프로그램Operating System 의 구조  Kernel : 프로세스 관리, 메모리 관리, 저장공간 관리, 장치 관리 등 컴퓨터에 속한 자원들에 대한 접근을 중재하는 역할  Interface : 사용자의 명령을 컴퓨터에 전달하고 결과를 사용자에게 알려주는 소통의 역할  System Call : 사용자나 프로그램이 직접적으로 컴퓨터 자원에 접근하는 것을 막고 커널을 보호하기 위해서 만든 코드 집합.  Driver : 프린터, 키보드 및 디스크 드라이브와 같은 하드웨어 장치와 운영체제 간의 통신을 가능하게 하는 소프트웨어  운영체제를 통해 안정적이고, 효율적인 동작을 하기 위해서는 사용자 또는 응용프로그램이 직접 하드웨어에 접근하는 것을 막아야 한다.이 때문에 User Mode(CPU 명령어 사용을 제한)와 Kernel Mode(CPU 명령어를 사용해 하드웨어를 직접 제어)로 분리해 운영체제를 사용User Mode와 Kernel Mode 사이는 System Call and Interrupt을 통해서 전환된다.System calls and APIsSystem Call  입출력, 메모리할당, 프로세스의 생성 등을 수행하는 코드의 집합.  유형          Process Control      File Management      Device Management      Information maintenance      Communications      Overview of popular operating systems (Windows, Linux, macOS)  운영체제는 데스크탑 뿐만 아니라 임베디드, 서버, 모바일 등의 기기에서도 사용됨대표적인 OS의 예시로는 Windows, Unix, macOS, Android 등이 있다.Processes and ThreadsProcess  job이나 task라고도 불리며 PCB(Process Control Block)이라는 걸로 Process들을 관리함Process state and transitions  new : 새롬게 생성된 Process  ready : CPU에서의 실행을 기다리는 상태  running : 실행중인 process  waiting : I/O(사용자의 입출력)이나 scheduling에 의한 대기 상태  terminated : 실행을 마친 상태Process Management  Process Control Block          각각의 process는 Process Control Block(PCB)에 관련된 정보를 저장한다.PCB에서 다루는 Process 정보        Process 생성          parent process에서 child process를 생성process들은 고유한 process identifier(pid)를 통해 구분, 관리 됨생성 시 child process는 부모의 PCB를 공유 받으며 어떤 정보를 공유할지는 공유 옵션에 따라 달라짐      Resource sharing option(Full/Partial/No sharing)Execution option(Overlay/Swapping)Address space option(Fixed/Variable)              Process 제거(termination)          exit system call을 통해 process를 삭제할 수 있다.present process는 wait system call를 통해 child가 정상적으로 제거되었는지 확인이 때 제대로 process가 제거되지 않으면 Zombie/Orphan 상태의 process가 만들어진다.      Zombie process: parent에서 child가 죽은걸 모르고 process table에 child에 대한 정보가 남아있는 경우Orphan: child가 terminate되기 전에 parent가 죽어버려서 부모가 없어진 경우              Process Scheduling          CPU 내부에서 어떤 process를 다음에 실행할지 선택하는 기능```        EX사용자는 동시에 여러 일을 수행하길 원함(웹서핑을 하면서 노래를 듣고, 카톡알림이 울리는 등).하지만 한번에 실행될 수 있는 Process의 수는 정해져 있음(보통 하나, Multi-core 환경에서는 늘어날 수 있음).따라서 Process Scheduling을 통해서 실행하는 Process를 바꿔주면서 여러 프로세스를 동시에 실행하는 것 같은 효과를 냄.```Context switching  Context switching은 Process가 종료되거나 Scheduling에 의해서 종료될 때 발생이전의 process 상태를 저장하고 새로운 프로세스의 PCB를 가져오는 역할Overhead가 심함Threads and multithreading  Thread          flow of control whitin a process(process와 subprocess로 이해할 수 있음)각각의 thread는 각자의 register state와 stack을 가지고 있다.CPU scheduling의 기본 단위      process는 프로세스 간의 전환에 대하여 PCB에 접근해서 Process address space를 복사해오는 등의 과정 때문에 overhead가 클 수 밖에 없는데,Thread는 Process에 비해 creation과 switching에 드는 시간이 적다는 장점이 있다. (Memory와 CPU 효율성 면에서 모두 장점을 가짐)                    Multi-threading          하나의 process에 대해서 여러 thread가 만들어질 수 있고, 이 때 code와 address space, operating resources를 공유한다.멀티쓰레딩을 통해서 동시성을 추구한다.병렬성(parallelism) : 여러 코어에서 동시에 process가 처리될 때 [ = num of CPUs(cores) ]동시성(concurrency) : illusion of parallelism        Multi-processing : 두 개 이상, 다수의 프로세서(처리장치, 프로세스 아님)가 협력적으로 작업을 동시에 처리하는 것Process SchedulingOverview of process scheduling  Process Scheduling을 통해서 CPU 효율성을 최대화할 수 있다.Scheduling은 process의 상태가 바뀔 때 일어나고 처리할 process는 ready queue와 device queue에 저장되어 관리된다.  단일프로세서는 하나의 running process를 가질 수 있기 때문에 더 많은 process가 존재한다면각각은 CPU에서 실행중인 process가 종료되거나 rescheduled 될 때까지 기다려야한다. 이 때 필요할 것이 process scheduling이다.  Scheduling Criteria  효율성을 판단할 때의 기준들          CPU utilization: CPU를 가능한 바쁜상태(일하고 있는 상태)로 유지하는가      Throughput: 일정한 단위 시간 동안 얼마나 많은 수의 프로세스가 완료되었는가      Turnaround time: 특정 프로세스를 실행하는데 걸리는 시간      Waiting time: ready queue에서 기다린 시간      Response time: process가 ready queue에서 기다리고 끝날때까지 걸린 시간.        CPU utilization과 Throughput을 최대화하고, turnaround time과 waiting time, response time을 최소화하는 것이 중요하다.Scheduling algorithms  non-preemptive(비선정형) : Process가 자원을 반납하기 전까지 다른 프로세스가 자원을 사용할 수 없음          수행시간이 긴 프로세스가 자원을 점유하게 되면 이후 실행되어야 하는 프로세스들이 자원을 할당받지 못하는 기아 현상이 발생        preemptive(선정형) : Process가 한번 실행될 때 제한된 시간만을 할당해서 사용          프로세스의 우선 순위에 따라 스케쥴링을 하게 되므로 우선순위가 낮은 프로세스는 기아 상태에 빠짐                  비선점형      선점형                  정해진 시간 없이process 종료 전까지 점유      일정 시간을 process에 할당해 해당 시간만 자원을 사용하고 반납              중간에 interupt가 일어나지 않음      interupt를 통해 실행 중인 process를 교체              종료 후 context switch 외에 추가적인 오버헤드 없음      context switch 가 일정 시간마다 일어나기 때문에 오버헤드 있음              프로세스 우선순위 고려 없음      프로세스에 대한 우선순위를 고려              FCFS, SJF, Priority Scheduling      Round-Robin, Multilevel Queue Scheduling        FCFS(First Come, First Served) Scheduling → 선착순          먼저 도착한 프로세스를 먼저 실행하고, 프로세스가 도착한 순서대로 CPU를 할당한다.보편적으로 프로세스들의 평균 대기 시간이 길어진다는 문제가 있다.        SJF(Shortest Job First) Scheduling          다음에 실행할 프로세스를 선택할 때 실행 시간이 가장 짧을 것으로 예상되는 프로세스를 선택하는 방식.이 경우 FCFS보다 평균 대기 시간이 줄어들지만 CPU burst time이 긴 프로세스의 경우 오히려 대기시간이 증가하고 심할 경우 starvation 상태가 되는 문제점이 있다.        RR(Round-Robin) Scheduling          각 프로세스에 차례로 일정한 시간 할당량(time quantum) 동안 CPU 자원을 차지할 수 있도록 함.time quantum 시간이 길다면 FCFS와 같은 형태로 작동하므로 RR 스케줄링을 사용하는 의미가 줄어들고, 시간이 너무 짧다면 너무 많은 Overhead가 생기기 때문에 좋지 않다.따라서 적절한 time quantum 길이를 찾는 것이 중요함.      Memory ManagementOverview of memory management  Memory : 메인 메모리 RAM(Random Access Memory)을 말하며, 프로그램 실행 시 정보들을 저장하고 가져다 사용할 수 있게 만드는 공간  Address binding 과 MMU          Physical address vs. Logical address      Physical address : 프로세스가 실행되면서 메모리 내부에 실제로 프로세스가 위치해 있는 주소를 의미물리적 주소의 경우 항상 그 주소가 비어있을것이라는 보장이 없고, 이미 메모리에 프로그램이 올라가 있으면 문제가 발생이를 해결하기 위해서 나온 것이 Logical addressLogical address : 가상 주소라고도 하며, 물리적 주소와 논리적 주소를 잘 매핑하는 것이 중요하다.            Address binding : 논리적 주소에 데이터를 저장해둔 뒤 데이터를 메모리에 로딩할 때나 프로세스를 실행할 때 물리적 주소에 직접 매핑하는 방법이러한 매핑은 MMU(Memory Management Unit)에서 수행하며, 보통 물리적 주소가 시작하는 base 주소를 논리적 주소에 더해서 데이터를 메모리에 올린다.Load time binding : 데이터를 메모리에 로딩할 때 논리 주소를 물리 주소에 매핑하는 방식              이 방식은 overhead가 너무 심해 요즘은 잘 사용하지 않고,프로세스를 실행할 때 데이터를 메모리에 올리는 Execution time binding 방식이 주로 쓰인다.            Contiguous memory allocation  logical address 가 연속적이라면 physical address도 연속적으로 배치하는 것을 의미이 경우 MMU가 실행 시간 바인딩에서 해야하는 연산이 적다는 장점이 있다.  또한 Memory Protection의 구현이 쉽다.  Memory Protection : 시스템에서 참고하는 메모리 주소가 참고 가능한 범위를 넘어서는지를 체크하는 것    연속 메모리 할당에서 메모리 공간을 분배하는 방법      Two Partition Allocation : Kernel과 User 모드 두 부분으로 메모리를 분할하여 활동하는 방식메모리 공간이 두 개의 파티션으로 분할되기 때문에, 메모리 공간의 낭비가 발생할 수 있다.    Multiple Partition Allocation      Fixed Size Partition : 메모리 공간을 고정 크기로 나누어 사용하는 메모리 관리 방식&gt; 프로세스가 필요로 하는 메모리 공간의 크기에 따라 파티션을 선택할 수 없기 때문에 내부 단편화가 발생할 가능성이 높음Variable-size Partition : 메모리 공간을 프로세스의 요구에 따라 가변적으로 할당하는 메모리 관리 방식&gt; 내부 단편화 문제를 완화시킬 수 있으나 파티션의 크기가 자주 변하기 때문에 메모리 할당 및 해제 과정이 복잡해지고, 외부 단편화 문제가 발생할 가능성이 높음            연속 메모리 할당 방식에서는 fragmentation(단편화) 의 문제가 발생하게 된다는 큰 문제가 있어 잘 쓰이지 않는다      외부 단편화: 메모리 내에 충분한 크기의 공간이 있더라도 연속적인 공간이 부족하여 메모리 할당이 불가능한 경우내부 단편화: 메모리 공간을 할당할 때, 필요한 공간보다 큰 크기의 공간을 할당하여 할당된 메모리에 사용하지 못하는 부분이 발생하는 경우            File System  컴퓨터에는 다양한 파일들이 저장장치에 저장이 된다.이 떄 운영체제가 저장장치에 있는 데이터를 효율적으로 CRUD 할 수 있는 것을 File System이라 한다.  구성요소          File      Directory      block : 저장 장치의 고정된 데이터 저장 최소 단위      inode : file, Directory의 구조에서 pointer를 이용한 관리를 위한 값      superblock : file system의 중요정보가 담긴 모듬      journaling : 데이터의 무결성을 보장하기 위한 기법        설계시 고려사항          데이터 관리 및 보존      성능 최적화      데이터 무결성      확장성      보안 및 접근 제어      File  File Attribute          Name, Type, Location, Size, TimestampsIdentifier : system이 사용하는 유니크한 값Protection : 접근 권한에 관련된 정보User identification : 생성, 수정자에 대한 정보        File Operation          CRUDReposition : 열여있는 현재 파일의 탐색을 위한 값Truncation : 파일을 자르는 개념Open, Close        Structure of File          Sequential File Structure : 순차적으로 이루어진 파일 구조이며, 불러오는 속도가 빠르며, 작성과 삭제를 하는 구조에서 오래걸리는 단점이 존재Index File Structure : Sequential 구조에 기반을 두고 있지만, 위치를 잡기위한 index가 첨가되어 있다.[DB와 같이 빠른 접근이 필요할 때 적절]Direct File Structure : 불연속 구조를 가지며, 물리적으로 위치를 옮겨가며 파일을 관리.[Hashing 함수를 이용하여 물리적 주소를 접근]        File Descriptor          Open Operation을 통하여 File을 열면 반환이 되는 포인터를 가지는 인덱스 값      Directory  Directory Operation          Create, Delete, SearchList : Directory 내부의 Directory or File을 list화 하여 관리RenameTraverse : 백업과 같이 다른 장치로 이동할 수 있다.        Structure of Directory          Single-Level Directory : 가장 단순한 형태의 구조로 사용자가 여러명일 경우 관리가 힘들다.      Two-Level Directory : 사용자간의 구별을 가지는 형태의 구성      Three-Level Directory : Tree구조라고도 하며 일반적인 컴퓨터에서 통용되는 구조      Acyclic-Graph Directory : Three-Level Directory에서 Link가 추가된 구조, 바로가기와 같은 기능이 추가되었다고 볼 수 있다.      General Graph Directory : 여타 구조들과 다르게 순환이 가능한 구조, 복잡하며 구현이 힘들다.      Allocation of File System  파일 시스템에서 데이터를 저장하게 될 때 할당되는 방식  Contiguous Allocation          디스크에 연속적으로 블록이 할당이 되는 방식 -&gt; 매우 빠른 동작을 하게 된다.하지만 할당 할 수 있는 영역이 부족한 상태에서는 할당하지 못하고 영역이 남는 조각화가 발생할 수 있다.        DisContiguous Allocation          불연속으로 할당을 하는 방식, 비어있는 블록에 데이터를 분산하여 저장하는 방식      Linked Allocation : 각 블록에 다음 블록의 위치를 저장하여 파일을 이어주는 방식[조각화 문제를 해결할 수 있다.]--&gt; 하지만 연결된 마지막 파일을 사용할때 속도가 느려질 수 있다.Index Allocation : 인덱스 블록을 사용하여 모든 파일 블록의 위치를 저장하는 방식            Type of File system  FAT(File Allocation Table)          초기에 사용하던 방식이며 윈도우에서 대부분 호한이 잘 된다.파일을 작은 클러스터로 나누어서 관리하며 최대용량이 작은 편이다.암호화, 압축이 지원되지 않으며, 조각화와 단편화가 발생하기 쉽다.        NTFS(New Technology File System)          FAT의 한계를 개선한 고급 파일 시스템으로, 파일 암호화, 압축, 파일 복구 등 다양한 기능을 지원        EXT(EXtended File system)          리눅스 기반 운영체제에서 사용하는 파일 시스템Journaling : 데이터 변경사항 발생시 로그에 기록 -&gt; 문제가 발생하게 되면 로그를 기반으로 하여 복구를 할 수 있게 지원Journaling 기능을 추가해 데이터 안정성을 강화하였으며, 대용량 파일 및 파티션을 지원        APFS(Apple File System)          Apple의 최신 파일 시스템으로, SSD 최적화, 스냅샷 기능, 파일 및 디렉토리 복제 성능 향상을 지원      ",
      "url": "/2024/12/11/Operating-System-Concepts.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 008",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 008 - 2024년12월10일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 008 - 2024년12월10일  tag : Discovering_Statistics_Using_R|R|로지스틱 회귀  로지스틱 회귀란 결과변수가 범주형변수이고 예측변수들이 연속변수 또는 범주형변수인 다중회귀이다.로지스틱 회귀에서는 Y의 값이 이미 알고 있는 \\(X_1\\)의 값(또는 X의 값들)일 확률을 예측한다.  로지스틱 회귀 방정식    \\[P(Y) = \\frac{1}{1 + e^{-(b_0+b_1X_{1i}+b_2X_{2i}+...+b_nX_{ni})}}\\]      모형의 평가  로그 가능도 통계량          로그 가능도는 예측값들과 실체 관측값들에 관한 확률들의 합이다.로그 가능도 통계량의 값이 크다는 것은 설명되지 않은 관측이 많이 남아 있다는 뜻으로, 그런 경우 해당 통계적 모형은 자료에 적합하지 않다.        이탈도(deviance) 통계량          이탈도는 카이제곱 분포를 따르기 때문에 로그 가능도를 그냥 사용하는 것보다 사용하기 편하다. -&gt; 유의성을 계산하기가 쉽다.        \\(R과 R^2\\)          모형이 자료에 얼마나 잘 들어맞는지를 측정하는데 사용하는 측도R-statistic : 결과변수와 각 예측변수 사이의 편상관계수로, 값의 범위는 -1부터 1까지        z 통계량          정규분포를 따르며, 주어진 예측변수의 b 계수가 0과 유의하게 다른지의 여부를 나타낸다.        승산비(odds ratio; 오즈비)          예측변수의 1단위 변화에 따른 승산의 변화 비율      로지스틱 회귀 방법강제 도입법  예측변수들을 모두 한꺼번에 회귀모형에 포함하고 각 예측변수의 매개변수들을 추정하는 방법단계적 방법  (전진, 후진, 또는 둘의 조합)을 선택해야 함.매개변수의 변화가 있을 때 마다 AIC, BIC 중 선택한 기준을 이용해서 개선되었는지 확인가정  선형성 : 임의의 연속 예측변수들과 결과변수의 로짓의 관계가 선형이라는 가정  오차의 독립성 : 자료의 사례들 사이에 관계가 없어야 함  다중공선성의 부재 : 예측변수들 사이의 상관관계가 너무 크면 안된다.R을 이용한 로지스틱 회귀이항 로지스틱 회귀Intervention  결과변수 : Cured[치료됨 or 치료되지 않음]  예측변수          Intervention : 개입했음, 처치가 없었음      Duration : 환자가 처치를 받기 전까지 문제를 겪은 기간, 단위는 일      &gt; head(eelData, n = 5)      Cured Intervention Duration1 Not Cured No Treatment        72 Not Cured No Treatment        73 Not Cured No Treatment        64     Cured No Treatment        85     Cured Intervention        7  변수를 요인으로 변경 후 기저를 부정인 값으로 지정&gt; eelData$Cured &lt;- as.factor(eelData$Cured)&gt; eelData$Intervention &lt;- as.factor(eelData$Intervention)&gt; eelData$Cured&lt;-relevel(eelData$Cured, \"Not Cured\")&gt; eelData$Intervention&lt;-relevel(eelData$Intervention, \"No Treatment\")  glm : generalized linear model  glm(결과변수 ~ 예측변수(들), data = 데이터프레임, family = 분포의 종류, na.action = 결측값처리방식)  family : binomial() -&gt; 로지스틱 회귀는 이항분포에 기초하므로 이렇게 지정해야 함&gt; eelModel.1 &lt;- glm(Cured ~ Intervention, data = eelData, family = binomial())  모형의 전반적인 적합도는 이탈도로 평가. [ 이 값이 클수록 통계적 모형으로서의 이 회귀모형은 자료에 잘 적합되지 않는 것 ]  귀무 이탈도(null deviance) : 상수만 있고 예측 변수는 전혀 없는 모형의 이탈도 [ = -2LL(기저모형) = 154.08 ]  잔차 이탈도를 제공 : -2LL(새모형) [ 144.16 ]  기저 이탈도(154.08)에서 Intervention을 추가(144.16)하니 이탈도가 감소했다. -&gt; 추가한 모형이 결과를 좀 더 잘 예측함  모형이 결과변수를 ‘얼마나 더 잘’예측하는지는 모형의 카이제곱 통계량으로 평가[카이제곱 : 현재의 모형과 상수만 있는 모형의 차이를 측정]  Z통계량의 유의확률이 0.05보다 작으므로 Intervention은 치료 여부의 유의한 예측변수라고 할 수 있다.&gt; summary(eelModel.1)Call:glm(formula = Cured ~ Intervention, family = binomial(), data = eelData)Coefficients:                         Estimate Std. Error z value Pr(&gt;|z|)   (Intercept)               -0.2877     0.2700  -1.065  0.28671   InterventionIntervention   1.2287     0.3998   3.074  0.00212 **---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 154.08  on 112  degrees of freedomResidual deviance: 144.16  on 111  degrees of freedomAIC: 148.16Number of Fisher Scoring iterations: 4  카이제곱 = 154.08 - 144.16 = 9.92 [ 이 값은 카이제곱 분포를 따르므로, 유의성을 확인하면 &lt; 0.05 수준에서 유의하므로 상수만 있는 모형보다 이 모형이 더 잘 예측한다고 할 수 있다.]  모형에 Intervention을 포함 함으로써 모형의 적합도가 \\(\\chi^2(1) = 9.93, p = 0.002\\)로 유의하게 개선되었다는 결론을 내릴 수 있다.&gt; modelChi &lt;- eelModel.1$null.deviance - eelModel.1$deviance # 카이제곱 구하기&gt; chidf &lt;- eelModel.1$df.null - eelModel.1$df.residual # 통계량의 자유도 구하기&gt; chisq.prob &lt;- 1 - pchisq(modelChi, chidf) # 카이제곱 통계량과 관련된 확률 구하기&gt; modelChi; chidf; chisq.prob[1] 9.926201[1] 1[1] 0.001629425 # &lt; 0.05 이므로 모형이 결과를 더 잘 예측하지 못한다는 귀무가설을 기각할 수 있다.&gt; exp(eelModel.1$coefficients) # 승산비 구하기             (Intercept) InterventionIntervention                0.750000                 3.416667# 결론 : 처치를 받은 환자가 치료될 승산이 처치를 받지 않은 환자가 치료될 승산의 3.42배라고 말할 수 있다.&gt; exp(confint(eelModel.1)) # 승산비들의 신뢰구간                             2.5 %   97.5 %(Intercept)              0.4374531 1.268674InterventionIntervention 1.5820127 7.625545# 결과에서 중요한 점은 하계와 상계의 값이 1보다 크다는 점# 왜냐하면, 예측변수가 증가함에 따라 환자가 치료될 승산이 커짐을 뜻하기 때문이다.# 만약 하계가 1보다 작다면, 모집단에서의 관계의 방향이 관측된 방향과 다를 여지가 존재하는 것 -&gt; 실험자의 개입이 환자가 치료될 승산을 증가한다고 확신할 수 없다.Intervention and Duration  Duration의 b 추정값이 0.008으로 꽤 작은 값이며, 그 변수에 관련된 확률은 0.964 &gt; 0.05이므로 이 값은 유의하지 않다.  두 모형의 이탈도(=144.16)으로 똑같고, AIC(150.16)으로 모형 1의 값(148.16)보다 큰 값으로, 이는 모형 1이 더 나은 모형임을 뜻한다.&gt; eelModel.2 &lt;- glm(Cured ~ Intervention + Duration, data = eelData, family = binomial())&gt; summary(eelModel.2)Call:glm(formula = Cured ~ Intervention + Duration, family = binomial(),    data = eelData)Coefficients:                          Estimate Std. Error z value Pr(&gt;|z|)   (Intercept)              -0.234660   1.220563  -0.192  0.84754   InterventionIntervention  1.233532   0.414565   2.975  0.00293 **Duration                 -0.007835   0.175913  -0.045  0.96447   ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1(Dispersion parameter for binomial family taken to be 1)    Null deviance: 154.08  on 112  degrees of freedomResidual deviance: 144.16  on 110  degrees of freedomAIC: 150.16Number of Fisher Scoring iterations: 4  두 모형을 비교하는 방법  두 모형의 이탈도 차이는 0.00198이며 자유도 차이는 1이다.  p값은 0.9645 &gt; 0.05이므로 모형1보다 유의하게 개선된 것은 아니라는 결론을 내릴 수 있다.&gt; anova(eelModel.1, eelModel.2)Analysis of Deviance TableModel 1: Cured ~ InterventionModel 2: Cured ~ Intervention + Duration  Resid. Df Resid. Dev Df  Deviance Pr(&gt;Chi)1       111     144.16                      2       110     144.16  1 0.0019835   0.9645로지스틱 회귀의 사례별 진단  기본적인 진단 통계량 구하는 코드eelData$predicted.probabilities&lt;-fitted(eelModel.1)eelData$standardized.residuals&lt;-rstandard(eelModel.1)eelData$studentized.residuals&lt;-rstudent(eelModel.1)eelData$dfbeta&lt;-dfbeta(eelModel.1)eelData$dffit&lt;-dffits(eelModel.1)eelData $leverage&lt;-hatvalues(eelModel.1)  예측변수 : Intervention  환자가 처치를 받지 않았을 때 치료될 확률은 0.429, 개입이 있었다면 환자가 치료될 확률은 0.719&gt; head(eelData[, c(\"Cured\", \"Intervention\", \"Duration\", \"predicted.probabilities\")], n=5)      Cured Intervention Duration predicted.probabilities1 Not Cured No Treatment        7               0.42857142 Not Cured No Treatment        7               0.42857143 Not Cured No Treatment        6               0.42857144     Cured No Treatment        8               0.42857145     Cured Intervention        7               0.7192982  잔차 통계량을 확인했을 때 확인해야 할 사항          지렛대      스튜던트화 잔차와 표준화잔차      상수의 DFBeta와 예측변수의 DFBeta      &gt; head(eelData[, c(\"leverage\", \"studentized.residuals\", \"dfbeta\")], n=5)    leverage studentized.residuals dfbeta.(Intercept) dfbeta.InterventionIntervention1 0.01785714            -1.0643627        -0.03886912                      0.038869122 0.01785714            -1.0643627        -0.03886912                      0.038869123 0.01785714            -1.0643627        -0.03886912                      0.038869124 0.01785714             1.3110447         0.04782751                     -0.047827515 0.01754386             0.8160435         0.00000000                      0.03225994  주의사항! 만약 현저한 이상치 사례들이 발견되었다면, 단지 모형을 좀 더 자료에 적합하게 만들기 위해 사례들을 제거하는 것은 정당한 일이 아니라는 점보고",
      "url": "/2024/12/10/Discovering-Statistics-Using-R-008.html"
    },
  
    {
      "title": "Computer Architecture",
      "tags": "Computer_Science",
      "desc": "Computer Architecture - 2024년12월09일  tag : Computer_Science|",
      "content": "Computer Architecture - 2024년12월09일  tag : Computer_Science|Computer Organization and Architecture  Computer Architecture란?          컴퓨터 시스템의 구조와 구성요소, 그리고 이들 간의 상호작용에 대한 설계와 구현을 다루는 분야      Components of a Computer System  CPU(Central Processing Unit) : 프로그램의 명령어를 해석하고 실행하는 역할을 한다.[산술 논리 연산 장치(ALU), 제어 장치, 레지스터 등으로 구성]  Main Memory : 프로그램이나 데이터를 저장하는 공간으로, CPU가 작업을 수행할 때 필요한 명령어나 데이터가 저장되어 있다.[RAM(Random Access Memory) 존재]  Input Device : 사용자가 컴퓨터에 데이터나 명령을 입력하는 장치[키보드, 마우스]  Output Device : 컴퓨터에서 처리한 결과를 사용자에게 보여주는 장치[모니터, 스피커]  Storage(Secondary Memory) : 데이터를 임시적, 반영구적으로 저장하는 장치[하드 디스크 드라이브, SSD, USB 플래시 드라이브]  Bus : 컴퓨터 내부에서 데이터와 명령어를 전송하는 통로[데이터 버스, 주소 버스, 제어 버스 등]  (Mother)Board : CPU, 메모리, 버스 등의 하드웨어 요소들이 담겨 있는 회로 기판폰 노이만 아키텍쳐(Von Neumann Architecture)  프로그램과 데이터가 같은 메모리 공간에서 저장되어 처리되는 구조구성요소 : CPU, 메모리, 입출력 장치특징 : 명령어와 데이터가 동일한 메모리에 저장되어 있기 때문에, 명령어와 데이터를 구분하기 위한 별도의 제어 신호가 필요하지 않다는 것단점 : 위 특징 때문에 명령어와 데이터를 동시에 처리할 수 없다는 단점을 가짐 -&gt; 현대 컴퓨터에서는 병렬 처리 기술 등을 이용해 이러한 한계를 극복하고 있다.Instruction Cycle  CPU가 메모리부터 1개의 명령어를 가져와 어떤 동작을 요구하는지 결정하고 이를 수행하는 연속적인 과정  과정          Fetch : CPU가 Instruction을 memory로부터 1word를 읽어 다음으로 실행할 명령어를 인출한다.      Decode : CPU가 인출한 명령어를 해독하여 명령어가 어떤 작업을 수행해야 하는지를 결정      Execute : CPU가 결정된 명령어를 실행      Memory(Access) : CPU가 메모리에 접근하여 데이터를 읽거나 쓸 수 있다.      Write Back : 계산된 결과를 레지스터(Register)에 저장한다.      Central Processing UnitCPU는 컴퓨터 시스템의 핵심 요소 중 하나로 프로그램의 명령어를 해석하고 실행하는 역할을 담당한다.따라서, CPU는 모든 컴퓨터 시스템에서 필수적인 구성 요소이며, 컴퓨터의 속도와 성능에 큰 영향을 미친다.CPU는 일반적으로 작은 실리콘 칩에 위치하며, 코어(core)라고 불리는 작은 계산 논리 회로의 집합으로 구성된다.이 코어들은 다양한 명령어를 수행하고, 데이터를 처리하고, 메모리와 입출력 장치와의 통신을 관리한다.최근의 CPU는 여러 개의 코어(physical core)를 가진 멀티코어 프로세서가 일반적이다.멀티코어 CPU는 여러 가지 작업을 병렬로 처리하여 컴퓨터의 처리 속도를 향상시키는 데 도움을 준다.또한, CPU는 클럭 속도라고 불리는 주파수로 측정되는 작업 속도를 가지고 있으며, 이 속도가 높을수록 CPU가 처리할 수 있는 명령어 수가 늘어나고, 컴퓨터의 처리 속도가 빨라진다.Components of CPU  Control Unit(CU) : 프로그램의 명령어를 해석하고, 실행하는 부분  Arithmetic/Logic Unit(ALU) : 수치 연산과 논리 연산을 처리하는 부분  Register : CPU내부에 저장되어 있는 소규모 메모리로, 데이터를 저장하거나 처리하는 데 사용된다.성능 향상을 위한 작업 방법Pipelining ( 단계별 병렬처리 ) -&gt; Sync ( 시간에 동기화되어있다. )  Pipelining : 한 가지 작업을 수행하는 데에 여러 단계의 작업이 필요한 경우, 이 단계들을 연속적으로 실행하여 시간을 단축시키는 기술Parallellism (독립적인 병렬처리) -&gt; Async( 시간에 동기화가 되어있지 않아도 된다 )  Parallellism : 한 번에 여러 작업을 수행하는 것[ EX. 여러 개의 CPU 코어를 사용하여 병렬적으로 작업을 처리하는 것 ]- 두 기술의 차이점1. Pipelining : 하나의 작업을 여러 단계로 분할하여 각 단계를 병렬적으로 처리2. Parallellism : 여러 작업을 동시에 처리하여 실행 시간을 단축Memory Organization and ManagementMemory Hierarchy  컴퓨터에서 데이터와 명령어를 저장하는 계층 구조  Speed = Register &gt; L1 Cache &gt; L2 Cache &gt; L3 Cache &gt; Main »&gt; Secondary(SSD)      memory capacity(저장 용량) = Register &lt; Cache « Main «&lt; Secondary(SSD)    Register : 가장 빠른 속도로 데이터에 접근할 수 있는 기억장치  Cache Memory(L1 &gt; L2 &gt; L3) : CPU와 메인 메모리 사이에 위치하는 기억장치, CPU가 처리할 데이터나 명령어를 미리 가져와 저장하므로, 더 빠른 속도로 접근이 가능  Main Memory(DRAM) : 프로그램이 실행될 때 필요한 데이터나 명령어가 저장되는 주 기억장치  Secondary Memory(=Storage) : HDD나 SSD와 같은 보조기억장치를 말함Types of memory(RAM, ROM, Flash Memory)  RAM(Random Access Memory)          컴퓨터가 작업 중인 데이터와 프로그램을 저장하는 메모리읽기와 쓰기 모두 가능하며, 전원이 꺼지면 내용이 지워진다.        ROM(Read-Only Memory)          주로 컴퓨터의 BIOS(Basic Input/Output System)나 장치 드라이브 등 시스템 소프트웨어를 저장하는 메모리읽기만 가능하며, 내용을 수정할 수 없다.전원이 꺼져도 내용이 지워지지 않는다.        [NAND]Flash Memory          컴퓨터나 디지털 기기에서 많이 사용되는 메모리저장하고 읽기와 쓰기 모두 가능하다.전원이 꺼져도 내용이 지워지지 않으며, 비교적 저렴한 가격과 높은 용량을 제공 [ USB 메모리, SD 카드, SSD(Solid State Drive)등에 사용된다 ]      I/OTypes of I/O Device  Block Device  HDD : 기계쩍인 구조로 이루어진 저장 장치, 자성을 띄는 원형 디스크가 회전하면서 헤드로 데이터를 읽고 쓰는 작업이 이루어진다.  구조      Platter : 여러 개의 자성 디스크가 쌓여 있으며, 고속으로 회전합니다.    Head : 데이터를 사용하기 위한 작은 장치로, Platter 위에서 자기 신호를 감지    Spindle : Platter 가 회전 하는걸 고정 해주는 축    Arm : Platter의 특정 위치로 Head를 옮겨주는 역할    장점 : 비교적 저렴한 가격단점 : 기계의 움직임으로 인하여 데이터 입출력이 비교적 느리며, 충격에 취약하다.  SSD : 반도체 메모리 칩(NAND 플래시 메모리)으로 이루어진 저장 장치  구조      NAND memory : 데이터가 저장되는 비휘발성 메모리    Controller : 데이터를 관리하는 역할을 하는 장치    DRAM : Cache를 지원하여 더 빠른 속도를 지원할 수 있다.    장점 : 빠른 동작 속도, 기계가 아니라서 충격에 강함단점 : NAND 메모리 성능의 한계, 비교적 비싼 가격  Character Device : Data가 단발성으로 발생이 되는 streaming data를 다루는 장치[Ex. Network, Keyboard]Components of I/O System  device controller : 각 I/O 장치마다 존재하며, 데이터 전송을 관리하는 하드웨어  구성요소      I/O port : 장치와 직접적으로 명령어 및 데이터를 주고 받는 부분    register : 데이터, 장치의 상태, 동작의 명령어등을 저장하고 보내는 영역    역할      해당하는 device의 하드웨어적인 동작 제어를 함.    데이터를 주고 받는다.    작업 상태에 따른 interrupt를 CPU에 전달    device driver : 하드웨어와 소프트웨어 간의 인터페이스를 제공하는 소프트웨어  구성요소      I/O control : 운영체제와 하드웨어 장치를 관리    buffering : 데이터를 임시저장하여 반응속도를 높여준다.    Interrupt handler : 발생한 interrupt를 처리할 수 있다.    역할      장치 제어, 데이터 관련 정보를 포멧에 맞게 맞춰 보내준다.    장치 상태를 추적하여 오류 발생 시 운영체제에게 알려준다.    종류      kernel mode driver : 자원을 활용하는 상태    user mode driver : 유저가 사용하는 제한된 상태    port : 컴퓨터와 연결이 되는 지점  Bus : I/O Device 간의 연결 및 Device와 CPU등과 연결을 도와준다.Methods of I/O Data transmission  Pooling Method : 장치가 준비되었을 때 CPU에 신호를 보내 작업을 처리하는 방식  동작 순서    - CPU가 명령어 레지스터에 명령어를 작성하고 자신이 할일을 수행    - controller가 명령어 레지스터에서 명령어를 받아서 수행    - controller가 수행완료 및 특이사항 발생시 interrupt를 유발     - CPU가 interrupt를 처리    - CPU가 controller의 결과물을 받는다.  장단점    - 장점 : 감시체계가 사라져서 리소스의 낭비가 사라졌음    - 단점 : interrupt의 발생이 많아질경우 CPU가 계속 응답해줘야 하기때문에 과부하가 발생할 수 있다.  Interrupt Method : CPU 개입 없이 I/O 장치가 직접 메모리와 데이터를 주고받는 방식  동작 순서    - CPU가 DMA controller에 데이터를 요청    - DMA controller가 I/O에서 memory로 데이터를 전송    - memory로 전송 완료 되면 interrupt를 발생    - CPU가 interrupt와 데이터를 사용  장단점    - 장점 : CPU의 간섭없이 많은 데이터를 송수신 할 수 있다.    - 단점 : DMA controller가 지원이 되어야 하며,  적은 데이터를 사용할때는 번거로울 수 있다.",
      "url": "/2024/12/09/Computer-Architecture.html"
    },
  
    {
      "title": "Data Structure and Algorithms",
      "tags": "Computer_Science",
      "desc": "Data Structure and Algorithms - 2024년12월08일  tag : Computer_Science|",
      "content": "Data Structure and Algorithms - 2024년12월08일  tag : Computer_Science|Introduction to Data StructuresData Structures  데이터를 구성, 저장, 조작하는 방법을 의미함 [ 데이터의 관리 효율성을 조절하기 위한 여러가지 구조에 관한 내용 ]대표적으로 Array, List, Stack, Queue, Tree, Graph 가 있다.Abstract Data Type(=ADT)  프로그래밍을 함에 있어 데이터를 추상화하여 논리적인 구조를 정의한 것  장점          데이터에 대한 정보를 이해하고 저장하는 방식을 결정함으로써 최적의 알고리즘을 개발할 수 있음      프로그래밍을 효율적으로 구현할 수 있도록 도와줌        Abstract          복잡한 데이터나 시스템 등으로부터 핵심적인 개념 또는 기능을 간추려 내는 것실제 데이터의 형태를 그대로 다루는 것이 아니라 데이터가 가져야하는 명세(specification)를 기반으로 만듬      Time and space complexity  Time complexity : 프로그램이 실행되고, 완료되는데 걸리는 시간          Compile TimeExecution Time [ 주로 이 복잡도 사용 ]        Space complexity : 프로그램이 실행되고, 완료되는데 필요한 메모리          고정 공간 요구량 : 자료구조가 사용하는 고정된 메모리 공간을 의미[Ex. int, double]가변 공간 요구량 : 필요에 따라 동적으로 할당, 해제되는 메모리 공간을 의미[Ex. function call]        Big Oh, Big Ω, and Big Θ notation          Big oh : 최악의 경우Big Ω : 최선의 경우Big Θ : 평균      \\[f(n) = O(g(n)) \\iff \\exists C &gt; 0, n_0 \\in \\mathbb{N} \\text{ such that } \\forall n \\geq n_0, 0 \\leq f(n) \\leq Cg(n)\\]Stacks and QueuesStack  LIFO(Last-In, First-Out) 구조를 가진 자료구조가장 최근에 삽입한 요소를 top이라고 지칭한다.Stack의 구조는 컴퓨터 가상메모리의 Stack 영역에서 사용되는데, 함수가 호출되면서 다시 복귀할 주소를 저장하거나, 지역변수, 매개변수 등을 임시로 저장하는데에 쓰인다.Queues  FIFO(First-In, First-Out) 구조를 가진 자료구조front와 rear로 가장 먼저 들어온 요소와 제일 마지막에 들어온 요소에 접근한다.Enqueue : queue에 원소를 삽입  | Dequeue : queue에서 원소를 삭제Linked List and Hash TableArray  연속된 메모리 공간에 같은 타입의 데이터를 순차적으로 저장하는 자료구조크기가 고정되어 있기 때문에 배열을 생성할 때 크기를 지정해줘야함[ 배열이 가득 차는 경우 새로운 데이터를 추가할 수 없거나 기존 데이터를 삭제해야 함 ]Linked List  생성 후에 자유롭게 원소를 추가/삭제할 수 있는 자료구조가변적인 길이를 가지고 있기 때문에, 새로운 데이터 추가에 대한 제한이 거의 없다는 장점이 있다.Hash Table  데이터의 key를 hash function 을 통해 hash value으로 변환하고, 이 값을 인덱스로 사용하여 데이터를 저장하거나 검색하는 효율적인 자료 구조평균적으로 O(1)의 시간 복잡도로 데이터에 접근할 수 있다.Hash Table의 사용 목적은 정해진 메모리에 여러 원소를 효율적으로 저장하여 indexing 성능을 O(1)에 가깝게 만드는 것Hash function을 통해 hashing을 하게 되면 다른 원소가 같은 index를 가지는 hash collision이 발생  Hash Collision을 해결하기 위한 방법          Separate Chaining                  방법 : 해시 테이블의 각 인덱스에 연결 리스트나 다른 자료 구조를 사용하여 여러 개의 key-value pair를 저장장점 : 간단하고 동적으로 크기를 조절하기 쉽다.                    Open Addressing                  방법 : 충돌이 발생하면 다른 빈 슬롯을 찾아 데이터를 저장                    Trees and Graphs  Tree는 데이터 속 항목을 계층적으로 구조화하는 자료구조  용어          Node : 트리 구조의 교점으로 Node는 데이터(value)를 가지고 있고, 자식노드를 가지고 있다.      Root Node : 트리 구조에서 가장 위에 있는 노드, 즉 시작점이 되는 노드      edge(link) : 트리를 구성하기 위해 노드와 노드를 연결하는 선      level : 트리의 특정 깊이를 가지는 노드의 집합      degree : 각 노드가 지닌 가지의 수를 말하며 ‘차수’라고도 함      Leaf Node(Terminal Node) : 하위에 다른 노드가 연결되어 있지 않은 노드      Internal Node : Leaf노드를 제외한 중간에 위치한 노드들      Depth(=Height) : 트리에 가장 큰 Level의 숫자      Type of TreesBinary Tree  자식 Node가 최대 둘 뿐인 TreeDecision Tree중에 가장 많이 사용되는 CART(Classification and Regression Tree)가 있다.Balanced Tree  어느 한쪽으로 데이터가 치우치지 않도록 균형을 지킬 수 있는 규칙을 가지고 있다.검색용 tree들은 다 검색 효율성을 위해 balanced tree로 정의[Ex. B-tree, B+ tree]Graphs  그래프는 관계를 모델링 하기 위한 자료구조  용어          Vertex(=Node) : Tree에서 Node와 같은 개념      Edge(=link) : 정점과 정점을 있는 선      weight : edge의 가중치 값      degree : Vertex에 연결되어 있는 Edge 수                  out-degree : 방향이 있는 그래프에서 정점에서부터 출발하는 간선의 수in-degree : 방향이 있는 그래프에서 정점으로부터 들어오는 간선의 수                    Path : \\(V_i\\)에서 \\(V_j\\)까지 간선으로 연결된 정점을 순서대로 나열한 리스트                  Ex. A -&gt; E : {A, B, D, E}                    Path length : 경로를 구성하는 간선의 수      Cycle : 경로 중에서 경로의 시작 정점과 마지막 정점이 같은 경로      Undirected Graph  두 정점을 연결하는 간선에 방향이 없는 그래프, 가장 기본적  [ (A, B) == (B, A) ]Directed Graph  간선에 방향이 있어 정해진 방향으로만 이동할 수 있는 그래프 [ &lt;A, B&gt; : A는 출발 정점, B는 도착 정점 ]Weighted Graph  정점을 연결하는 간선에 가중치(Weight)를 할당한 그래프Basic Algorithm Design  Algorithm은 어떠한 문제를 해결하기 위해 정해진 일련의 절차나 방법을 공식화한 형태로 표현한 것  Algorithm은 입력, 출력, 명확성, 유한성, 효율성의 조건을 만족해야 한다.  좋은 Algorithm이란 효율성을 고려한 Algorithm -&gt; 공간복잡도와 시간복잡도를 고려해 알고리즘을 짜야함[‘efficiency’]Sorting Algorithm  주어진 데이터를 정해진 순서대로 재배열하는 알고리즘이다.-&gt; 데이터 간의 비교가 가능해야 함-&gt; 비교를 하려면 기준이 있어야 함-&gt; 기준을 정하려면 계산 방법이 있어야 함Bubble Sort  Time complexity : \\(O(N^2)\\)  인접한 두 원소를 비교하면서 큰 값을 뒤로 보내며 정렬이 이루어짐Selection Sort  Time complexity : \\(O(N^2)\\)  리스트에서 최솟값을 찾아 맨 앞의 요소와 위치를 바꾼다.Insertion Sorting  리스트의 요소를 하나씩 가져와서 이미 정렬된 앞 부분과 비교하여 적절한 위치에 삽입함.Quick Sort  분할 정복(divide and conquer)방법을 통해 리스트를 정렬하는 알고리즘          분할 정복이란? 복잡한 전체의 문제를 부분으로 나누어, 부분적인 문제를 해결하고 다시 합쳐 전체를 해결하는 방식        구현 방식Searching AlgorithmLinear Search(순차 탐색)  Time complexity : \\(O(N)\\)  처음부터 끝까지 순서대로 모든 데이터를 탐색하는 방법Binary Search(이진 탐색)  정렬된 데이터에서 특정 값을 찾을 때 사용하는 방법[Time complexity : \\(O(log_{2}N)\\)]  IF 정렬이 안되어있을 때, 정렬하는데 O(NlogN)시간을 지불하고 사용 가능!정렬이 되어있지 않을 때- Linear Search : m * O(N)- Binary Search : O(NlogN) + m * O(logN)&gt; m &lt; logN 이면, Linear Search를 사용, 그렇지 않으면 Binary Search를 사용",
      "url": "/2024/12/08/Data-Structure-and-Algorithms.html"
    },
  
    {
      "title": "Introdiction to Computer Science",
      "tags": "Computer_Science",
      "desc": "Introdiction to Computer Science - 2024년12월07일  tag : Computer_Science|",
      "content": "Introdiction to Computer Science - 2024년12월07일  tag : Computer_Science|Computer Science and Engineering(=CSE)  자동화 및 정보를 연구하는 학문  분야          Programming Language      Coding Test      Computer Architecture      Operating System      Computer Network      DataBase System      Software Engineering        CSE는 컴퓨터를 이용하여 주어진 문제를 계산 가능하게 하는 것에 있다.  Computational Thinking : 컴퓨터가 이해할 수 있게 문제를 정의    과정1. Decomposition : 전체 문제를 작은 여러가지 문제로 나누는 과정2. Pattern Recognition : 주어진 문제의 반복적으로 나타나는 패턴을 찾는 과정 -&gt; through 데이터 분석3. Abstraction : 문제에서 해결해야할 주요한 정보를 수치화(또는 계산 가능하게)하는 과정4. Algorithmic Thinking : 문제 해결 방식을 step-by-step 으로 만드는 과정        Problem Solving : 효율적으로 컴퓨터가 계산할 수 있게 하는 영역          자료구조나 알고리즘을 사용하여 주어진 문제에 대한 풀이법을 코드로 작성하는 과정      CS(컴퓨터 과학)  컴퓨터 이론  알고리즘  프로그램 언어  설계  수학적 지식이 필요하며, Data Science 지식도 필요하다.프로그램을 만들기 위한 기본 코딩능력이 있어야 하며, 창의적 사고도 필요하다.CE(컴퓨터 공학)  전자 공학(EE, Electrical Engineering)을 기반으로 컴퓨터와 관련된 전자공학에 컴퓨터 과학을 접목한 학문",
      "url": "/2024/12/07/Introduction-to-Computer-Science.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 007",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 007 - 2024년12월06일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 007 - 2024년12월06일  tag : Discovering_Statistics_Using_R|R|회귀  regression analisys : 하나의 모형을 자료에 적합시키고 그것을 이용해서 하나 이상의 독립변수들로부터 종속변수의 값들을 예측하는 것  단순 회귀 : 한 예측변수로부터 한 결과변수를 예측한는 것  다중 회귀 : 여러 개의 예측변수들로부터 한 결과변수를 예측하는 것  잔차 : 선(=모형)과 실제 자료의 수직 거리모형이 자료에서 벗어난 정도 = \\(\\sum(관측값 - 모형)^2\\) = \\(총제곱합(= SS_T)\\) = \\(잔차제곱합(=SS_R)\\)\\(SS_M\\) : Y의 평균값과 회귀선의 차이들로 이루어진다.\\(R^2 = \\frac{SS_M}{SS_T}\\)회귀분석인 경우 자유도는 N-P-1 [ N : 표본 크기, P : 예측변수의 개수]R을 이용한 회귀분석단순 회귀# lm(결과변수 ~ 예측변수(들), data = dataFrame, na.action = 결측값 처리 방식)&gt; albumSales.1 &lt;- lm(sales ~ adverts, data = album1)&gt; summary(albumSales.1)Call:lm(formula = sales ~ adverts, data = album1)Residuals:     Min       1Q   Median       3Q      Max-152.949  -43.796   -0.393   37.040  211.866Coefficients:             Estimate Std. Error t value Pr(&gt;|t|)    (Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***adverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 65.99 on 198 degrees of freedomMultiple R-squared:  0.3346,\tAdjusted R-squared:  0.3313F-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16  \\(R^2\\) = 0.3346# 피어슨 상관계수(R)&gt; sqrt(0.3346)[1] 0.5784462  F-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16          F 비는 99.59 [ 이 값은  p&lt;0.001수준에서 유의하다 ]이 결과는 만일 귀무가설이 참이라면 이 F비가 나올 확률이 0.1% 미만이라는 뜻따라서, 회귀모형을 이용해서 음반 판매량을 예측하는 것이 음반 판매량의 평균을 사용해서 판매량을 예측할 때보다 유의하게 더 낫다는 결론을 내릴 수 있다.        Pr(&gt;|t|) &lt;2e-16          이 관측 유의확률이 0.05보다 작으면, 그 결과가 진짜 효과를 반영한다는데 동의따라서, 광고비가 음반 판매량 예측에 유의하게(p &lt; 0.001) 기여한다.        $$ 음반 판매량 = b_0 + (b_1 * 광고비) = 134.14 + (0.096 * 광고비_i)다중회귀\\(R^2, AIC\\)  \\(결과_i = (모형) + 오차_i\\)\\(R^2\\)의 큰 문제점은 모형에 변수를 추가할수록 \\(R^2\\) 값이 점점 커진다는 점이를 극복하기 위한 수단으로, 아카이케 정보기준(akaike information criterion, AIC)를 사용  : 예측변수가 많을수록 벌점을 준다는 특징이 있다.\\(AIC = nln(\\frac{SSE}{n}) + 2k\\) -&gt; SSE : 모형의 오차제곱합, k는 예측변수 개수[벌점에 해당]AIC값이 크다는 것은 모형이 덜 적합하다는 뜻특이한 점은 AIC 값이 크다 작다를 판단할 수 있는 기준점이 없다는 점, 그리고 같은 자료에 대한 모형들만 비교할 수 있다는 점예측변수 선택 방법  위계적 방법          과거 연구에 기초해서 예측변수들을 선택하되, 그 예측변수들을 모형에 도입하는 순서를 실험자가 결정        강제 도입법          모든 예측변수를 모형에 동시에 도입하는 것        단계별 방법          전진 : 상수(\\(b_0\\))만 있는 상태로 시작한 후 변수 하나씩 추가하여 AIC를 사용하여 추가할지 판단후진 : 모든 예측변수를 모형에 추가한 후 변수를 하나씩 제거하면서 AIC가 낮아지는지 확인단계별 방법을 사용할 때는 후진 방법이 좋다.이는 억제인자 효과 때문인데, 예측변수가 효과를 가지되 다른 어떤 변수를 고정했을 때만 효과를 가지는 경우에 발생전진 방향에서는 제2종 오류를 범할 위험이 크다.        전부분집합 방법          단계별 방법의 문제점 : 이미 모형에 존재하는 다른 변수들에 기초해서 한 변수의 적합도를 평가전부분집합 회귀 : 변수들의 모든 집합(부분집합)을 시도해서 최량적합을 찾는다.단점은 예측변수의 수가 늘어남에 따라 가능한 부분집합의 수가 지수적으로 늘어난다.      회귀모형의 정확도 평가  모형이 자료에 잘 적합하는가?  모형이 다른 표본들로도 일반화되는가?이상치와 잔차IF 이상치가 존재, then 잔차가 큰 값이 존재.-&gt; 이러한 잔차는 비표준화잔차라고 하는데 이 문제를 극복하기 위해 표준화잔차를 사용-&gt; 표준화잔차 = 보통의 잔차 / 표준편차 추정값  규칙          크기(절댓값)가 3.29보다 큰 표준화잔차는 문제가 될 수 있다.      표준화잔차의 크기가 2.58보다 큰 표본 사례들이 전체 표본 사례의 1% 이상이라는 것은 모형의 오차 수준이 받아들일 수 없는 정도라는 증거      표준화잔차의 크기가 1.96보다 큰 표본 사례가 전체의 5% 이상이라는 것은 모형이 실제 자료를 그리 잘 대표하지 않는다는 것      영향력이 큰 사례들  모형의 매개변수들에 지나치게 큰 영향을 주는 사례들을 살펴보기          이런 사례가 있다면, 삭제했을 때 회귀계수들이 달라지는지를 확인해야 한다.        수정 예측값(adjusted predicted value) : 해당 사례를 제외한 자료로 만든 모형이 예측한 값          만일 해당 사례가 사실은 모형에 그리 큰 영향을 미치지 않았다면 수정 예측값은 원래의 예측값과 아주 비슷할 것DFFit : 수정 예측값과 원래의 예측값의 차이        제외 잔차 : 수정 예측값에 기초한 잔차  스튜던트화 잔차 : 제외 잔차를 표준오차로 나누어서 표준화한 값  쿡의 거리 : 모형에 대한 사례 하나의 전반적인 영향력을 측정한 것 -&gt; 이 거리가 1보다 크면 문제가 될 수 있음을 제시  hat value(=지렛대(leverage)) : 결과변수의 실제 측정값이 예측값에 미치는 영향을 측정한 것          평균 지렛대 값 = \\((k+1) / n\\) [ k : 모형의 예측변수 개수, n : 참가자의 수 ]지렛대 값은 0(사례가 예측에 아무런 영향을 주지 않음) ~ 1(사례가 예측에 완전한 영향을 줌)까지      - 중요한 한마디만일 어떤 점이 Y 축에서 유의한 이상치로 판명되었지만 그 점의 쿡의 거리가 &lt; 1 이면, 그 점은 회귀분석에 큰 영향을 미치지 않으므로 굳이 삭제할 필요가 없다.그러나, 모형이 그런 점에 적합되지 않은 이유를 이해하기 위해 그런 점을 좀 더 연구할 필요는 있다.회귀모형의 평가 : 일반화가정 검정  한 표본으로 얻은 모형으로 어떤 모집단에 관한 결론을 얻으려면 다음과 같은 여러 가정이 성립해야 한다.          변수의 종류                  모든 예측변수는 양적 변수 또는 범주형 변수 이어야 한다.결과변수는 반드시 연속이자 비유계(unbounded)인 양적 변수이어야 한다.                    0이 아닌 분산 : 예측변수의 분산이 0이 아니어야 한다.      완전 다중공선성의 부재 : 둘 이상의 예측변수 사이에 완전한 선형 관계가 없어야 한다.      ‘외부 변수’와는 무관한 예측변수 : 그 어떤 변수도 회귀모형에 포함된 임의의 예측변수와 상관이 없어야 한다는 뜻                  결과를 모형만큼이나 잘 예측할 수 있는 다른 변수들이 존재하는 것이므로                    등분산성 : 예측변수들의 각 수준에서 잔차 항들의 분산이 일정해야 한다.      오차의 독립성 : 임의의 두 관측값의 잔차들이 무관해야 한다.(즉, 독립적이여야 한다.)                  이를 자기상관(autocorrelation)이 없다고 말한다.오차들의 이연상관을 검사하는 더빈-왓슨 검정으로 확인할 수 있다.                    오차의 정규분포 : 모형의 잔차들이 평균이 0인 정규분포를 따르는 무작위 값      독립성 : 이 가정은 결과변수의 모든 값이 독립적이여야 한다.      선형성 : 예측변수의 값이 증가함에 따라 결과변수의 평균값들이 하나의 직선을 형성해야 한다.      모형의 교차 타당성 검증  수정 \\(R^2\\) : 예측 능력의 손실정도를 나타낸다.          \\(R^2\\) 값이 표본에서 얻은 회귀모형이 Y의 변동을 얼마나 설명하는지 말해주는 값수정 \\(R^2\\) 값은 만일 표본이 얻은 모집단으로부터 회귀모형을 유도했다면 그 모형이 Y의 변동을 얼마나 설명할 것인지를 말해주는 값R은 훼리 방정식을 통해 수정된 \\(R^2\\) 값을 계산해주는데 이 공식은 회귀모형이 완전히 다른 자료 집합을 얼마나 잘 예측할 것인지에 관해서는 아무것도 말해주지 않는다.따라서 스타인의 공식을 사용한다. -&gt; \\(수정 R^2 = 1 - [(\\frac{n-1}{n-k-1})(\\frac{n-2}{n-k-2})(\\frac{n+1}{n})](1-R^2)\\) | n : 참가자의 수, k : 모형의 예측변수의 개수      표본의 개수  최소 표본 크기에 관한 일반 규칙          회귀모형의 전반적인 적합도를 검사할 때[Ex. \\(R^2\\)] -&gt; 50 + 8k [ k : 예측변수의 개수]      모형의 개별 예측변수를 검사할 때[Ex. b값] -&gt; 104 + k                  둘 다 관심이 있다면 큰 값을 사용하면 OK                    공선성  다중공선성 : 회귀모형에 있는 둘 이상의 예측변수들 사이에 강한 상관관계가 존재하는 것          만약 하나의 예측변수가 다른 어떤 예측변수와 완벽한 선형관계일 때 완전공선성이 존재한다고 말한다.        공선성이 높으면 생기는 문제점          b들을 믿을 수 없게 된다.      R의 크기가 제한된다.      예측변수들의 중요도 평가가 어렵다.                  해결법 : 분산팽창인자(variance inflation factor, VIF) : 주어진 한 예측변수가 다른 예측변수(들)와 강한 선형 관계를 가지고 있는지를 말한다.VIF값이 10 이상이라면 걱정할 필요가 있다고 제시또한, 만일 평균 VIF값이 1보다 크면 다중공선성 때문에 회귀모형이 편향될 수 있다.                    R을 이용한 다중상관 분석&gt; albumSales.2&lt;-lm(sales ~ adverts, data = album2)&gt; albumSales.3&lt;-lm(sales ~ adverts + airplay + attract, data = album2) # 다중 회귀&gt; summary(albumSales.2)Call:lm(formula = sales ~ adverts, data = album2)Residuals:     Min       1Q   Median       3Q      Max-152.949  -43.796   -0.393   37.040  211.866Coefficients:             Estimate Std. Error t value Pr(&gt;|t|)    (Intercept) 1.341e+02  7.537e+00  17.799   &lt;2e-16 ***adverts     9.612e-02  9.632e-03   9.979   &lt;2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 65.99 on 198 degrees of freedomMultiple R-squared:  0.3346,\tAdjusted R-squared:  0.3313F-statistic: 99.59 on 1 and 198 DF,  p-value: &lt; 2.2e-16  R = 0.3346 : 광고비가 음반 판매량 변동의 33.5%를 설명\\(R^2 = 0.5784\\)&gt; summary(albumSales.3)Call:lm(formula = sales ~ adverts + airplay + attract, data = album2)Residuals:     Min       1Q   Median       3Q      Max-121.324  -28.336   -0.451   28.967  144.132Coefficients:              Estimate Std. Error t value Pr(&gt;|t|)    (Intercept) -26.612958  17.350001  -1.534    0.127    adverts       0.084885   0.006923  12.261  &lt; 2e-16 ***airplay       3.367425   0.277771  12.123  &lt; 2e-16 ***attract      11.086335   2.437849   4.548 9.49e-06 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 47.09 on 196 degrees of freedomMultiple R-squared:  0.6647,\tAdjusted R-squared:  0.6595F-statistic: 129.5 on 3 and 196 DF,  p-value: &lt; 2.2e-16  R = 0.6647 : 예측변수들이 음반 판매량 변동의 66.5%를 설명따라서 방송 횟수와 매력은 또 다른 33%(= 66.5% - 33.5%)에 해당판매량 = -26.61 + (0.08 * 광고비) + (3.38 * 방송홧수) + (11.09 * 매력)Pr(&gt;|t|) &lt; 0.05 보다 작으면 해당 예측변수는 모형에 유의한 수준으로 기여한다고 할 수 있다. 그리고 t값이 클수록 예측변수의 기여도가 커진다.&gt; lm.beta(albumSales.3) # 표준화된 벡터를 구하는 공식  adverts   airplay   attract0.5108462 0.5119881 0.1916834  이 추정값들은 예측변수의 표준편차가 1 단위 변할 때의 결과변수의 표준편차의 변화량이다.&gt; confint(albumSales.3) # 각 추정값의 신뢰구간을 구하는 함수                   2.5 %      97.5 %(Intercept) -60.82960967  7.60369295adverts       0.07123166  0.09853799airplay       2.81962186  3.91522848attract       6.27855218 15.89411823 # 조금 넓긴 한데 0을 포함하지 않고 있으며 다른 변수들보다 이 변수의 매개변수 추정값이 참값을 덜 대표한다.  좋은 모형의 신뢰구간은 그 범위가 좁다.b의 부호(양, 음)는 예측변수와 결과변수의 관계의 방향을 말해준다.따라서 아주 나쁜 모형의 신뢰구간은 0을 포함할 것이다.모형의 비교  둘째 모형의 \\(R^2\\)이 첫 모형의 \\(R^2\\)보다 유의하게 더 큰지 판단해야 한다.\\(R^2\\)의 유의성은 F비로 검사할 수 있다.\\(F = \\frac{(N-k-1)R^2}{k(1-R^2)}\\)&gt; anova(albumSales.2, albumSales.3)Analysis of Variance TableModel 1: sales ~ advertsModel 2: sales ~ adverts + airplay + attract  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    1    198 862264                                  2    196 434575  2    427690 96.447 &lt; 2.2e-16 ***---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  F 값 = 96.447, Pr(&gt;F) = 2.2e-16 &lt; 0.001 이므로 albumSales.2에 비해 albumSales.3가 자료에 유의한 수준으로 더 적합이상치와 영향이 큰 사례들&gt; album2$residuals&lt;-resid(albumSales.3) # 잔차&gt; album2$standardized.residuals &lt;- rstandard(albumSales.3) # 표준화잔차&gt; album2$studentized.residuals &lt;- rstudent(albumSales.3) # 스튜던트화 잔차&gt; album2$cooks.distance&lt;-cooks.distance(albumSales.3) # 쿡의 거리&gt; album2$dfbeta &lt;- dfbeta(albumSales.3) # DFBeta&gt; album2$dffit &lt;- dffits(albumSales.3) # DFFit&gt; album2$leverage &lt;- hatvalues(albumSales.3) # hat-value&gt; album2$covariance.ratios &lt;- covratio(albumSales.3) # 공분산비# 보통의 표본에서는 사례의 95%는 표준화잔차가 약 +-2% 이내라고 기대한다.&gt; album2$large.residual &lt;- album2$standardized.residuals &gt; 2 | album2$standardized.residuals &lt; -2 # 각 사례에 잔차가 -2보다 작거나 +2보다 큰 값 찾기&gt; sum(album2$large.residual) # 12# 공분산비에 대해서 기준 적용# CVR_i &gt; 1 + [3(k+1)/n] = 1 + [3(3+1)/200] = 1.06# CVR_i &lt; 1 - [3(k+1)/n] = 1 - [3(3+1)/200] = 0.94&gt; album2$out.cvr &lt;- album2$covariance.ratios &gt; 1.06 | album2$covariance.ratios &lt; 0.94&gt; sum(album2$out.cvr) # 21독립성 가정의 평가  durbinWatsonTest()    &gt; durbinWatsonTest(albumSales.3) # == dwt(albumSales.3) lag Autocorrelation D-W Statistic p-value 1       0.0026951      1.949819   0.722 Alternative hypothesis: rho != 0        검정 통계량이 1보다 작거나 3보다 크면 주의할 필요가 있다는 것[이 값은 2에 가까울수록 좋다, 여기서는 1.95]여기서 p &gt; 0.05 이므로, 이 검정통계량은 전혀 유의하지 않다.다중공선성 부재 가정의 평가  VIF # 만일 VIF 값이 10보다 크면 문제의 여지가 있다.    &gt; vif(albumSales.3) adverts  airplay  attract1.014593 1.042504 1.038455        허용 통계량 = 1/VIF # 허용 통계량이 0.2보다 작으면 모형에 뭔가 문제가 있을 가능성이 있다. 0.1보다 작으면 모형에 심각한 문제가 있는 것    &gt; 1/vif(albumSales.3)adverts   airplay   attract0.9856172 0.9592287 0.9629695        평균 VIF # 평균 VIF값이 1보다 확연하게 크면 회귀모형이 편향되어있을 수 있다.    &gt; mean(vif(albumSales.3))[1] 1.03185        결론 : 우리의 모형에 다중공선성이 존재하지는 않는다고 결론을 내려도 안전잔차에 관한 여러 가정의 검정&gt; plot(albumSales.3)        가정이 깨졌다면? -&gt; 강건한 회귀 : 부트스트래핑&gt; bootReg&lt;-function(formula, data, i)+ {+     d &lt;- data[i,]+     fit &lt;- lm(formula, data = d)+     return(coef(fit)) # 주어진 모형의 절편과 예측변수들의 기울기 계수들을 출력+ }&gt; bootResults&lt;-boot(statistic = bootReg, formula = sales ~ adverts + airplay + attract, data = album2, R = 2000) # 부트스트랩 표본 얻기&gt; boot.ci(bootResults, type = \"bca\", index = 1) # bca : 신뢰구간의 종류, index : 통계량의 위치(1 = 절편)BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONSBased on 2000 bootstrap replicatesCALL :boot.ci(boot.out = bootResults, type = \"bca\", index = 1)Intervals :Level       BCa          95%   (-55.26,   9.02 )  Calculations and Intervals on Original Scale  절편의 신뢰구간 : -55.26 ~ 9.02이러한 신뢰구간은 앞에서 대입(plug-in) 접근방식으로 얻은 신뢰구간과 비교해보자(여기서는 상당히 비슷하다)다중회귀의 보고            —      \\(R^2\\)      B      SE B      \\(\\beta\\)      p              단계 1      0.34      —      —      —      &lt;.001              상수      —      134.14      7.54      —      &lt;.001              광고비      —      0.10      0.01      0.58*      &lt;.001              단계 2      0.33      —      —      —      &lt;.001              상수      —      -26.61      17.35      —      .127              광고비      —      0.09      0.01      0.51*      &lt;.001              BBC 라디오 1 방송 횟수      —      3.37      0.28      0.51*      &lt;.001              밴드 매력      —      11.09      2.44      0.19*      &lt;.001      범주형 예측변수와 다중회귀  범주형 변수를 예측변수로 사용하려고 할 때 문제는, 사용하려는 범주형 변수의 범주의 개수가 두 개를 넘는다.Ex. 종교 : 이슬람교, 유대교, 가톨릭, …해결책 -&gt; 가변수(dummy variable)사용  [ 부호화하고자 하는 그룹(집단)의 개수에서 1을 뺀 것만큼 변수를 생성]가변수  가변수 과정          부호화하려는 그룹의 수에서 1을 뺀다      단계 1에서 구한 개수만큼 새 변수를 만든다. [ = 이들이 가변수 ]      그룹 중 하나를 기저(baseline) 그룹으로 선택한다.      모든 가변수에 대해, 기저 그룹에 그랍 값 0을 배정한다.      첫 가변수에 대해, 기저 그룹과 비교할 첫 그룹에 값 1을 배정한다.      둘째 가변수에 대해, 기저 그룹과 비교할 둘째 그룹에 값 1을 배정한다.      나머지 반복      이 가변수들을 모두 회귀모형에 추가      &gt; head(gfr, n = 10)   ticknumb                  music day1 day2 day3 change1      2111               Metaller 2.65 1.35 1.61  -1.042      2229                 Crusty 0.97 1.41 0.29  -0.683      2338 No Musical Affiliation 0.84   NA   NA     NA4      2384                 Crusty 3.03   NA   NA     NA5      2401 No Musical Affiliation 0.88 0.08   NA     NA6      2405                 Crusty 0.85   NA   NA     NA7      2467              Indie Kid 1.56   NA   NA     NA8      2478              Indie Kid 3.02   NA   NA     NA9      2490                 Crusty 2.29   NA   NA     NA10     2504 No Musical Affiliation 1.11 0.44 0.55  -0.56  music 컬럼이 문자열&gt; gfr$music &lt;- factor(gfr$msuic) # 요인으로 변경&gt; contrasts(gfr$music)&lt;-contr.treatment(4, base = 4)# contrasts(gfr$music) : gfr 데이터프레임의 변수 music 에 대한 대비를 설정# contr.treatment(그룹 수, base = 기저 그룹의 번호) : 모든 그룹을 하나의 기저 조건과 비교한 결과에 기초해서 대비를 계산&gt; gfr$musicattr(,\"contrasts\")                       1 2 3Crusty                 1 0 0Indie Kid              0 1 0Metaller               0 0 1No Musical Affiliation 0 0 0Levels: Crusty Indie Kid Metaller No Musical Affiliation# 위 과정에 변수 이름 설정하는 방법&gt; crusty_v_NMA&lt;-c(1, 0, 0, 0)&gt; indie_v_NMA&lt;-c(0, 1, 0, 0)&gt; metal_v_NMA&lt;-c(0, 0, 1, 0)&gt; contrasts(gfr$music)&lt;-cbind(crusty_v_NMA, indie_v_NMA, metal_v_NMA)attr(,\"contrasts\")                       crusty_v_NMA indie_v_NMA metal_v_NMACrusty                            1           0           0Indie Kid                         0           1           0Metaller                          0           0           1No Musical Affiliation            0           0           0Levels: Crusty Indie Kid Metaller No Musical Affiliation가변수를 이용한 회귀분석&gt; glastonburyModel&lt;-lm(change ~ music, data = gfr)&gt; summary(glastonburyModel)Call:lm(formula = change ~ music, data = gfr)Residuals:     Min       1Q   Median       3Q      Max-1.82569 -0.50489  0.05593  0.42430  1.59431Coefficients:                  Estimate Std. Error t value Pr(&gt;|t|)    (Intercept)       -0.55431    0.09036  -6.134 1.15e-08 ***musiccrusty_v_NMA -0.41152    0.16703  -2.464   0.0152 *  musicindie_v_NMA  -0.40998    0.20492  -2.001   0.0477 *  musicmetal_v_NMA   0.02838    0.16033   0.177   0.8598    ---Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1Residual standard error: 0.6882 on 119 degrees of freedom  (결측으로 인하여 687개의 관측치가 삭제되었습니다.)Multiple R-squared:  0.07617,\tAdjusted R-squared:  0.05288F-statistic:  3.27 on 3 and 119 DF,  p-value: 0.02369  \\(R^2\\) = 7.6% -&gt; 위생 점수 변화량의 변동의 7.6%는 참가자의 음악 취향으로 설명된다.F 통계량에 따르면, 이 모형은 모형이 없을 때보다 위생 점수의 변화를 유의한 수준으로 더 잘 예측한다.&gt; round(tapply(gfr$change, gfr$music, mean, na.rm=TRUE), 3)                Crusty              Indie Kid               Metaller No Musical Affiliation                -0.966                 -0.964                 -0.526                 -0.554  크러스티 그룹과 무취향 그룹 평균의 차이는 -0.966 - (-0.554) = -0.412 이다.따라서, 크러스티 그룹의 위생 점수 변화가 무취향 그룹의 위생 점수 변화보다 크다.",
      "url": "/2024/12/06/Discovering-Statistics-Using-R-007.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 006",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 006 - 2024년12월05일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 006 - 2024년12월05일  tag : Discovering_Statistics_Using_R|R|상관관계를 측정하는 방법  두 변수가 어떤 식으로든 관계가 있는지 파악하는 가장 간단한 방법은 두 변수의 분산에 공통점이 있는지를 보는 것공분산  공분산 : \\(cov(x, y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{N - 1}\\)          공분산이 양수라는 것은 한 변수가 평균에서 이탈하면 다른 변수도 같은 방향으로 이탈함을 뜻한다.측정의 축척에 의존한다는 것 -&gt; 서로 다른 단위로 측정한 자료 집합들의 공분산들이 주어졌을 때 둘 중 어떤 것이 더 큰지 비교하는 것은 무의미      표준화  표준화 : 측정 척도 의존성 문제를 극복하려면 공분산을 어떤 표준 단위로 변환하는 과정  표준화된 공분사(= 상관계수) : \\(r = \\frac{COV_{xy}}{S_{x}S_{y}}\\) [ = 피어슨 상관계수 ]          \\(\\pm 0.1\\) : 작은 효과\\(\\pm 0.3\\) : 중간 효과\\(\\pm 0.5\\) : 큰 효과      상관계수의 유의성  유의성 검정하는 방법 -&gt; z-score 사용기존 z-score는 만일 자료가 정규분포라면 주어진 z 값이 나올 확률과 비교하는 식으로 활용그런데 피어슨의 r에는 그것이 정규분포가 아닌 표집분포를 따른다는 문제점이 있다.아래의 공식을 사용하면(=피셔) r의 표집분포가 실제로 정규분포가 되게 만들 수 있다.  \\(z_r = \\frac{1}{2}log_{e}\\frac{1+r}{1-r}\\)\\(SE_{z_r} = \\frac{1}{\\sqrt{N-3}}\\)  Ex. r = .87 -&gt; \\(z_r\\) = 1.33, SE = .71  여기서 우리가 알 고 싶은 것은 상관계수가 0과 다른지를 확인\\(z = \\frac{z_r}{SE_{z_r}}\\)  Ex. z = 1.33 / 0.71 = 1.87 -&gt; 이 z 값으로 한쪽꼬리 확률은 0.307 -&gt; 양쪽꼬리 확률 = 0.0307 * 2 = 0.0614  따라서 한쪽꼬리에서는 이 상관관계가 유의하고(p &lt; 0.05), 양쪼꼬리에서는 유의하지 않다.  하지만 일반적으로 z-score로 검증하기보다는 자유도가 N - 2인 t통계량을 이용해서 검증하는 것이 일반적이다.\\(t_r = \\frac{r\\sqrt{N-2}}{\\sqrt{1-r^2}}\\)r의 신뢰구간  Ex. 95% 신뢰구간을 구하는 공식          신뢰구간의 하계 = \\(\\bar{X} - (1.96 * SE)\\)신뢰구간의 상계 = \\(\\bar{X} + (1.96 * SE)\\)        변환된 상관계수의 경우          \\(신뢰구간의 하계 = z_r - (1.96 * SE_{z_{r}})\\)\\(신뢰구간의 상계 = z_r + (1.96 * SE_{z_{r}})\\)-&gt; 하계 : 1.33 - (1.96 * .71) = -0.062, 상계 : 1.33 + (1.96 * .71) = 2.72-&gt; 그런데 이 값들은 z-score의 축척을 따르므로, 원래의 상관계수 단위로 변환할 필요가 있다.-&gt; \\(r = \\frac{e^{2z_r} - 1}{e^{2z_r} + 1}\\) -&gt; r의 신뢰구간의 상계는 0.991, 하계는 -0.062      해석에 관한 경고  상관계수를 해석할 때는, 상관계수가 인과관계의 방향을 말해주지는 않는다는 점을 주의해야 한다.  제 3변수 문제 : 결과에 영향을 미치는 다른 어떤 변수가 있을 수 있으므로 두 변수 사이의 인과관계를 가정할 수 없다.  인과관계의 방향 : 상관관계가 A -&gt; B or B - A 쪽으로 방향을 말해주지는 않는다.상관분석을 위한 R# cor(x,y, use = \"everything\", method = \"correlation type\")# use : 특정한 결측값 처리 방식을 지정#  &gt; everything : NA 출력, all.obs : 결측값이 존재하면 오류 메시지 발생,#  &gt; complete.obs : 모든 변수가 완전히 갖추어진 사례들로만 상관계수를 계산[= 목록별 결측값 제외],#  &gt; pairwise.complete.obs : 모든 변수 쌍에서 두 변수 모두 관측값이 존재하는 사례들에 대해 두 변수의 상관계수를 계산[= 쌍별 결측값 제외]# method : 상관계수의 종류에 해당하는 문자열 지정[Ex. pearson, spearman, kendall]&gt; cor(examData$Exam, examData$Anxiety, use = \"complete.obs\", method = 'pearson') # -0.4409934&gt; cor.test(examData$Exam, examData$Anxiety, alternative = 'less', method = 'pearson', conf.level = 0.99)# alternative : two.sided : 양쪽꼬리 검정, 'less' : 음의 상관을 예측, 'greater' : 양의 상관을 예측# conf.level : 상관계수의 신뢰구간 너비 지정# --- 결과# Pearson's product-moment correlation# data:  examData$Exam and examData$Anxiety# t = -4.938, df = 101, p-value = 0.000001564# alternative hypothesis: true correlation is less than 0# 99 percent confidence interval:# -1.0000000 -0.2362782# sample estimates:#      cor# -0.4409934피어슨 상관계수  피어슨 r 의 가정들          피어슨 상관계수의 유일한 요구조건은 자료가 구간자료이어야 한다는 것.검정통계량이 유효하려면 표집분포가 정규분포이어야 함      &gt; cor(examData2)              Exam    Anxiety     ReviseExam     1.0000000 -0.4409934  0.3967207Anxiety -0.4409934  1.0000000 -0.7092493Revise   0.3967207 -0.7092493  1.0000000결정계수  \\(R^2\\)          한 변수의 변이성 또는 변동을 다른 변수가 어느 정도나 공유하는지 말해주는 측도EX. 상관계수 값 : -0.4410 -&gt; \\(R^2 = 0.194\\) -&gt; 두 변수의 변동 관련성은 19.4% 뿐, 나머지 80.6%는 다른 어떤 변수들과 관련되어 있다.      cor(examData2)^2           Exam   Anxiety    ReviseExam    1.0000000 0.1944752 0.1573873Anxiety 0.1944752 1.0000000 0.5030345Revise  0.1573873 0.5030345 1.0000000            스피어먼 상관계수  스피어먼 상관계수는 비모수적 통계량이다.따라서 정규분포가 아닌 자료 등 모수적 자료의 가정들을 위반하는 자료에 사용할 수 있다.&gt; cor.test(liarData$Position, liarData$Creativity, alternative = \"less\", method = \"spearman\")# Spearman's rank correlation rho# data:  liarData$Position and liarData$Creativity# S = 71948, p-value = 0.0008602# alternative hypothesis: true rho is less than 0# sample estimates:#      rho# -0.3732184  유의확률 &lt; 0.05 , 상관계수의 절댓값이 꽤 크기 때문에 두 변수는 유의한 관계가 존재한다고 결론을 내릴 수 있다.켄달의 타우(비모수적 상관계수)  자료 집합의 크기가 작고 동순위 점수들이 많을 때는 스피어먼 상관계수 대신 이 상관계수를 사용한다.&gt; cor.test(liarData$Position, liarData$Creativity, alternative = \"less\", method = \"kendal\")# \tKendall's rank correlation tau# data:  liarData$Position and liarData$Creativity# z = -3.2252, p-value = 0.0006294# alternative hypothesis: true tau is less than 0# sample estimates:#        tau# -0.3002413부트스트랩 방법을 적용한 상관분석  피어슨의 r에 깔린 가정들을 만족하지 않는 자료를 다루는 또 다른 방법&gt; bootTau&lt;-function(liarData,i)cor(liarData$Position[i], liarData$Creativity[i], use = \"complete.obs\", method = \"kendall\") # 함수 작성&gt; boot_kendall&lt;-boot(liarData, bootTau, 2000) # boot(데이터, 함수, 반복 횟수)&gt; boot_kendallORDINARY NONPARAMETRIC BOOTSTRAPCall:boot(data = liarData, statistic = bootTau, R = 2000)Bootstrap Statistics :      original        bias    std. errort1* -0.3002413 -0.0004526635  0.09607203  95% 신뢰구간 구하기&gt; boot.ci(boot_kendall, 0.95)BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONSBased on 2000 bootstrap replicatesCALL :boot.ci(boot.out = boot_kendall)Intervals :Level      Normal              Basic         95%   (-0.4881, -0.1115 )   (-0.4940, -0.1153 )  Level     Percentile            BCa          95%   (-0.4852, -0.1065 )   (-0.4763, -0.0927 )  Calculations and Intervals on Original Scale이연 상관과 점이연 상관  이연 상관 : 범주들 사이에 연속체가 있는 경우 -&gt; 시험 통과 여부 [ 100점으로 통과, 75점으로 가까스로 통과 ]  점이연 상관 : 범주들 사이에 연속체가 전혀 없는 경우 -&gt; 사망 여부&gt; catData   time gender recode1    41      1      02    40      0      13    40      1      04    38      1      0# 점이연&gt; cor.test(catData$time, catData$gender)# \tPearson's product-moment correlation# data:  catData$time and catData$gender# t = 3.1138, df = 58, p-value = 0.002868# alternative hypothesis: true correlation is not equal to 0# 95 percent confidence interval:#  0.137769 0.576936# sample estimates:#       cor# 0.3784542  이연, 점이연 상관에서 상관계수의 부호는 전적으로 어떤 범주에 어떤 코드를 배정하는가에 달려 있을 뿐이므로, 상관의 방향에 관한 모든 정보는 무시해야 한다.\\(R^2 = 0.143\\) -&gt; 고양이의 성별이 고양이가 집 밖에서 보낸 시간의 변동의 14.3%를 설명한다는 결론  점이연 -&gt; 이연 상관계수로 변경          \\(r_b = \\frac{r_{pb}\\sqrt{pq}}{y}\\)\\(r_b\\) : 이연 상관계수\\(r_{pb}\\) : 점이연 상관계수p : 가장 큰 범주에 속하는 사례들의 비율q : 가장 작은 범주에 속하는 사례들의 비율      &gt; catFrequencies&lt;-table(catData$gender)&gt; prop.table(catFrequencies)        0         10.5333333 0.4666667# 수고양이(1)의 비율 : 0.467, 암고양이(0)의 비율 : 0.533# 적은 비율인 수고양이 값을 q, 암고양이 비율을 p&gt; polyserial(catData$time, catData$gender) # 이연 상관계수[1] 0.4749256  이연 상관계수의 유의성을 판정하려면 표준오차를 구해야 함  \\(SE_{r_b} = \\frac{\\sqrt{pq}}{y\\sqrt{N}}\\)Ex. \\(SE_{r_b}\\) = 0.162 -&gt; \\(z_{r_b} = \\frac{r_b - \\bar{r_b}}{SE_{r_b}}\\)모집단의 평균이 0이라고 가정(귀무가설)했으므로 -&gt; \\(\\bar{r_b} = 0\\) -&gt; \\(z_{r_b} = 2.93\\)한쪽꼬리 확률 : 0.00169 -&gt; 양쪽꼬리 확률 : 0.00338 &lt; 0.01 이므로 유의편상관  편상관 : 다른 변수의 효과를 고정한 상태에서의 두 변수의 상관&gt; pc&lt;-pcor(c(\"Exam\", \"Anxiety\", \"Revise\"), var(examData2)) # pcor(c(\"변수1\", \"변수2\", \"제어 변수1\", \"제어 변수2\", ...), var(데이터프레임))&gt; pc # Exam 과 Anxiety 두 변수의 편상관계수[1] -0.2466658  Exam : 시험 점수, Anxiety : 시험 불안, Revise : 복습 시간&gt; pc^2 # 결정계수[1] 0.06084403  유의성 판정&gt; pcor.test(pc, 1, 103) # pcor로 만든 객체, 제어 변수의 개수, 표본 크기$tval[1] -2.545307$df[1] 100$pvalue[1] 0.01244581  P &lt; 0.05 로 유의하지만, \\(R^2 = 0.06\\)이므로 시험 불안이 시험 성적 변동의 6%만 공유한다는 뜻",
      "url": "/2024/12/05/Discovering-Statistics-Using-R-006.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 005",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 005 - 2024년12월04일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 005 - 2024년12월04일  tag : Discovering_Statistics_Using_R|R|자료에 관한 가정모수적 자료의 가정들  모수적 검정 : 통계학자들이 밝혀낸 무수히 많은 분포 중 하나를 따르는 자료를 요구하는 검정          그 자료는 반드시 특정한 가정들을 만족하는 모수적 자료      ** 모수적 검정의 네 가지 기본 가정  분포의 정규성 : 가설 검정에 깔린 논리는 뭔가[Ex. 표집분포, 모형의 오차]가 정규분포를 따른다는 점에 의존  분산의 동질성 : 자료 전반에서 분산들이 동일하다는 뜻  구간 자료 : 자료가 적어도 구간 수준에서 측정한 것  독립성 : 서로 다른 참가자들의 자료가 독립적이어야 함정규성 가정  정규성을 점검하는 것이 유익하다. [ 단순히 표본이 크거나 표본 자료가 정규분포를 따르는 경향이 있더라도 !]눈으로 정규성 확인하기histogram&gt; dlf &lt;- read.delim(\"data/DownloadFestival(No Outlier).dat\", header=TRUE) # DataFrame 불러오기&gt; hist.day1 &lt;- ggplot(dlf, aes(day1)) + theme(legend.position = \"none\") + geom_histogram(aes(y=..density..), colour=\"black\", fill=\"white\") + labs(x=\"Hygiene score on day 1\", y = \"Density\")&gt; hist.day1 + stat_function(fun = dnorm, args = list(mean = mean(dlf$day1, na.rm = TRUE), sd = sd(dlf$day1, na.rm = TRUE)), colour = \"black\", size = 1)# hist.day2, hist.day3 도 그리기        ggplot(dlf, aes(day1)) : dlf 데이터프레임에 있는 day1 변수를 그리기geom_histogram(aes(y=..density..), colour=”black”, fill=”white”) : 선을 검은색으로 막대 내부를 흰색으로stat_function(fun = dnorm : 정규 곡선을 그리기mean = mean(dlf$day1, na.rm = TRUE) : 정규분포의 평균을 결측값을 모두 제거한 후의 day1 변수의 평균으로 설정sd = sd(dlf$day1, na.rm = TRUE) : 정규분포의 표준편차를 day1 변수의 표준편차로 설정Q-Q plot&gt; ggplot(dlf, aes(sample=day1)) + stat_qq()&gt; ggplot(dlf, aes(sample=day2)) + stat_qq()&gt; ggplot(dlf, aes(sample=day3)) + stat_qq()            변수 day1 이 가장 정규분포에 가깝다고 생각할 수 있다.  정규성의 수량화describe()&gt; describe(dlf$day1)&gt; describe(dlf[, c(\"day1\", \"day2\", \"day3\")]) # 묶어서 한번에 보기# basic = FALSE : 기초 통계량 제외, norm = TRUE : 점수들의 분포에 관련된 통계량들을 표시&gt; stat.desc(dlf[, c(\"day1\", \"day2\", \"day3\")], basic = FALSE, norm = TRUE) # 묶어서 한번에 보기&gt; round(stat.desc(dlf[, c(\"day1\", \"day2\", \"day3\")], basic = FALSE, norm = TRUE), digits = 3) # 소수점 3째 자리까지 반올림해서 보기  정규분포에서는 비대칭도(skewness)와 첨도(kurtosis)가 모두 0이어야 한다.비대칭도와 첨도가 유용하긴 하지만, 이 값들을 z-score 로 변환해 보는 것도 좋다.z-score : 실제 점수를 평균이 0, 표준편차가 1인 분포에 맞추어 변환한 값  z-score 로 변환하는 값이 유용한 이유1. 서로 다른 측정 방법으로 얻은 서로 다른 표본들의 비대칭도와 첨도를 비교할 수 있다는 점2. 자료의 비대칭도와 첨도가 발생할 확률을 파악할 수 있다는 점    | \\(z_{비대칭도} = \\frac{S - 평균}{SE_{비대칭도}}\\) | \\(z_{첨도} = \\frac{K - 평균}{SE_{첨도}}\\) |절댓값이 1.96보다 큰 z-score는 p &lt; 0.05 에서 유의 [크기가 작은 표본]절댓값이 2.58보다 큰 z-score는 p &lt; 0.01 에서 유의  [큰 표본]절댓값이 3.29보다 큰 z-score는 p &lt; 0.001 에서 유의 [아주 큰 표본]  skew.2SE, kurt.2SE : 비대칭도와 첨도를 두 표준오차로 나눈 값          z &gt; 1.96 -&gt; \\(\\frac{k}{1.96 * SE} &gt; 1\\) [p &lt; 0.05, 단! 크기가 작은 표본에서 유의!]      EX. skew.2SE : -0.025 3.61 2.30 -&gt; day2, day3의 분포가 유의하게 기울었음을 의미            자료 그룹들 살펴보기&gt; rexam &lt;- read.delim(\"data/RExam.dat\", header = TRUE)&gt; rexam$uni&lt;-factor(rexam$uni, levels = c(0:1), labels = c(\"Duncetown University\", \"Sussex University\"))  rexam 데이터프레임에 uni 컬럼의 값 0, 1 을 “Duncetown University”, “Sussex University” 로 대체&gt; by(data=rexam$exam, INDICES=rexam$uni, FUN=describe) # data : 분석할 자료, INDICES : 그룹화 변수, FUN : 자료에 적용할 함수&gt; by(rexam[, c(\"exam\", \"numeracy\")], rexam$uni, stat.desc, basic = FALSE, norm = TRUE)분포의 정규성 검정샤피로-윌크 검정(Shapiro-Wilk test)  표본의 점수들을 그 표본과 표준편차가 같은 정규분포에서 뽑은 점수들과 비교이 검정에서 유의하지 않은 결과(p &gt; 0.05)가 나왔다면, 표본 분포가 정규분포와 그리 다르지 않다는 것  주의!          이 검정에서 유의한 결과가 나왔다고 해서, 이 자료의 분포는 정규분포에서 많이 벗어나니 정규성을 가정한 통계적 절차들을 사용하면 안된다고 성급하게 판단하면 안 된다.이유 : 표본이 크면 정규분포를 조금만 벗어나도 유의한 결과가 나오기 쉽다는 점을 고려따라서 검정에만 의존하지 말고 그래프를 그려서 눈으로도 확인해야 한다.      &gt; shapiro.test(rexam$exam)     # W = 0.96131, p-value = 0.004991 &lt; 0.05 -&gt; 변수가 정규성에서 이탈&gt; shapiro.test(rexam$numeracy) # W = 0.92439, p-value = 2.424e-05  개별 분포가 아닌 각 그룹의 분포가 중요하다!!&gt; by(rexam$exam, rexam$uni, shapiro.test) # 대학별로 검정 실행# Duncetown University : W = 0.97217, p-value = 0.2829# Sussex University : W = 0.98371, p-value = 0.7151# 두 대학 모두 정규분포에 가깝다&gt; by(rexam$numeracy, rexam$uni, shapiro.test)# Duncetown University : W = 0.94082, p-value = 0.01452# Sussex University : W = 0.93235, p-value = 0.006787# 두 대학 모두 유의한 수준으로 정규분포가 아니라는 결과가 나왔다.# 그래프로 비교&gt; ggplot(rexam, aes(sample=exam)) + stat_qq()&gt; ggplot(rexam, aes(sample=numeracy)) + stat_qq()    분산의 동질성 검정  분산의 동질성 : 여러 그룹의 자료를 수집한 경우, 이 가정은 각 그룹에서 결과변수(들)의 분산이 동일해야 한다는 뜻레빈 검정  서로 다른 그룹의 분산이 같을 것이라는 귀무가설을 검정만일 레빈 검정이 p &lt;= 0.05 에서 유의하다면 귀무가설은 거짓이며, 따라서 분산들이 유의하게 다르다는 뜻이런 경우 분산들의 동질성 가정이 성립하지 않는다.&gt; leveneTest(rexam$exam, rexam$uni, center = median) # F(1, 98) = 2.09, p=0.15 이므로 대학생들의 분산은 그리 다르지 않다.&gt; leveneTest(rexam$numeracy, rexam$uni, center = median) # F(1, 98) = 5.37, p=0.02 이므로 분산들이 유의하게 다르며, 분산의 동질성 가정이 깨졌다.",
      "url": "/2024/12/04/Discovering-Statistics-Using-R-005.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 004",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 004 - 2024년12월03일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 004 - 2024년12월03일  tag : Discovering_Statistics_Using_R|R|그래프를 이용한 자료 탐색  이번 챕터에서 사용하는 라이브러리는 ggplot2&gt; install.packages(\"ggplot2\")&gt; library(ggplot2)그래프 그리기geom_point&gt; graph &lt;- ggplot(facebookData, aes(NPQC_R_Total, Rating)) # aes(a, b) = a : x축, b : y축 지정&gt; graph + geom_point()&gt; graph + geom_point(aes(colour = Rating_Type), position = 'jitter') # jitter : 점들에 무작위 값을 추가해서 위치를 조정&gt; graph + geom_point(aes(shape = Rating_Type), position = 'jitter') # shape -&gt; 모양으로 구분scatter plot&gt; scatter &lt;- ggplot(examData, aes(Anxiety, Exam)) # Anxiety : 시험 불안, Exam : 시험 점수를 퍼센트로 환산한 값&gt; scatter + geom_point() + labs(x = \"Exam Anxiety\", y = \"Exam Performance %\") # labs : 그래프 축 이름표 추가&gt; scatter + geom_point() + geom_smooth() + labs(x = \"Exam Anxiety\", y = \"Exam Performance %\") # 회귀선[곡선]을 추가&gt; scatter + geom_point(method=\"lm\") + geom_smooth() + labs(x = \"Exam Anxiety\", y = \"Exam Performance %\") # 회귀선[직선]을 추가    &gt; scatter + geom_point() + geom_smooth(method='lm', aes(fill=Gender, alpha=0.1)) + labs(x='Exam Anxiety', y='Exam Performance %', colour = 'Gender')# aes(fill=Gender, alpha=0.1) : 신뢰구간을 Gender에 따라 색 지정하기histogram&gt; festivalData &lt;- read.delim(\"data/DownloadFestival.dat\", header=TRUE)&gt; festivalHistogram &lt;- ggplot(festivalData, aes(day1)) + theme(legend.position = \"none\") # legend.position : 범례 표시&gt; festivalHistogram + geom_histogram(binwidth = 0.4) + labs(x = \"Hygiene (Day 1 of Festival)\", y = \"Frequency\") # binwidth : 너비boxplot&gt; festivalBoxplot &lt;- ggplot(festivalData, aes(gender, day1))&gt; festivalBoxplot + geom_boxplot() + labs(x = \"Gender\", y=\"Hygiene (Day 1 of Festival)\") # 이상치 값 존재&gt; festivalData &lt;- festivalData[order(festivalData$day1), ] # 정렬 후 이상치 값 확인 [ 여기서는 마지막 값인 20.02 ]&gt; festivalData[festivalData$day1 == 20.02, \"day1\"] &lt;- 2.02 # 이상치 값을 정상 데이터 값으로 변경&gt; festivalBoxplot &lt;- ggplot(festivalData, aes(gender, day1))&gt; festivalBoxplot + geom_boxplot() + labs(x = \"Gender\", y=\"Hygiene (Day 1 of Festival)\")    밀도 그림(density plot)&gt; density &lt;- ggplot(festivalData, aes(day1))&gt; density + geom_density() + labs(x = \"Hygiene (Day 1 Festival)\", y=\"Density Estimate\")bar chart&gt; chickFlick &lt;- read.delim(\"data/ChickFlick.dat\", header=TRUE) # film : 관람한 영화 제목, arousal : 각성 점수&gt; bar &lt;- ggplot(chickFlick, aes(film, arousal))&gt; bar + stat_summary(fun.y = mean, geom = \"bar\", fill = \"white\", colour = \"Black\") # fun.y = mean : 평균 계산, fill : 막대 내부 색, colour : 막대의 테두리&gt; bar + stat_summary(fun.y = mean, geom = \"bar\", fill = \"white\", colour = \"Black\") + stat_summary(fun.data = mean_cl_normal, geom = \"pointrange\") + labs(x = \"Film\", y = \"Mean Arousal\") # fun.data = mean_cl_normal : 정규분포를 가정하지 않은 95% 신뢰구간    &gt; bar &lt;- ggplot(chickFlick, aes(film, arousal, fill=gender))&gt; bar + stat_summary(fun.y = mean, geom=\"bar\", position='dodge')&gt; bar + stat_summary(fun.y = mean, geom=\"bar\", position='dodge') + stat_summary(fun.data = mean_cl_normal, geom = 'errorbar', position=position_dodge(width=0.90), width=0.2) + labs(x='Film', y=\"Mean Arousal\", fill = \"Gender\") # position='dodge' : 겹치지 않고 나란히 배치&gt; bar &lt;- ggplot(chickFlick, aes(film, arousal, fill = film))&gt; bar + stat_summary(fun.y = mean, geom = 'bar') + stat_summary(fun.data = mean_cl_normal, geom = 'errorbar', width=0.2) + facet_wrap( ~ gender) + labs(x = \"Film\", y = \"Mean Arousal\") + theme(legend.position = \"none\") # facet_wrap : 분할    Line&gt; hiccupsData &lt;- read.delim(\"data/Hiccups.dat\", header=TRUE)&gt; hiccups &lt;- stack(hiccupsData) # 자료의 구조를 긴 형식으로 변경&gt; names(hiccups) &lt;- c('Hiccups', 'Intervention') # 컬럼 생성&gt; hiccups$Intervention_Factor &lt;- factor(hiccups$Intervention, levels(hiccups$Intervention))&gt; line &lt;- ggplot(hiccups, aes(Intervention_Factor, Hiccups))&gt; line + stat_summary(fun.y = mean, geom = 'point') # 1&gt; line + stat_summary(fun.y = mean, geom = 'point') + stat_summary(fun.y = mean, geom = \"line\", aes(group = 1)) # 2&gt; line + stat_summary(fun.y = mean, geom = 'point') + stat_summary(fun.y = mean, geom = \"line\", aes(group = 1), colour = 'Blue', linetype = 'dashed') # 3      &gt; line + stat_summary(fun.y = mean, geom = 'point') + stat_summary(fun.y = mean, geom = \"line\", aes(group = 1), colour = 'Blue', linetype = 'dashed') + stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.2) # 4&gt; line + stat_summary(fun.y = mean, geom = 'point') + stat_summary(fun.y = mean, geom = \"line\", aes(group = 1), colour = 'Blue', linetype = 'dashed') + stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.2) + labs(x = \"Intervention\", y = \"Mean Number of Hiccups\") # 5    ",
      "url": "/2024/12/03/Discovering-Statistics-Using-R-004.html"
    },
  
    {
      "title": "상수도 관망 이상 감지 AI 알고리즘 개발 - 02",
      "tags": "Dacon, Time-Series, Anomaly_Detection",
      "desc": "상수도 관망 이상 감지 AI 알고리즘 개발 - 02 - 2024년12월02일  tag : Dacon|Time-Series|Anomaly_Detection|                    Dacon_site        Github_site            ",
      "content": "상수도 관망 이상 감지 AI 알고리즘 개발 - 02 - 2024년12월02일  tag : Dacon|Time-Series|Anomaly_Detection|                    Dacon_site        Github_site            Data 학습  MinMaxScaler 적용 [ Train, Test Data]  P에 대한 각 컬럼마다 모델을 적용시켜 이상치를 알아야 하기 때문에 총 8개의 다른 모델을 학습[P1 ~ P8]  TEST 데이터가 T시점까지 존재하기 때문에 T+1 인 시점 데이터를 생성[By. from statsmodels.tsa.holtwinters import ExponentialSmoothing]  위 2번에서 학습한 모델을 갖고 각 컬럼에 맞게 예측한 뒤 submission.csv 파일로 저장하여 제출MinMaxScalerfrom sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler(feature_range=(0, 1))df_C.loc[:, x_c_col] = scaler.fit_transform(df_C.loc[:, x_c_col])df_C.head()Model Fitfrom sklearn.ensemble import IsolationForest# Isolation Forest 모델 학습model_p1 = IsolationForest(contamination=0.05, random_state=42) # 이상치 비율 설정model_p1.fit(x_train['P1'].values.reshape(-1,1))model_p2 = IsolationForest(contamination=0.05, random_state=42)model_p2.fit(x_train['P2'].values.reshape(-1,1))model_p3 = IsolationForest(contamination=0.05, random_state=42)model_p3.fit(x_train['P3'].values.reshape(-1,1))model_p4 = IsolationForest(contamination=0.05, random_state=42)model_p4.fit(x_train['P4'].values.reshape(-1,1))model_p5 = IsolationForest(contamination=0.05, random_state=42)  model_p5.fit(x_train['P5'].values.reshape(-1,1))model_p6 = IsolationForest(contamination=0.05, random_state=42)  model_p6.fit(x_train['P6'].values.reshape(-1,1))model_p7 = IsolationForest(contamination=0.05, random_state=42)  model_p7.fit(x_train['P7'].values.reshape(-1,1))model_p8 = IsolationForest(contamination=0.05, random_state=42)model_p8.fit(x_train['P8'].values.reshape(-1,1))Test data T+1 시점 생성test_C = pd.read_csv(f\"../data/test/C/{i}.csv\")test_C = test_C[x_c_col]test_C[:] = scaler.fit_transform(test_C[:])predictions = {}for j in x_c_col:    model = ExponentialSmoothing(        test_C[j],        seasonal_periods=24,  # 24시간 주기        trend=None,        seasonal=None    )    fitted_model = model.fit()    forecast = fitted_model.forecast(1).values  # 다음 1시점 예측    predictions[j] = float(forecast)predictions_df = pd.DataFrame([predictions])result_df = pd.concat([test_C, predictions_df], ignore_index=True)  데이터 불러와서 MinMaxScaler 적용각 모델마다 다음 시점 예측해서 데이터 프레임의 마지막 행에 추가Predictresult = []p_predict = model_p1.predict(result_df['P1'].values.reshape(-1,1))[-1]p_predict = (p_predict == -1).astype(int)result.append(p_predict)p_predict = model_p2.predict(result_df['P2'].values.reshape(-1,1))[-1]p_predict = (p_predict == -1).astype(int)result.append(p_predict)***sub_df.loc[count, 'flag_list'] = result # 제출 데이터에 저장앞으로 추가해야 할 것  from sklearn.ensemble import IsolationForest 외에 다른 이상치 모델이 있는지 확인          그리고 매개변수에 대한 개념도 정리        T+1 시점 생성하는 ExponentialSmoothing 기법도 원리랑 개념 확인하기  다른 변수들을 활용하는 방법 생각하기",
      "url": "/2024/12/02/%EC%83%81%EC%88%98%EB%8F%84-%EA%B4%80%EB%A7%9D-%EC%9D%B4%EC%83%81-%EA%B0%90%EC%A7%80-AI-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EA%B0%9C%EB%B0%9C-002.html"
    },
  
    {
      "title": "상수도 관망 이상 감지 AI 알고리즘 개발 - 01",
      "tags": "Dacon, Time-Series, Anomaly_Detection",
      "desc": "상수도 관망 이상 감지 AI 알고리즘 개발 - 01 - 2024년12월01일  tag : Dacon|Time-Series|Anomaly_Detection|",
      "content": "상수도 관망 이상 감지 AI 알고리즘 개발 - 01 - 2024년12월01일  tag : Dacon|Time-Series|Anomaly_Detection|                    Dacon_site        Github_site            Meta Data# TRAIN_A.csv [파일]: A 관망 구조의 분 단위 데이터 (24/05/27 00:00 ~ 24/06/26 15:00)# TRAIN_B.csv [파일]: B 관망 구조의 분 단위 데이터 (24/07/01 00:00 ~ 24/07/29 23:59)# timestamp: 분 단위 시점# Q: 유량# M: 펌프가동정보 (On=1, Off=0)# P: 압력# anomaly: 해당 시점에서의 정상(0), 이상(1) 여부# P_flag: 해당 시점에서의 압력계의 정상(0), 이상(1) 여부# test [폴더]: 예측 현재 시점 T를 포함한 최대 일주일(Lookback 기간)에 대한 평가 데이터 샘플# C [폴더]: C 관망 구조의 현재 예측 시점 T로 구성된 TEST_C_0000.csv ~ TEST_C_2919.csv의 추론용 평가 데이터 샘플# D [폴더]: D 관망 구조의 현재 예측 시점 T로 구성된 TEST_D_0000.csv ~ TEST_D_2737.csv의 추론용 평가 데이터 샘플# timestamp: 현재 시점 T가 비식별화된 분 단위 시점# Q: 유량# M: 펌프가동정보 (On=1, Off=0)# P: 압력# meta_관망구조이미지 [폴더]# train [폴더]: A, B 관망 구조 이미지# test [폴더]: C, D 관망 구조 이미지# test.csv [파일]# ID : 평가 데이터 샘플 식별 ID# path : 평가 데이터 샘플 경로# sample_submission.csv [제출 양식]# ID : 평가 데이터# P : 압력에 대한 정상 or 비정상 구분Data InfoColumnsdf_A = pd.read_csv(\"../data/train/TRAIN_A.csv\")df_B = pd.read_csv(\"../data/train/TRAIN_B.csv\")df_A['timestamp'] = pd.to_datetime(df_A['timestamp'], format='%y/%m/%d %H:%M')df_B['timestamp'] = pd.to_datetime(df_B['timestamp'], format='%y/%m/%d %H:%M')df_A.set_index('timestamp', inplace=True)df_B.set_index('timestamp', inplace=True)sub_df = pd.read_csv(\"../data/sample_submission.csv\")sub_df.head()  flag_list          test_c 일 때 -&gt; P1 ~ P8 까지 변수 사용      test_d 일 때 -&gt; P1 ~ P6 까지 변수 사용        따라서 두 가지 경우로 나눠서 학습x_c_col = ['P1','P2','P3','P4','P5','P6','P7','P8']x_d_col = ['P1','P2','P3','P4','P5','P6']y_col = 'anomaly'TEST Cdf_A_C = df_A[x_c_col + [y_col]]df_B_C = df_B[x_c_col + [y_col]]df_C = pd.concat([df_A_C, df_B_C])  간단한 시각화import matplotlib.pyplot as plt# 서브플롯 생성fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 2행 4열의 서브플롯axes = axes.flatten()  # 2D 배열 -&gt; 1D로 평탄화# 각 피처별 그래프 그리기for i, feature in enumerate(x_c_col):    axes[i].scatter(df_C.index, df_C[feature], c=df_C['anomaly'], cmap='coolwarm', s=10, label=feature)    axes[i].set_title(f'{feature}', fontsize=14)    axes[i].set_xlabel('Index')    axes[i].set_ylabel(feature)    axes[i].legend(loc='upper right')plt.show()preprocessingMinMaxScalerfrom sklearn.preprocessing import MinMaxScalerscaler = MinMaxScaler(feature_range=(0, 1))df_C.loc[:, x_c_col] = scaler.fit_transform(df_C.loc[:, x_c_col])train_test_splitfrom sklearn.model_selection import train_test_split# 독립 변수(X)와 종속 변수(y) 분리X = df_C.drop(columns=['anomaly'])y = df_C['anomaly']# 시계열 순서 유지하며 분리split_idx = int(len(X) * 0.8)x_train, x_val = X.iloc[:split_idx], X.iloc[split_idx:]y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]Modelingfrom sklearn.ensemble import IsolationForestfrom sklearn.metrics import classification_report, accuracy_score# Isolation Forest 모델 학습model = IsolationForest(contamination=0.05, random_state=42)  # 이상치 비율 설정model.fit(x_train)# x_val에 대한 예측 수행val_predictions = model.predict(x_val)# 예측값 변환 (-1 → 1, 1 → 0)val_predictions = (val_predictions == -1).astype(int)from sklearn.metrics import classification_report, accuracy_score# 성능 평가print(\"Classification Report:\")print(classification_report(y_val, val_predictions))# 정확도accuracy = accuracy_score(y_val, val_predictions)print(f\"Accuracy: {accuracy:.2f}\")앞으로 추가해야 할 것  Test_C, Test_D 데이터에 대해서 학습한 모델로 예측한 값을 submission에 넣은 뒤 제출해보기  시각화를 통해 Train_A, Train_B 데이터의 범위가 차이가 많이 있음을 확인 -&gt; 해결방법 생각해보기",
      "url": "/2024/12/01/%EC%83%81%EC%88%98%EB%8F%84-%EA%B4%80%EB%A7%9D-%EC%9D%B4%EC%83%81-%EA%B0%90%EC%A7%80-AI-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%EA%B0%9C%EB%B0%9C-001.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 003",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 003 - 2024년11월30일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 003 - 2024년11월30일  tag : Discovering_Statistics_Using_R|R|R의 기본적인 사용법자료 사이트  R의 명령은 객체와 함수라는 두 부분으로 구성된다.          객체 &lt;- 함수객체가 함수로부터 생성된다.      작업환경 설정  setwd(‘경로 입력’)  setwd('C:/github/R/Discovering-Statistics-Using-R')getwd() # 현재 작업환경 보기  c() : 연결 함수(concatenate function)  객체를 생성하고 주어진 여러 인수를 하나로 묶는 역할을 한다.  metallica &lt;- c(\"Lars\", \"James\", \"Jason\", \"Kirk\")  객체에서 값 제외하기&gt; metallica != \"Jason\"[1]  TRUE  TRUE FALSE  TRUE&gt; metallica[metallica != \"Jason\"][1] \"Lars\"  \"James\" \"Kirk\"&gt; metallica = metallica[metallica != \"Jason\"]&gt; metallica[1] \"Lars\"  \"James\" \"Kirk\"값 추가하기&gt; metallica &lt;- c(metallica, \"Rob\")&gt; metallica[1] \"Lars\"  \"James\" \"Kirk\"  \"Rob\"  파일 불러오기&gt; myData &lt;- read.delim(\"../data/data.dat\")&gt; myData     name birth_date job friends alcohol income neurotic1     Ben   7/3/1977   1       5      10  20000       102  Martin  5/24/1969   1       2      15  40000       173    Andy  6/21/1973   1       0      20  35000       144    Paul  7/16/1970   1       4       5  22000       135  Graham 10/10/1949   1       1      30  50000       216  Carina  11/5/1983   2      10      25   5000        77  Karina  10/8/1987   2      12      20    100       138    Doug  1/23/1989   2      15      16   3000        99    Mark  5/20/1973   2      12      17  10000       1410    Zoe 11/12/1984   2      17      18     10       13패키지 설치 및 불러오기install.packages(\"DSUR\") # DSUR 이라는 패키지 설치library(DSUR)패키지 안에 있는 함수 사용하기car::recode() # car 패키지에 있는 recode()함수를 사용변수 생성&gt; job&lt;-c(1,1,1,1,1,2,2,2,2,2) # 기본적인 생성 방법&gt; job [1] 1 1 1 1 1 2 2 2 2 2&gt; job&lt;-c(rep(1, 5),rep(2, 5)) # 반복 사용&gt; job [1] 1 1 1 1 1 2 2 2 2 2&gt; job&lt;-factor(job, levels = c(1:2), labels = c(\"Lecturer\", \"Student\")) # job 객체를 1 -&gt; Lecturer, 2 -&gt; Student 로 변환&gt; job [1] Lecturer Lecturer Lecturer Lecturer Lecturer Student  Student  Student  Student  StudentLevels: Lecturer Student&gt; job&lt;-gl(2, 5, labels = c(\"Lecturer\", \"Student\")) # gl : general level, gl(수준 수, 각 수준의 사례 수, 전체 사례 수, labels = c(\"이름표1\", \"이름표2\",...))&gt; job [1] Lecturer Lecturer Lecturer Lecturer Lecturer Student  Student  Student  Student  StudentLevels: Lecturer Student데이터프레임생성&gt; metallicaAges &lt;- c(47,47,48,46)&gt; metallicaNames &lt;- metallica&gt; metallica &lt;- data.frame(Name=metallicaNames, Age = metallicaAges)&gt; metallica   Name Age1  Lars  472 James  473  Kirk  484   Rob  46컬럼값 확인하기&gt; metallica$Name[1] Lars  James Kirk  Rob  Levels: James Kirk Lars Rob컬럼 추가하기&gt; metallica$childAge&lt;-c(12,12,4,6)&gt; metallica   Name Age childAge1  Lars  47       122 James  47       123  Kirk  48        44   Rob  46        6컬럼명 확인하기&gt; names(metallica)[1] \"Name\"     \"Age\"      \"childAge\"기본 변수들로 새 변수 계산하기&gt; metallica$fatherhoodAge&lt;-metallica$Age - metallica$childAge&gt; metallica   Name Age childAge fatherhoodAge1  Lars  47       12            352 James  47       12            353  Kirk  48        4            444   Rob  46        6            40데이터프레임 특정 부분 선택&gt; lecturerPersonality &lt;- lecturerData[, c(\"friends\", \"alcohol\", \"neurotic\")] # 일부 변수들만 보고 싶을 때&gt; lecturerPersonality   friends alcohol neurotic1        5      10       102        2      15       173        0      20       144        4       5       135        1      30       216       10      25        77       12      20       138       15      16        99       12      17       1410      17      18       13조건에 맞는 데이터 확인&gt; lecturerOnly &lt;- lecturerData[job==\"Lecturer\",]&gt; lecturerOnly    name birth_date job friends alcohol income neurotic1    Ben   7/3/1977   1       5      10  20000       102 Martin  5/24/1969   1       2      15  40000       173   Andy  6/21/1973   1       0      20  35000       144   Paul  7/16/1970   1       4       5  22000       135 Graham 10/10/1949   1       1      30  50000       21subset 활용&gt; lecturerOnly &lt;- subset(lecturerData, job==\"Lecturer\")",
      "url": "/2024/11/30/Discovering-Statistics-Using-R-003.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 002",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 002 - 2024년11월29일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 002 - 2024년11월29일  tag : Discovering_Statistics_Using_R|R|  통계적 모형  전체 오차 = 편차들의 합 =&gt; \\(\\sum (x_i - \\bar{x})\\)          오차의 부호에 영향을 받음 -&gt; 해결책 : 오차를 제곱        오차제곱합(SS, sum of squared errors) = \\(\\sum(x_i - \\bar{x})^2\\)          data의 크기가 클수록 SS도 커진다. -&gt; 해결책 : 관측값들의 개수 N으로 나누기        분산(variance) = \\(\\frac{SS}{N-1} = \\frac{\\sum(x_i - \\bar{x})^2}{N-1}\\)          단위가 제곱이라는 문제점이 발생 -&gt; 해결책 : 제곱근을 사용        표준편차(standard deviation, SD) = \\(\\sqrt{\\frac{SS}{N-1}} = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{N-1}}\\)          표준편차가 평균보다 상대적으로 작다는 것은 자료점들이 평균에 가깝다는 의미표준편차가 평균보다 상대적으로 크다는 것은 자료점들이 평균에 멀다는 의미        표준오차(SE, standard error) = \\(\\sigma _{\\bar{x}} = \\frac{s}{\\sqrt{N}}\\)          표준오차 : 표본평균들의 표준편차주어진 표본이 모집단을 얼마나 잘 대표하는지 나타내는 측도값이 클 경우 : 표본이 모집단을 잘 대표하지 않을 가능성이 크다.값이 작을 경우 : 표본이 모집단을 정확하게 반영할 가능성이 크다.        신뢰구간(confidence interval)          Ex. 신뢰구간 95% 구간신뢰구간의 하계 = \\(\\bar{X} - (1.96 * SE)\\)신뢰구간의 상계 = \\(\\bar{X} + (1.96 * SE)\\)        [표본이 작을 때는 t분포를 사용]          신뢰구간의 하계 = \\(\\bar{X} - (t_{n-1} * SE)\\)신뢰구간의 상계 = \\(\\bar{X} + (t_{n-1} * SE)\\)      \\[검증 통계량 = \\frac{모형이 설명하는 변동}{모형이 설명하지 못하는 변동} = \\frac{효과}{오차}\\]  검정 통계량(test statistics) : 체계적 변동 대 비체계적 변동의 비(ratio) 또는 효과 대 오차의 비  제1종 오류 : 모집단에 효과가 진짜로 존재한다고 믿지만 사실은 모집단에 아무런 효과도 없는 것          피셔의 기준으로 모집단에 효과가 존재하지 않을 때 이 오류가 생갈 확률은 5% = \\(\\alpha-수준\\)        제2종 오류 : 모집단에 실제로 효과가 존재하지만 모집단에 아무 효과도 존재하지 않는다고 믿는 것          코언은 제2종 오류의 허용 가능한 최대 확룰로 20%를 제안 = \\(\\beta-수준\\)      ",
      "url": "/2024/11/29/Discovering-Statistics-Using-R-002.html"
    },
  
    {
      "title": "Discovering Statistics Using R - 001",
      "tags": "Discovering_Statistics_Using_R, R",
      "desc": "Discovering Statistics Using R - 001 - 2024년11월28일  tag : Discovering_Statistics_Using_R|R|",
      "content": "Discovering Statistics Using R - 001 - 2024년11월28일  tag : Discovering_Statistics_Using_R|R|CH01  양적 연구 방법 : 정량적 방법질적 연구 방법 : 정성적 방법  반증 : 가설이나 이론이 틀렸음을 증명하는 것  독립변수 : 원인으로 제시된 변수종속변수 : 결과(효과)로 제시된 변수  측정수준 : 측정 대상과 그 측정 대상이 나타내는 수치 사이의 관계변주형변수 : 범주들로 구성이분변수 : 남성 또는 여성처럼 서로 다른 두 가지 것을 지칭하는 것명목변수 : 어떤 의미에서 동등하다고 간주되는 대상들에 같은 이름을 부여하되, 그런 이름들이 두 가지보다 많은 경우순서변수 : 순서 있는 범주들을 나타내는 범주형변수연속변수 : 순서변수에서 측정수준을 한 단계 더 높인 것[Ex. 구간변수]비율변수 : 구간변수에서 한 걸음 더 나아간 것으로, 척도상에서 값들의 비(ratio)들이 의미가 있어야 한다  측정오차 : 우리가 측정 대상을 나타내는 데 사용하는 수치들과 측정 대상의 실제 값의 차이  측정오차를 최소화하는 방법          타당성 : 측정 장치가 우리가 측정하고자 하는 것을 실제로 측정했는지 여부      신뢰성 : 서로 다른 상황에서 측정 장치를 일관되게 해석할 수 있는지의 여부 또는 정도를 뜻 함        상관연구 : 세상에서 자연스럽게 발생하는 일을 직접적인 간섭 없이 관찰하는 방식  실험연구 : 변수 하나를 조작해서 그것이 다른 변수들에 미치는 영향을 보는 방식  여러 변수를 동시에 측정하면 발생하는 문제          서로 다른 변수들의 시간관계에 대해 알 수 없다는 것      제3의 요소 : 그 특성이 아직 알려지지 않은 어떤 제3의 인물이나 사물을 뜻 함        반복 측정 시 주의해야 할 점          연습 효과 : 참가자가 실험 상황이나 측정 방식에 익숙해져서 둘째 조건에서는 이전과 다르게 행동할 수 있음      권태 효과 : 참가자가 첫 조건을 마치고 지치거나 지루해져 둘째 조건에서는 이전과 다르게 행동할 수 있다.        왜도와 첨도          최빈값 : 자료에서 가장 빈번하게 나타나는 점수  중앙값 : 점수들을 그 크기순으로 정렬했을 때 가운데에 오는 점수  평균이 0, 표준편차가 1인 값으로 바꾸려면 다음과 같은 변환을 한다.    \\[z = \\frac{X - \\bar{X}}{s} | bar{X} : 평균, s : 표준편차\\]        대립가설(H1) : 이론에서 비롯한 가설 또는 예측은 흔히 어떤 효과가 존재할 것임을 진술  귀무가설(H0) : 대립가설의 역",
      "url": "/2024/11/28/Discovering-Statistics-Using-R-001.html"
    },
  
    {
      "title": "Encoding, Decoding, Closure, Decorator, Iterator, Generator, Type Annotation",
      "tags": "Python_Basic, Python",
      "desc": "Encoding, Decoding, Closure, Decorator, Iterator, Generator, Type Annotation - 2024년11월27일  tag : Python_Basic|Python|",
      "content": "Encoding, Decoding, Closure, Decorator, Iterator, Generator, Type Annotation - 2024년11월27일  tag : Python_Basic|Python|인코딩과 디코딩인코딩  인코딩 = 코드화 = 암호화 = 부호화동영상이나 문자 인코딩 뿐만 아니라 사람이 인지할 수 있는 형태의 데이터를 약속된 규칙에 의해 컴퓨터가 사용하는 0과 1로 변환하는 과정python3 는 utf-8이 기본  # -*- coding: utf-8 -*-  # 맨 위에 작성하면 이 코드는 utf-8로 인코딩되어 있는 것  디코딩  디코딩 = 역코드화 = 복호화사람이 이해할 수 있는 언어로 돌려주는 것클로저와 데코레이터클로저(Closure)  함수 안의 함수를 결과로 반환할 때, 그 내부 함수를 클로저라고 한다.사용되는 곳 : 콜백함수, 함수의 순차적 실행, 데코레이터 함수def mul(m):  def wrapper(n):    return m * n  return wrappermul3 = mul(3)print(mul3(10)) # 30데코레이터(Decorator)  함수를 꾸며주는 함수함수를 인수로 받는 클로저@를 이용한 어노테이션으로 사용사용되는 곳 : 반복되는 작업을 여러 함수에 적용할 경우, 기존 함수를 수정하지 않고 추가 기능을 구현하고 싶을 경우def func1(a, b):  val = a + b  return valdef func2(a, b):  val = a * b  return valdef elapsed(func):  def wrapper(a, b):    print(\"함수가 실행됩니다\")    start = time.time()    result = func(a, b)    end = time.time()    print(\"함수 수행시간: %f 초\" % (end - start))    return result  return wrapperdeco1 = elapsed(func1)result = deco1(1,2)print(result)@elapseddef func1(a, b):  val = a + b  return val@elapseddef func2(a, b):  val = a * b  return valIterator와 GeneratorIterator  집합에서 값을 차례대로 꺼낼 수 있는 객체(Object)를 말함한번 반복하면 다시 사용할 수 없다.a = [1,2,3]iterator = iter(a)next(iterator) # 1next(iterator) # 2next(iterator) # 3next(iterator) # ErrorGenerator  iterator를 생성해주는 함수def generator():  yield 'a'  yield 'b'  yield 'c'g = generator()next(g) # 'a'next(g) # 'b'next(g) # 'c'next(g) # errordef client_count(total_client):  n = 1  for num in range(total_client):    print(f'{n}번째 고객님 입장하세요!')    n +=1    yieldmygen = client_count(100)next(mygen) # 1번째 고객님 입장하세요!next(mygen) # 2번째 고객님 입장하세요!next(mygen) # 3번째 고객님 입장하세요!Type Annotation  변수나 상수를 선언할 때, 그 타입을 명시적으로 선언해 줌으로써 어떤 타입의 값이 저장될 것인지를 직접 알려주는 방법사용 시, 코드 가독성과 협업 효율성이 높아짐def add(a: int,b: int) -&gt; int: # a, b는 int형 | 결과값고 int 값이 반환된다는 것을 알려줌  return a + badd.__annotations__ # {'a' : int, 'b' : int, 'return' : int}",
      "url": "/2024/11/27/Python-Basic-001.html"
    },
  
    {
      "title": "Python Regular Expression - 03",
      "tags": "Regular_Expression, Python",
      "desc": "Python Regular Expression - 03 - 2024년11월26일  tag : Regular_Expression|Python|",
      "content": "Python Regular Expression - 03 - 2024년11월26일  tag : Regular_Expression|Python|정규 표현식더 많은 패턴 기능메타 문자 더 보기  | : or 연산자 , Ex. Crow|Servo 는 ‘Crow’나 ‘Servo’와 일치  ^ : 줄의 시작 부분에 일치print(re.search('^From', 'From Here to Eternity')) # &lt;re.Match object; span=(0, 4), match='From'&gt;print(re.search('^From', 'Reciting From Memory')) # None  $ : 줄의 끝부분과 일치하는데, 문자열의 끝이나 줄 바꿈 문자 다음에 오는 모든 위치로 정의print(re.search('}$', '{block}')) # &lt;re.Match object; span=(6, 7), match='}'&gt;print(re.search('}$', '{block} ')) # Noneprint(re.search('}$', '{block}\\n')) # &lt;re.Match object; span=(6, 7), match='}'&gt;  \\A : 문자열의 시작 부분에서만 일치 == ^  \\Z : 문자열 끝부분에서만 일치  \\b : 단어 경계, 단어의 시작이나 끝부분에서만 일치하는 폭이 없는 어서션 &lt;-&gt; \\B : 현재 위치가 단어 경계에 있지 않을 때만 일치p = re.compile(r'\\bclass\\b')print(p.search('no class at all')) # &lt;re.Match object; span=(3, 8), match='class'&gt;print(p.search('the declassified algorithm')) # Noneprint(p.search('one subclass is')) # None그룹  () : 일치하는 텍스트의 시작과 끝 인덱스도 포착p = re.compile('(a(b)c)d')m = p.match('abcd')m.group(0) # 'abcd'm.group(1) # 'abc'm.group(2) # 'b'문자열 수정하기  split() : RE가 일치하는 모든 곳에서 분할하여, 문자열을 리스트로 분할          .split(string, maxsplit=0) : maxsplit 값이 0이 아니면 최대 maxsplit번 분할만 이루어지고, 나머지 문자열은 리스트의 마지막 요소로 반환      p = re.compile(r'\\W+')p.split('This is a test, short and sweet, of split().')# ['This', 'is', 'a', 'test', 'short', 'and', 'sweet', 'of', 'split', '']p.split('This is a test, short and sweet, of split().', 3)# ['This', 'is', 'a', 'test, short and sweet, of split().']  sub() : RE가 일치하는 모든 부분 문자열을 찾고, 다른 문자열로 대체          .sub(replacement, string, count=0)string 에서 가장 왼쪽에 나타나는 겹쳐지지 않은 RE의 일치를 replacement로 치환count는 치환될 패턴 일치의 최대 수      p = re.compile('(blue|white|red)')p.sub('colour', 'blue socks and red shoes') # 'colour socks and colour shoes'p.sub('colour', 'blue socks and red shoes', count=1) # 'colour socks and red shoes'  subn() : sub()와 같은 일을 하지만, 새로운 문자열과 치환 횟수를 반환p = re.compile('(blue|white|red)')p.subn('colour', 'blue socks and red shoes') # ('colour socks and colour shoes', 2)p.subn('colour', 'no colours at all') # ('no colours at all', 0)",
      "url": "/2024/11/26/Python_Regular_Expression-03.html"
    },
  
    {
      "title": "Python Regular Expression - 02",
      "tags": "Regular_Expression, Python",
      "desc": "Python Regular Expression - 02 - 2024년11월25일  tag : Regular_Expression|Python|",
      "content": "Python Regular Expression - 02 - 2024년11월25일  tag : Regular_Expression|Python|파이썬 정규표현식정규식 사용하기import rep = re.compile('ab*')일치 수행하기match() : 문자열의 시작 부분에서 RE가 일치하는지 판단합니다.p = re.compile('[a-z]+')m = p.match('tempo') # &lt;re.Match object; span=(0, 5), match='tempo'&gt;  group() : RE와 일치하는 문자열을 반환합니다  start() : 일치의 시작 위치를 반환  end() : 일치의 끝 위치를 반환합니다  span() : 일치의(시작, 끝) 위치를 포함하는 튜플을 반환합니다.m.group() # 'tempo'm.start(), m.end() # (0, 5)m.span() # (0, 5)search() : 이 RE가 일치하는 위치를 찾으면서, 문자열을 훑습니다.print(p.match('::: message')) # Nonem = p.search('::: message') # &lt;re.Match object; span=(4, 11), match='message'&gt;m.group() # 'message'm.span() # (4, 11)findall() : RE가 일치하는 모든 부분 문자열을 찾아 리스트로 반환합니다.p = re.compile(r'\\d+')p.findall('12 drummers drumming, 11 pipers piping, 10 lords a-leaping') # ['12', '11', '10']finditer() : RE가 일치하는 모든 부분 문자열을 찾아 이터레이터로 반환합니다.iterator = p.finditer('12 drummers drumming, 11 pipers piping, 10 lords a-leaping')for match in iterator:    print(match.span())# (0, 2)# (22, 24)# (40, 42)컴파일 플래그  그냥 읽어보기            플래그      의미                  ASCII, A      \\w, \\b, \\s 및 \\d와 같은 여러 이스케이프가 해당 속성이 있는 ASCII 문자에만 일치하도록 합니다.              DOTALL, S      .가 개행 문자를 포함한 모든 문자와 일치하도록 합니다.              IGNORECASE, I      대소 문자 구분 없는 일치를 수행합니다.              LOCALE, L      로케일을 고려하는 일치를 수행합니다.              MULTILINE, M      다중 행 일치, ^와 $에 영향을 줍니다.              VERBOSE, X (‘확장’ 용)      더 명확하고 이해하기 쉽게 정리될 수 있는 상세한 RE를 활성화합니다.      ",
      "url": "/2024/11/25/Python_Regular_Expression-02.html"
    },
  
    {
      "title": "Python Regular Expression - 01",
      "tags": "Regular_Expression, Python",
      "desc": "Python Regular Expression - 01 - 2024년11월24일  tag : Regular_Expression|Python|",
      "content": "Python Regular Expression - 01 - 2024년11월24일  tag : Regular_Expression|Python|파이썬 정규표현식  정규식(RE, regexes 또는 regex 패턴)참고 사이트단순한 패턴문자 일치  [ 와 ]          EX. [abc] : a,b 또는 c 문자와 일치 == [a-c]EX. [a-z] : 소문자들만 일치        ^ : 여집합          EX. [^5] : 5를 제외한 모든 문자와 일치        \\ : 백 슬래시 다음에 다양한 특수 시퀀스를 알리는 다양한 문자가 따라올 수 있다.          Ex. [\\] : \\ 일치              \\d : 모든 십진 숫자와 일치 == [0-9]      \\D : 모든 비 숫자 문자와 일치 == [^0-9]      \\s : 모든 공백 문자와 일치 == [ \\t\\n\\r\\f\\v]      \\S : 모든 비 공백 문자와 일치 == [^ \\t\\n\\r\\f\\v]      \\w : 모든 영숫자와 일치 == [a-zA-Z0-9_]      \\W : 모든 비 영숫자와 일치 == [^a-zA-Z0-9_]4.. : “모든 문자”와 일치시키려고 할 때 자주 사용      반복하기  * : 리터럴 문자 ‘*‘와 일치하지 않는다. 대신 이전 문자를 정확히 한 번이 아닌 0번 이상 일치시킬 수 있도록 지정          Ex. [cat] : ‘ct’ (0개의 a문자), ‘cat’(1개의 a문자), ‘caaat’(3개의 a문자) 등과 일치* 은 탐욕적이다 -&gt; RE를 반복할 때, 일치 엔진은 가능한 한 여러 번 반복하려고 시도Ex. [a[bcd]b]이 정규식을 ‘abcbd’와 일치시킨다고 생각                  단계      일치된 것      설명                  1      a      RE의 a가 일치합니다.              2      abcbd      엔진은 가능한 한 길게 [bcd]*와 일치시키려고 문자열의 끝까지 갑니다.              3      실패      엔진은 b를 일치하려고 시도하지만, 현재 위치가 문자열의 끝이므로 실패합니다.              4      abcb      물러서서, [bcd]*가 하나 적은 문자와 일치합니다.              5      실패      b를 다시 시도하지만, 현재 위치는 ‘d’ 인 마지막 문자에 있습니다.              6      abc      다시 물러서서, [bcd]*가 bc하고 만 일치합니다.              6      abcb      b를 다시 시도합니다. 이번에는 현재 위치의 문자가 ‘b’이므로 성공합니다.        + : 하나 이상과 일치  ? : 한 번 또는 0번 일치          Ex. [home-?brew]인 정규표현식은 ‘homebrew’ or ‘home-brew’와 일치        {m, n} : m, n은 십진수 정수, 최소 m번, 최대 n번의 반복이 있어야 함을 의미          [a/{1,3}]인 정규표현식은 ‘a/b’, ‘a//b’, ‘a///b’와 일치{m}인 경우 이전 항목과 정확히 m번 일치      ",
      "url": "/2024/11/24/Python_Regular_Expression-01.html"
    },
  
    {
      "title": "Python Crawling 정리",
      "tags": "Crawling, Python",
      "desc": "Python Crawling 정리 - 2024년11월23일  tag : Crawling|Python|",
      "content": "Python Crawling 정리 - 2024년11월23일  tag : Crawling|Python|크롤링  참고 사이트Library Importimport pandas as pdfrom tqdm import tqdmfrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom bs4 import BeautifulSoupfrom urllib.request import Request, urlopenfrom selenium.webdriver.common.keys import KeysDataFrame 정의player_info_cols = ['Name', 'Age', 'Position(s)', 'Foot', 'Height', 'Weight', 'Club', 'Wages']field_player_cols = [    'corners', 'crossing', 'dribbling', 'finishing', 'first-touch', 'free-kick-taking', 'heading', 'long-shots',    'long-throws', 'marking', 'passing', 'penalty-taking', 'tackling', 'technique', 'aggression', 'anticipation', 'bravery',    'composure', 'concentration', 'decisions', 'determination', 'flair', 'leadership', 'off-the-ball', 'positioning',    'teamwork', 'vision', 'work-rate', 'acceleration', 'agility', 'balance', 'jumping-reach', 'natural-fitness', 'pace',    'stamina', 'strength']goalkeeper_cols = [    'aerial-reach', 'command-of-area', 'communication', 'eccentricity', 'first-touch', 'handling', 'kicking',    'one-on-ones', 'passing', 'punching-tendency', 'reflexes', 'rushing-out-tendency', 'throwing','aggression',    'anticipation', 'bravery', 'composure', 'concentration', 'decisions', 'determination', 'flair', 'leadership',    'off-the-ball', 'positioning', 'teamwork', 'vision', 'work-rate', 'acceleration', 'agility', 'balance',    'jumping-reach','natural-fitness', 'pace', 'stamina', 'strength', 'free-kick-taking', 'penalty-taking', 'technique']field_player_df = pd.DataFrame(columns = player_info_cols + field_player_cols)goalkeeper_df = pd.DataFrame(columns = player_info_cols + goalkeeper_cols)   골키퍼와 필드 플레이어는 공통된 player_info를 갖고 있다.단, 스탯 속성은 다르기 때문에 따로 컬럼을 분류하여 만들어야 한다.Crawling 시작# 크롤링 시작options = webdriver.ChromeOptions()options.add_argument(\"--headless\")  # 헤드리스 모드driver = webdriver.Chrome(options=options)driver.get('https://fminside.net/')driver.implicitly_wait(20)driver.find_element(By.XPATH, '//*[@id=\"menu\"]/ul[2]/li[3]/a').click()driver.implicitly_wait(20)  헤드리스 모드를 사용하는 이유 : 광고 팝업창이 자꾸 떠서 다음 창으로 안넘어가는 현상을 막기 위해서 사용for i in tqdm(range(1, 101)): # 100개 데이터만 수집    while True: # Load More Players 버튼 클릭        try:            driver.find_element(By.XPATH, f'//*[@id=\"player_table\"]/div/div[3]/ul[{i}]/li[3]/span[2]/b/a').send_keys(                Keys.CONTROL + Keys.ENTER)            break        except:            driver.find_element(By.XPATH, '//*[@id=\"player_table\"]/div/div[3]/a').click()            driver.implicitly_wait(10)    driver.switch_to.window(driver.window_handles[-1]) # 새 창 전환    driver.implicitly_wait(20)    info_url = driver.current_url # 현재 URL    info_req = Request(info_url, headers={'User-Agent': 'Mozilla/5.0'})    info_html = urlopen(info_req).read()    bs_obj = BeautifulSoup(info_html, 'html.parser')    driver.close()    driver.switch_to.window(driver.window_handles[-1])    pi_data = []    # player_info 공통된 컬럼    player_info_table = bs_obj.select('#player &gt; div:nth-child(2) &gt; ul')    for i in range(1, 7):        pi_data.append(player_info_table[0].select(f'li:nth-child({i}) &gt; span.value')[0].text)    player_contract_table = bs_obj.select('#player &gt; div:nth-child(3) &gt; ul')    pi_data.append(player_contract_table[0].select(f'li:nth-child({1}) &gt; span.value')[0].text)    pi_data.append(player_contract_table[0].select(f'li:nth-child({3}) &gt; span.value')[0].text)    def field_player():        # field_player_info        fp_data = []        player_stats_table = bs_obj.select('#player_stats')[0]        technical_info = player_stats_table.select('div:nth-child(3) &gt; table &gt; tr &gt; td')        for i in range(1, len(technical_info), 2):            fp_data.append(technical_info[i].text)        mental_info = player_stats_table.select('div:nth-child(4) &gt; table &gt; tr &gt; td')        for i in range(1, len(mental_info), 2):            fp_data.append(mental_info[i].text)        physical_info = player_stats_table.select('div:nth-child(5) &gt; table &gt; tr &gt; td')        for i in range(1, len(physical_info), 2):            fp_data.append(physical_info[i].text)        return fp_data    def goalkeeper():        # goalkeeper_info        gk_data = []        player_stats_table = bs_obj.select('#player_stats')[0]        goalkeeping_info = player_stats_table.select('div:nth-child(3) &gt; table &gt; tr &gt; td')        for i in range(1, len(goalkeeping_info), 2):            gk_data.append(goalkeeping_info[i].text)        mental_info = player_stats_table.select('div:nth-child(4) &gt; table &gt; tr &gt; td')        for i in range(1, len(mental_info), 2):            gk_data.append(mental_info[i].text)        physical_info = player_stats_table.select('div:nth-child(5) &gt; table &gt; tr &gt; td')        for i in range(1, len(physical_info), 2):            gk_data.append(physical_info[i].text)        return gk_data    if pi_data[2] == 'GKGK': # 골키퍼인 경우        gk_data = goalkeeper()        goalkeeper_df.loc[len(goalkeeper_df)] = pi_data + gk_data    else:        fp_data = field_player() # 필드 플레이어인 경우        field_player_df.loc[len(field_player_df)] = pi_data + fp_data결과&lt;!DOCTYPE html&gt;                Name  Age  Position(s)  Foot  Height  Weight  Club  Wages  corners  crossing  dribbling  finishing  first-touch  free-kick-taking  heading  long-shots  long-throws  marking  passing  penalty-taking  tackling  technique  aggression  anticipation  bravery  composure  concentration  decisions  determination  flair  leadership  off-the-ball  positioning  teamwork  vision  work-rate  acceleration  agility  balance  jumping-reach  natural-fitness  pace  stamina  strength  0  Kevin De Bruyne  32  MC, MR, ML, AMCMC, AMC  Right  181 CM  68 KG  Manchester City  € 397,800 pw  70  95  80  80  80  85  30  85  35  45  90  75  45  90  60  70  65  70  75  90  85  80  65  75  50  70  99  75  80  65  70  50  80  70  80  65  1  Erling Haaland  22  STST  Left  195 CM  94 KG  Manchester City  € 397,800 pw  35  50  70  90  80  65  75  65  25  30  65  85  35  75  75  95  75  90  75  75  99  80  65  90  35  70  70  65  85  80  85  90  95  95  70  85  2  Kylian Mbappé  24  AMR, AML, STAML, ST  Right  178 CM  73 KG  PSG  € 1,038,960 pw  65  60  90  85  90  55  40  75  15  20  75  90  20  85  30  85  60  85  70  75  90  90  65  85  15  50  75  60  99  80  70  40  75  99  70  55  3  Lionel Messi  36  AMR, AMC, STAMR, AMC, ST  Left  169 CM  67 KG  Inter Miami  € 368,240 pw  75  75  99  85  95  90  50  80  20  20  95  85  35  99  35  80  50  80  65  90  99  99  70  70  25  70  99  45  80  75  90  30  70  75  65  45  4  Harry Kane  29  AMC, STST  Right  188 CM  86 KG  FC Bayern  € 423,280 pw  45  75  70  95  75  55  80  75  30  40  90  99  45  85  55  75  70  90  75  90  95  60  80  85  40  90  95  80  60  60  75  75  80  70  80  75",
      "url": "/2024/11/23/Python_Crawling.html"
    },
  
    {
      "title": "Python Class 정리",
      "tags": "Python, Class",
      "desc": "Python Class 정리 - 2024년11월22일  tag : Python|Class|",
      "content": "Python Class 정리 - 2024년11월22일  tag : Python|Class|객체 지향 프로그래밍(Object Oriented Programming)  프로그램 설계 방법론프로그램을 여러 개의 독립적인 단위인 ‘객체’라는 기본 단위로 나누고‘객체’들의 상호작용을 통해 프로그램을 설계하고 개발하는 방식  장점 : 코드 재사용 용이, 유지보수 용이, 대형 프로젝트 적합  단점 : 실행 속도가 상대적으로 느림, 객체가 많으면 프로그램 용량 커짐, 설계시 많은 시간 소요클래스(Class) &amp; 객체(Object) &amp; 인스턴스  class : 객체 지향 프로그래밍(OOP)에서 특정 객체를 생성하기 위해 변수와 메소드를 정의하는 일종의 틀(template)이다.object : 클래스에서 정의한 것을 토대로 메모리(실제 저장공간)에 할당된 것으로 프로그램에서 사용되는 데이터 또는 식별자에 의해 참조되는 공간을 의미객체지향 프로그래밍에서 객체는 클래스의 인스턴스이다.메소드(method)  클래스가 가지고 있는 함수를 의미생성자(constructor)  클래스로부터 인스턴스가 생성될 때 자동으로 실행되는 함수상속(inheritance)  새로운 클래스를 만들 때 기존에 있던 클래스의 기능을 물려 받을 수 있는 역할상속 선언  상속 선언을 할 시 자식 클래스에서 부모 클래스의 속성과 메소드는 따로 기재하지 않아도 자동으로 포함메소드 오버라이딩  부모 클래스의 메소드를 자식 클래스에서 재정의 하는 것일반적인 메소드 오버라이딩 : 자식 클래스에서 생성된 객체의 메소드를 부르면 부모 클래스의 메소드는 무시부모 메소드 호출 : 부모 클래스의 메소드도 수행하고 자식 클래스의 메소드도 함께 수행하고 싶을 때 사용코드로 한번에 보는 정리class animal():    def __init__(self, name, voice):        self.name = name        self.voice = voice    def say(self):        print(self.name + \"(이)가 \" + self.voice + \"하고 웁니다\")class cry_01(animal):    def say(self):        print(\"아직 울지 못해요..\")class cry_02(animal):    def say(self):        super().say()        print(\"신기하네요\")cat = animal('고양이', '냐옹')cat.say() # 고양이(이)가 냐옹하고 웁니다baby_cat = cry_01('고양이', '냐옹')baby_cat.say() # 아직 울지 못해요..cat = cry_02('고양이', '먀')cat.say()# 고양이(이)가 먀하고 웁니다# 신기하네요  animal : classobject : catcat은 animal의 인스턴스method : animal class의 say함수생성자 : init상속 선언 : cry_01(animal)일반적인 메소드 오버라이딩 : cry_01 클래스 안에 say함수 재정의부모 메소드 호출 : cry_02 클래스 안에 super().say() 호출",
      "url": "/2024/11/22/Python_Class.html"
    },
  
    {
      "title": "Machine Learning At Work - 12",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 12 - 2024년11월21일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 12 - 2024년11월21일  tag : Machine_Learning_At_Work|Book|온라인 광고에서의 머신러닝  온라인 광고 송출 시스템은 광고 요청을 받은 시점에서 실시간 추론을 하고 결과를 이용해 즉작적으로 최적의 행동을 해야 한다.온라인 광고 비즈니스광고 영역 거래  광고 송출 사업자(DSP, demand side platform) : 광고주를 위한 서비스          광고를 내보낼 수 있는 기회가 있을 때마다 광고 영역을 얼마에 판매하고 어떤 광고를 표시할지 결정해야 한다.        광고 영역 매매 거래 -&gt; 광고 거래소를 통한 경매  방법 : 1위 가격 경매, 봉인 입찰 1위 가격 경매DSP의 행동 정책  한정된 예산으로 최대 광고 효과를 얻을 수 있도록 하는 방법          광고를 표시할 때 오디언스의 응답 예측      광고 영역의 시장가격 예측      예산 안에서의 입찰 금액 최적화      1위 가격 경매의 특징  다른 입찰자가 제시한 최고 입찰 가격에 따라 최저 입찰 가격이 결정된다.잉여 : 경매로 얻은 물품의 가치와 지불 금액의 차이최적 입찰 가격 : 잉여를 최대로 하는 가격문제 정식화시장가격과 승률  최고 입찰금액 : z  입찰 금액 : b  입찰 금액 b을 경정했을 때의 승률 : w(b)  승률 함수 : w(x, b)효용(utility)  광고주가 얻는 잉여를 경제학 용어인 효용이라고 바꾸어 부른다.  오디언스의 응답값(이진값이라고 가정) : \\(y \\in \\{0, 1\\}\\)  오디언스에서 얻은 긍정적인 응답을 금액으로 환산한 값 : v  \\[효용 = \\left\\{\\begin{matrix}vy - b ( 경매에 승리했을 때 ) \\\\ 0 (기타)\\end{matrix}\\right.\\]    \\[기대 효용 = \\sum_{i=1}^{N}[vf(x_i)-b_i]w(x_i, b_i)\\]  예측의 역할 및 구현오디언스의 응답 예측  광고 클릭률 예측(CTR, click through rate prediction) and 전환율 예측(CVR, conversion rate prediction)이 있다.MODEL : 로지스틱 회귀, GBDT, Factorization Machine  기대가치          가치 있는 광고 영역과 오디언스를 높게 평가할 수 있다.      가치 없는 광고 영역을 낮게 평가할 수 있다.      승률(시장가격) 예측  기대가치          불필요하게 높은 입찰을 억제할 수 있다.      승률을 통제할 수 있다.      광고 송출 로그의 특징  피드백 루프  불균형 데이터          배너 광고가 클릭되는 경우는 실제로 매우 적다 -&gt; 네거티브 다운 샘플링 사용네거티브 다운 샘플링 장점 : 전처리와 훈련 계산 비용의 감소        카디널리티(표현하는 값의 패턴)가 큰 범주형 변수          원-핫 인코딩 상태에서 L1 정규화를 사용해 특징량을 선택하는 방법최근에는 인테테 임베딩 사용        잘린 데이터          EX. 신용카드 광고라면 신용카드 신청 후 심사가 완료되는 지점이 전환 시점이 된다 -&gt; 전환 알림이 오기까지 시간이 걸림 -&gt; 잘린 데이터 생성잘려진 데이터를 분석할 때는 관측 지연 시간 분포에서 경향 점수를 구한 후 레코드별로 샘플 가중치를 보정해야 한다.      머신러닝 예측 모델 운영예측 실패 시 처리  복원할 수 있는 일이라면 간단한 제어 기능과 조합해 해결할 수 있다.리스크에 맞춰 stopper를 적용해 장애를 줄일 수 있다.",
      "url": "/2024/11/21/MachineLearningAtWork-12.html"
    },
  
    {
      "title": "Machine Learning At Work - 11",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 11 - 2024년11월20일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 11 - 2024년11월20일  tag : Machine_Learning_At_Work|Book|슬롯머신 알고리즘을 활용한 강화 학습 입문  현실에서는 데이터가 충분하지 않거나 데이터 수집에 상당한 비용이 든다.이때는 지도 학습을 하지 않고 훈련 데이터를 수집하는 동시에 학습을 수행하는 강화 학습을 활용슬롯머신 알고리즘 용어 정리  팔 : 어떤 시점에서 선택 가능한 선택지를 의미  방책 : 미리 정한 알고리즘에 따라 팔을 선택하는 방법을 의미, 정책(policy)라고도 하며 방책에 따라 슬롯머신 알고리즘의 성능이 좌우한다.  시행 : 팔을 선택하면 얻는 보상을 의미한다. ‘(팔을)당긴다’라는 용어로 사용하기도 한다.  보상 : 어떤 팔을 선택했을 때 얻을 수 있는 가치를 의미한다. return이라 부르기도 한다. 각 팔의 고유한 확률 밀도 분포에 따라 보상을 얻는다고 가정  표본 : 시행 결과에 따라 얻는 보상의 집합을 의미, 당기는 팔이 같아도 얻는 보상은 매번 다르다는 것에 주의  탐색 : 적은 시행 횟수로 정보가 많지 않아 모평균이 불확실한 팔을 선택하는 것을 의미,          탐색을 통해 선택한 팔의 시행 횟수가 증가하면 그에 따라 정보의 양도 증가해 신뢰 구간이 좁아진다.        활용 : 여러 팔 중에서 모평균이 큰 팔을 선택하는 것을 의미, 활용을 통해 여러 차례 시행해 누적 보상을 최대화하는 것을 목표로 한다.확률적 슬롯머신 알고리즘비즈니스 현장에서는 최적의 팔을 선택하는 것이 반드시 최적의 전략이라고는 할 수 없다.비슷한 수준의 보상을 제공하는 팔이 여럿이라면 어떤 것을 선택해도 무방하다.알고리즘 종류  무작위 알고리즘  베이즈 UCB, UCB1  소프트맥스 알고리즘  어닐링 소프트맥스  TS 알고리즘정리기존의 라이브러리- ZOZO Technologies에서 공개한 OPEN BANDIT PIPELINE- 각종 콘텍스트 기반 슬롯머신 알고리즘을 비교한 contextualbandits슬롯머신 알고리즘은 훈련 데이터가 없거나 사용자에게 추천할 때 효과적으로 동작후기  책을 보면서 수식은 이해가 가지만 작동원리나 실제 사례에서 사용한 코드에 대한 이해가 부족조금 자세하게 들여다 보거나 다른 예제를 보면서 이해하는 시간이 필요해 보임",
      "url": "/2024/11/20/MachineLearningAtWork-11.html"
    },
  
    {
      "title": "Machine Learning At Work - 10",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 10 - 2024년11월19일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 10 - 2024년11월19일  tag : Machine_Learning_At_Work|Book|업리프트 모델링을 이용한 마케팅 리소스 효율화  업리프트 모델 : 역학 통계나 다이렉트 마케팅에서 활용되는 머신러닝 기법업리프트 모델링의 사분면            개입 안 함      개입함      범주      대응 방안                  전환하지 않음      전환하지 않음      무관심      비용이 드는 개입은 자제한다              전환하지 않음      전환함      설득 가능      가능한 개입 대상에 포함시킨다              전환함      전환하지 않음      청개구리      개입 대상에 절대로 포함시키지 않는다              전환함      전환함      잡은 물고기      비용이 드는 개입은 자제한다.      데이터 셋 만들기import randomdef generate_sample_data(num, seed=1):    is_cv_list = []          # 전환여부    is_treat_list = []       # 실험군 여부    feature_vector_list = [] # 8차원 특징량    random_instance = random.Random(seed)    feature_num = 8    base_weight = [0.02, 0.03, 0.05, -0.04, 0.00, 0.00, 0.00, 0.00] # 대조군이 가진 가중치    lift_weight = [0.00, 0.00, 0.00, 0.05, -0.05, 0.00, 0.00, 0.00] # 개입에 의해 달라지는 가중치    for i in range(num):        feature_vector = [random_instance.random() for n in range(feature_num)]         # [0, 1] 로 구성된 8차원 특징량 벡터 생성        is_treat = random_instance.choice((True, False))                                # 실험군과 대조군을 무작위로 결정        cv_rate = sum([feature_vector[n] * base_weight[n] for n in range(feature_num)]) # 내부적인 전환율 구하기        if is_treat: # 실험군에 속하면 lift_weight의 영향을 추가            cv_rate += sum([feature_vector[n] * lift_weight[n] for n in range(feature_num)])        is_cv = cv_rate &gt; random_instance.random() # 전환 여부        is_cv_list.append(is_cv)        is_treat_list.append(is_treat)        feature_vector_list.append(feature_vector)    return is_cv_list, is_treat_list, feature_vector_list\\[cv\\_rate = \\left\\{\\begin{matrix} feature\\_vector \\cdot base\\_weight \\cdots (대조군)\\\\ feature\\_vector \\cdot (base\\_weight + lift\\_weight) \\cdots (실험군)\\end{matrix}\\right.\\]두 가지 예측 모델을 이용한 업리프트 모델링  원시적인 형태의 업리프트 모델링에서는 실험군과 대조군 각각의 예측 모델을 만든다.      대조군 예측 모델에서는 개입을 하지 않았을 때의 전환율을 예측한다.    실험군 예측 모델에서는 개입했을 때의 전환율을 예측한다.  # train 데이터 생성sample_num = 100000train_is_cv_list, train_is_treat_list, train_feature_vector_list = generate_sample_data(sample_num, seed=1)# 데이터를 treatment(실험군)와 control(대조군)로 분리treat_is_cv_list = []treat_feature_vector_list = []control_is_cv_list = []control_feature_vector_list = []for i in range(sample_num):    if train_is_treat_list[i]:        treat_is_cv_list.append(train_is_cv_list[i])        treat_feature_vector_list.append(train_feature_vector_list[i])    else:        control_is_cv_list.append(train_is_cv_list[i])        control_feature_vector_list.append(train_feature_vector_list[i])# treatment_cvr 0.031220247540463344# control_cvr 0.031905453372055505          실험군과 대조군의 전환율의 차이가 거의 없다고 볼 수 있다.A/B TEST 였다면 유의미한 차이가 아니므로 실패로 판단했을 것# 전환 예측from sklearn.linear_model import LogisticRegression# 학습기 생성treat_model = LogisticRegression(C=0.01)control_model = LogisticRegression(C=0.01)# 학습기 구현treat_model.fit(treat_feature_vector_list, treat_is_cv_list)control_model.fit(control_feature_vector_list, control_is_cv_list)  업리프트 모델링 점수 = 실험군 예측값 / 대조군 예측값# predict_proba : 특징 벡터를 인수로 받아 특정 클래스에 소속될 확률을 numpy.ndarray 타입 배열로 얻을 수 있다.test_is_cv_list, test_is_treat_list, test_feature_vector_list = generate_sample_data(sample_num, seed=42)# 각 학습기별로 전환율 예측treat_score = treat_model.predict_proba(test_feature_vector_list)control_score = control_model.predict_proba(test_feature_vector_list)# 점수를 계산한다(실험군 에측 전환율 / 대조군 예측 전환율)# predict_proba는 각 클래스에 속할 확률의 리스트를 반환하므로 첫 번째 값만 확인한다.# 반환값이 numpy.ndarray 타입이므로 그대로 나눠도 요소 단위 나눗셈이 된다.score_list = treat_score[:,1] / control_score[:,1]  시각화  점수가 높을수록 실험군의 전환율이 높고 대조군은 낮다는 것을 확인할 수 있다.그래프 기준으로 점수 상위 40%에만 개입하면 전체 전환율을 개선할 수 있다는 것도 알 수 있다.AUCC로 업리프트 모델링 평가  AUCC : area under the uplift scuveAUUC 값이 클수록 업리프트 모델링 성능이 좋은 것  순서          점수가 높은 순서대로 대상을 살펴보면서 그 시점까지의 전환율을 측정한다.      전환율의 값 차이에서 개입이 일으킨 전환 증가 건수(lift)를 계산한다.      무작위로 개입했을 때 전환 증가 건수인 lift의 원점과 끝점을 연결한 직선을 베이스 라인으로 가정한다.      lift와 base_line이 둘러싼 영역의 넓이를 계산하고 정규화해 AUUC를 구한다.        AUUC 값 : 267.09977536176643  업리프트 모델링의 정확할수록 높은 점수에서는 실험군에서 전환하는 고객, 대조군에서는 전환하지 않은 고객이 모이며 낮은 점수에서는 반대가 된다.이 때문에 lift곡선은 처음에는 실험군의 전환이 모여 양의 기울기를 가지며 정확도가 향상되면서 양의 기울기가 커진다.마지막 부분에는 대조군의 전환이 모여 음의 기울기를 가지며 정확도가 향상되면서 기울기가 커진다.따라서 lift곡선은 정확도가 높을수록 위쪽으로 볼록해지며 lift와 base_line으로 둘러싸인 넓이와 AUUC 점수가 커진다.  실제 서비스에서는 이 점수를 기반으로 해 개입 여부를 결정해야 한다.개입 기준점을 정하기 위해 점수를 가로축으로 하는 lift모델 그리기  모델링 점수 1.2 이상에서 개입하면 lift를 최대가 된다는 것을 알 수 있다.- Memo&gt; uplift 모델에 대해서 조금 더 자세한 공부가 필요해 보인다.  &gt; 결과를 해석하는데 조금 어려움을 겪음",
      "url": "/2024/11/19/MachineLearningAtWork-10.html"
    },
  
    {
      "title": "Machine Learning At Work - 09",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 09 - 2024년11월18일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 09 - 2024년11월18일  tag : Machine_Learning_At_Work|Book|킥스타터 분석하기 : 머신러닝을 사용하지 않는 선택지  가공하지 않은 데이터를 직접 보는 것은 분석 효율에 큰 영향을 준다.머신러닝을 이용하더라도 데이터의 전체적인 형태를 파악해두지 않으면 결과에 이상이 있을 때 그 원인을 깨닫기 어렵다.반드시 가공되지 않은 데이터를 직접 살펴보도록 하자.데이터 살펴보기  모금액(pledged)과 목표액(goal) 정보를 이용해 달성률(pledged/goal) 확인  100% 인 부분에서 특이점이 발생하는 것 처럼 보인다.상태(state)를 기준으로 필터링한 뒤 비교해보자.  state가 live이면서 마감에 도달하지 않은 프로젝트에는 이 특이점이 없는 것 같다.즉 이 특이점은 프로젝트 종료 직전에 생겨난 것이라는 사실을 알 수 있다.  why? 킥스타터는 종료 시점에 임박한 프로젝트를 최상위 페이지에 소개한다.  why? 거의 목표에 달성했을 때 프로젝트 제안자가 더 열심히 홍보한다.피벗 테이블로 다양하게 파악하기  달성률을 기준으로 파악하기state가 live인 데이터는 제외하고 종료된 프로젝트만을 대상으로 진행통화 단위는 USD로 모금한 프로젝트로 대상을 좁힌다.  달성률에 따라 프로젝트를 크게 3가지로 분류 가능      달성률 50% 미만\t：　전형적인 실패한 프로젝트    달성률 50~200%\t：　종료 즈음에 후원이 집중된 프로젝트    달성률 200% 초과\t：　크게 성공한 프로젝트    전형적인 실패한 프로젝트달성률 10% 미만인 프로젝트가 54% 차지프로젝트가 성공하려면 후원자 100명을 넘길 방법을 고민해야 함보고서 작성하기  분석 절차를 보여주면서 어떤 방법을 사용했는지분석 결과와 그래프 그리고 발견한 사실 및 정리 후 해결 방안까지 정리 후 작성하기  보고서 예시",
      "url": "/2024/11/18/MachineLearningAtWork-09.html"
    },
  
    {
      "title": "Foundation Model",
      "tags": "Foundation_Model, Blog",
      "desc": "Foundation Model - 2024년11월17일  tag : Foundation_Model|Blog|",
      "content": "Foundation Model - 2024년11월17일  tag : Foundation_Model|Blog|Foundation Model정의대규모 데이터세트를 기반으로 훈련된 파운데이션 모델(FM)은 데이터 사이언티스트가 기계 학습(ML)에 접근하는 방식을 변화시킨 대규모 딥 러닝 신경망데이터 사이언티스트는 처음부터 인공 지능(AI)을 개발하지 않고 파운데이션 모델을 출발점으로 삼아 새로운 애플리케이션을 더 빠르고 비용 효율적으로 지원하는 ML 모델FM이라는 용어는 연구자들이 광범위한 일반화된 데이터와 레이블이 지정되지 않은 데이터에 대해 훈련되고언어 이해, 텍스트 및 이미지 생성, 자연어 대화와 같은 다양한 일반 작업을 수행할 수 있는 ML 모델을 설명하기 위해 만들어졌습니다.독특한 점적응성일반적으로 감정을 포착하기 위한 텍스트 분석, 이미지 분류, 트렌드 예측과 같은 특정 작업을 수행하는 기존의 ML 모델과는 다른블로그 게시물 작성, 이미지 생성, 수학 문제 해결, 대화 참여, 문서 기반 질문 등 여러 영역에 걸쳐 다양한 작업을 수행할 수 있다.중요한 이유  현재 FM을 처음부터 개발하는 데 수백만 달러의 비용이 들지만 장기적으로는 유용하다.데이터 사이언티스트는 고유한 ML모델을 처음부터 훈련시키는 것보다 사전 훈련된 FM을 사용하여 새로운 ML 애플리케이션을 개발하는 것이 더 빠르고 저렴하다.어떻게 작용하는가?  GAN, 트랜스포머, 변량 인코더 등 복잡한 신경망을 기반으로 한다.FM모델은 자체 지도 학습을 사용하여 입력 데이터에서 레이블을 생성한다.즉, 레이블이 지정된 훈련 데이터 세트로 모델을 가르치거나 훈련시킨 사람은 아무도 없다.무엇을 할 수 있는가?  자연어 처리  시각적 이해  코드 생성  인간 중심의 참여  음성을 텍스트로 변환EX  BERT  GPT  Amazon Titan  AI21 Jurassic  Claude  Cohere  Stable Diffusion  BLOOM  Hugging Face도전과제  도전 과제          인프라 요구 사항      이해력 부족 : 프롬프트의 문맥을 이해하는 데 어려움을 겪는다.      신뢰할 수 없는 답변 : 특정 주제에 대한 질문에 대한 답변은 신뢰할 수 없으며 때로는 부적절하거나 유해하거나 틀릴 수 있다.        참고 사이트",
      "url": "/2024/11/17/Foundation_Model.html"
    },
  
    {
      "title": "Machine Learning At Work - 08",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 08 - 2024년11월16일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 08 - 2024년11월16일  tag : Machine_Learning_At_Work|Book|머신러닝 모델 해석하기비즈니스 환경에서는 예측 결과를 상사나 클라이언트, 사용자에게 설명해야 할 때가 있다.예측 이유는 무엇이며 어떤 특징량이 기여하는지, 어떤 특징량을 조합하면 더 나은 예측이 가능한지 등이다.예측 이유와 사람의 직감이 일치하는지 판단해 예측 모델의 타당성을 평가한다.  유명한 논문 Grad-CAM회귀  목적 변수와 숫자 타입을 추출한다.          한 종류의 값만 갖는 열은 예측에 사용할 수 없으니 버린다.개인을 특정할 수 있는 특징량(EX. 임직원번호)은 과적합의 원인이 되어 제거한다.범주 타입 데이터를 원-핫 인코딩한 뒤 숫자 타입과 결합하여 데이터프레임을 만든다.        데이터를 만들었으니 상관행렬을 확인해 데이터의 경향을 파악한다.  선형 회귀 계수를 이용한 원인 해석          절편(intercept)의 값은 다른 계수에 따라 달라져 우선은 무시Min-Max scaler를 활용해서 데이터를 0~1의 영역으로 정규화한 뒤 확인하기회귀 계수의 값이 발산했다면 다중공선성 때문이다.다중공선성 : 한 특징량이 다른 특징량과 매우 비슷해 예측 가능한 상태에 있음을 의미이를 해결하기 위한 한가지 방법은 pd.get_dummies를 할 때 drop_first 옵션을 활성화하면 원-핫 인코딩을 할 때 한 종류의 데이터를 버리고 n-1 종류의 데이터로 만들 수 있다.다중공선성이 있는 데이터에서는 회귀 계수가 커지는 과적합이 쉽게 일어나 라쏘, 릿지, 일레스틱넷과 같이 정규항을 추가해 회귀 계수가 커지는 것을 억제하는 선형 회귀 알고리즘을 사용한다.        회귀 계수 p값 구하기          p값이 큰 특징량은 목적 변수에 기여하지 않는다고 판단한다.      분류  분류 원인 분석 중 DecisionTree or dtreeviz를 사용해서 시각화해서 볼 수 있다.shap은 분류 모델은 회귀 모델로 사용할 수도 있지만, 목적 변수가 이진값이면 회귀 모델로 입력하는 것이 시각화가 잘 된다",
      "url": "/2024/11/16/MachineLearningAtWork-08.html"
    },
  
    {
      "title": "Machine Learning At Work - 07",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 07 - 2024년11월15일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 07 - 2024년11월15일  tag : Machine_Learning_At_Work|Book|효과 검증: 머신러닝 기반 정책 성과 판단하기효과 검증  비즈니스 지표를 이용한 정책 평가          팀 내부에서 공유할 때는 ‘이 기능을 출시하면 예측 정확도가 향상된다.’정도로 충분하지만팀 외부로 보고할 때는 예측 정확도가 향상되면 무엇이 좋아지는지 설명해야한다.            정책을 실행한 후의 효과 검증의 필요성    오프라인 검증과 온라인 검증          오프라인 검증 : 과거 데이터를 사용해 시뮬레이션온라인 검증 : 실제 제품에 머신러닝을 적용해 실험을 수행        지표 선정인과 효과 추정  상관관계와 인과관계          상관관계가 있다고 해서 반드시 인과관계가 있는 것은 아니다        루빈의 인과관계 모델          EX. 인터넷 광고의 효과인과 추론에서는 광고를 보여주는 것구매 행동을 결과 변수개입한 표본을 개입군 또는 실험군개입하지 않은 표본을 대조군 또는 통제군        선택 편향에 의한 위장 효과          구하고자 하는 평균 처리 효과를 계산할 때는 ‘개입군의 결과 변수의 평균’과 ‘대조군의 결과 변수의 평균’의 차를 구하면 충분선택 편향은 개입과 결과 변수 사이에 상관관계를 만들어 효과가 없음에도 효과가 있는 듯 보이는 데이터가 된다.        무작위 비교 시험(Randomized Control Trial, RCT)          편향이 없는 상태를 만들어서 비교하는 방법        과거와 비교한 판단가설 검정 프레임  가설 검정 : A/B TEST에 기반한 표본(샘플)을 사용해 모집단을 판단하는 기법  거짓 양성과 거짓 음성          드물게 발생하는 일이 발생하면 귀무 가설을 기각한다고 판단하면 귀무가설이 참이라고 해도 유의 수준의 확률에 따라 귀무가설을 잘못 기각할 수 있다.이처럼 잘못된 발견을 거짓 양성이라고 한다.반대로 실제로는 유의한 차이가 있지만 귀무가설을 기각하지 않는 경우를 거짓 음성이라고 한다.      A/B TEST 설계 및 수행  과정          지표 선정      두 그룹 선정      A/A 테스트 : 무작위 추출에 따라 품질이 같은 두 그룹을 얻었는지 확인하는 것      한 그룹에 개입      결과 확인      테스트 종료      오프라인 검증  비즈니스 지표를 사용한 예측 모델 평가          예측 모델 정확도가 비즈니스 지표에 미치는 영향을 알기는 어렵지만 예측을 기반으로 행동한 결과를 고려하면 더욱 쉽게 알 수 있다.        반사실 다루기          분명하지 않은 결과를 다룰 때 가장 편한 방법은 상환 역략을 예측하고 이를 이용한 결과를 만들어 평가하는 것        Off Policy Evaluation          효과 검증에서 중요한 점은 예측 모델의 출력 자체가 아니라 예측으로 어떤 행동을 하는 시스템 관점에서의 성능OPE : 과거의 다른 정책에 따라 생성한 로그에 기반해 행동 결정 정책을 평가하는 것      A/B TEST를 수행할 수 없을 경우  관찰 데이터를 사용한 효과 검증          차이의 차이 기법 : 실험군과 유사한 경향을 나타내는 그룹의 결과 변수의 시간 변화를 활용해 개입 전후를 비교하는 기법      ",
      "url": "/2024/11/15/MachineLearningAtWork-07.html"
    },
  
    {
      "title": "Machine Learning At Work - 06",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 06 - 2024년11월14일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 06 - 2024년11월14일  tag : Machine_Learning_At_Work|Book|지속적인 머신러닝 활용을 위한 기반 구축하기머신러닝 시스템만의 독특한 어려움  데이터 과학자 vs 소프트웨어 개발자    &gt; 데이터 과학자의 KPI는 분석 결과에서 도출한 후 액션이나 높은 정확도의 예측 모델을 사용한 매출 개선 등이다.&gt; 소프트웨어 개발자는 시스템이 운영 환경에서 잘 운영되도록 개발하는 것을 중시        동일한 예측 결과를 얻기 어렵다.    &gt; 머신러닝 시스템에서는 조금이라도 변경(EX.데이터)하면 시스템 전체의 행동이 바뀐다.&gt; 이 특성을 CACE(change anything, change everything)이라고 부른다.        지속적인 학습(Continuous Training)과 서빙이 필요하다.    &gt; 예측 모델을 업데이트할 때는 새로운 데이터 준비만으로는 충분하지 않다.&gt; 데이터양이 많아지면 예측 모델을 학습하는 데 매우 많은 시간이 걸려 계산 시간을 줄일 수 있는 구조도 필요하다.      지속적인 학습과 MLOps  MLOPS : ML 시스템 개발(Dev)과 ML시스템 운영(Ops)의 통합을 목표로 하는 ML 엔지니어링 문화 및 방식MLOps를 통해 통합, 테스트, 출시, 배포, 인프라 관리를 비롯한 ML 시스템 구성의 모든 단계에서 자동화 및 모니터링을 지원할 수 있다.  수행          CI(Continuous Integration) : 소스 관리, 단위 테스트, 통합 테스트의 지속적 통합      CD(Continuous Delivery) : 소프트웨어 모듈 또는 패키지의 지속적 배포      CT(Continuous Training) : 모델의 지속적 학습      머신러닝 인프라 구축 단계  공통 실험 환경          공통 실험용 도커 이미지 등을 만들고 버전 관리를 해 실험 환경을 쉽게 재현      공통 주피터 랩 또는 클라우드 서비스에서 제공하는 주피터 환경을 사용해 실험 내용을 공유하고 재 실행해서 쉽게 확인 및 리뷰를 할 수 있다.      MLflow와 같은 실험 관리 도구를 활용해 같은 모델을 학습하는 데 필요한 정보나 실험 결과를 쉽게 공유하도록 한다.                  IF 데이터 과학자가 도커 이미지를 만들기 어려워한다면 cookiecutter or cookiecutter-docker-science등의 도구를 활용해 도커 이미지 템플릿을 만드는 것이 좋다.Hydra등을 활용해 설정 파일로 다양한 파라미터를 관리하면 더욱 쉽게 실험을 재현할 수 있다.                      예측 결과 서빙          학습한 예측 모델을 실제 운영 환경에 배포하고 예측 API 서버를 통해 결과를 반환하는 구조flask 같은 간단한 웹 애플리케이션을 적용해 간단히 구현할 수 있다.      클라우드 서비스 : 구글 클라우드 AI 플랫폼 프리딕션, 아마존 세이지메이커의 포스팅 서비스OSS로 직접 구축  - 특정 프레임워크에 특화된 서빙 : 텐서플로 서빙, 터치서브  - 여러 프레임워크에 대응하는 서빙 : 샐던 코어, BentoML, Cortex모델 버전 관리 : MLflow 모델 레지스트리, 아마존 세이지메이커, 구글 클라우드 AI 플랫폼 프리딕션              학습 및 예측의 공통 처리 파이프라인          워크플로 엔진 사용                  범용 소프트웨어 : 아파치 에어플로, 퍼펙트클라우드 서비스 : 구글 클라우드 컴포저, 워크플로스, 아마존 매니지드 워크플로스 포 아파치 에어플로머신러닝용으로 만들어진 것 : 메타플로, 쿠베플로 파이프라인, 케드로          전처리를 공통화하는 의미 : 피처 스토어 컴포넌트클라우드 서비스 : 아마존 세이지메이커 피처 스토어, OSS인 피스트, 홈스웍스                                지속적인 모델 학습과 배포          새로운 모델의 재학습과 배포를 자동화하기 위해 갖춰야 할 구조```      특징량 엔지니어링 로직 단위 테스트      학습 시의 손실 수령 여부 테스트      파이프라인 산출물 생성 확인 테스트(모델 등)      예측 시 초당 쿼리 수      예측 정확도의 임계값 초과 여부 테스트      스테이징 환경으로의 풀 리퀘스트 병합 등을 트리거로 하는 모델 배포 테스트      스테이징 환경에서의 파이프라인 동장 검증 테스트```      지속적인 예측 결과 서빙  감시 및 모니터링          예측 결과 서빙 시 지표                  메모리/CPU 등 하드웨어 리소스 사용량예측 응답 시간예측값이 존재하는 윈도우의 평균값, 최댓값, 최솟값, 표준편차 등 통곗값이나 분포입력값의 통계값, 특히 결손값 또는 NAN의 빈도                    학습 시 지표        정기적인 테스트          운영 환경에서 예측한 결과와 최신 정답 데이터와의 예측 정확도 검증      데이터 품질 검증      ",
      "url": "/2024/11/14/MachineLearningAtWork-06.html"
    },
  
    {
      "title": "Machine Learning At Work - 05",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 05 - 2024년11월13일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 05 - 2024년11월13일  tag : Machine_Learning_At_Work|Book|학습 리소스 수집하기학습 리소스 수집 방법  입력 : 엑세스 로그 등에서 추출한 특징량  출력 : 분류 레이블 또는 예측값  공개된 데이터셋이나 모델 활용          UCI 머신러닝 저장소      Kaggle      ImageNet        주의할 점      모델이나 데이터셋의 라이선스는 상업적으로 사용할 수 있는가?    내가 운용하는 시스템이나 서비스에 학습 완료된 모델이나 데이터셋을 적용할 수 있는가?    개발자가 직접 훈련 데이터 작성          기존 데이터를 활용할 수 없을 때는 필요한 훈련 데이터를 개발자가 직접 만든다.        동료나 지인에게 데이터 입력 요청          EX) 스프레드시트에 대상 데이터를 나열하고 레이블을 부여하는 것여러 명에게 작업을 의뢰할 때는 사전에 데이터 내용을 말로 표현할 수 있도록 작업 내용이나 판단 기준을 확실하게 설명해야 한다.        크라우드소싱 활용          랜서스 or 크라우드웍스 : 다수의 불특정 사용자가 경쟁하는 방식(공모형)      아마존 미캐니컬 터그 or 야후!크라우드소싱 : 단기간에 가능한 단순 작업을 의뢰하는 형태(마이크로 태스크 방식)        서비스에 통합해서 사용자가 입력          EX) reCAPTCHA      ",
      "url": "/2024/11/13/MachineLearningAtWork-05.html"
    },
  
    {
      "title": "Machine Learning At Work - 04",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 04 - 2024년11월12일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 04 - 2024년11월12일  tag : Machine_Learning_At_Work|Book|기존 시스템에 머신러닝 통합하기시스템 설계  배치 처리  : 일괄적으로 어떤 처리를 하는 것 또는 그러한 처리 자체를 의미 &lt;-&gt; 실시간 처리 : 실시간으로 전송되는 센서 데이터나 로그 데이터를 순차적으로 처리하는 것배치 학습(=일괄 학습)온라인 학습(=순차 학습)  배치 처리에서 학습을 수행하는 세 가지 예측 패턴과 실시간 처리 패턴          배치 처리로 학습과 예측, 예측 결과를 DB에 저장하고 서비스를 제공                  웹 애플리 케이션에 활용하기 좋은 패턴이며 첫 번째로 시도하기에 무난한 방법                      특징예측에 필요한 정보는 예측 배치 처리를 실행할 때만 필요하다.이벤트에 대한 예측 결과를 즉시 반환하지 않아도 된다.                              배치 처리로 학습, 실시간 처리로 예측, 예측 결과를 API 경유로 제공                  웹 애플리케이션과 달리 예측 처리를 별도 API 서버에서 제공하는 패턴                      특징웹 애플리케이션 등의 클라이언트와 머신러닝에 사용하는 프로그래밍 언어를 분리할 수 있다.웹 애플리케이션 등 클라이언트 측 이벤트가 발행했을 때 실시간으로 예측을 수행할 수 있다.간단하게 테스트하고 싶다면 : 구글 인공지능 예측 플랫폼, 애저 머신러닝, 아마존 머신러닝 or BentoML, Cortex [ 예측 결과 서빙 프레임워크 ]예측 API 서버를 직접 만들지 않고도 AWS 람다, 클라우드 런으로 이벤트에 맞춰 확장할 수 있는 환경도 구축할 수 있다.                              배치 처리로 학습, 최종단 클라이언트에서의 실시간 처리로 예측                  서버 사이드에서 배치 처리로 학습한 모델로 사용해 스마트폰 또는 브라우저의 자바스크립트, 임베디드 장치 등의 최종단 클라이언트에서 예측하는 패턴                      특징학습한 모델을 클라이언트에서 이용할 수 있는 크기로 최적화해서 변환클라이언트에서 예측을 수행해 통신 지연으로 인한 예측 시간을 줄인다.                              실시간 처리로 학습, 예측, 서비스를 제공                  슬롯 머신 알고리즘 등의 일부 알고리즘이나 실시간 추천이 필요하다면 파라미터를 실시간으로 즉시 업데이트해야 한다.                    로그 설계특징량과 훈련 데이터에 사용할 정보  사용자 정보  콘텐츠 정보  사용자 행동 로그로그 저장 위치  분산 RDMBS, 데이터 웨어하우스  분산 처리 기반의 하둡 클러스터인 하둡 분산 파일 시스템에 저장  클라우드의 오브젝트 스토리지에 저장최근에는 아마존 레드시프트, 구글 빅쿼리 같은 풀 매니지먼트가 가능한 클라우드형 분산 DB서비스가 제공되고 있다.아파치 하이브, 아파치 임팔라, 프레스토 등 하둡과 조합해 동작하는 SQL 엔진을 사용하면 SQL로 데이터에 손쉽게 접근할 수 있다.SQL과 아파치 스파크를 조합해 DataFrame API를 사용한 절차적 처리도 고려할 수 있다.하둡 사용과 비슷한 클라우드 저장소를 이용하는 방법도 있다.이때는 AWS글루, 구글 클라우드 데이터프록, 애저 HDinsight 같은 매니지드 분산 처리 서비스를 사용해 SQL이나 맵리듀스뿐만 아니라 스파크를 사용한 복잡한 처리도 할 수 있다.최근에는 아마존 S3 같은 객체 저장소에 저장하고 임팔라, 하이브, 프레스토 또는 AWS 아테나로 쿼리를 직접 실행하는 형태도 늘고 있다.하지만 최종적으로는 데이터 웨어하우스에 저장해 SQL로 접근할 수 있도록 하는 것이 좋다.로그는 Fluentd나 아파치 플룸, 로그스태시 같은 로그 수집 소프트웨어로 수집한 후 저장 장소로 전송한다.최근에는 Embulk 같이 배치로 데이터를 전송하는 소프트웨어나 분산 메시징 시스템인 아파치 카프카를 활용해 스케쥴링 로그 수집 기반을 만들거나 구글 클라우드 로깅 같은매니지드 로그 관리 서브시를 이용하는 등 다양한 선택지가 있다.로그 설계 시 주의점  첫 시도에 유효한 특징량을 찾아내기는 어렵다.  KPI를 설계할 때는 가능한 적은 지표 수를 설정하는 것이 좋지만 머신러닝에 사용하는 정보는 많은 편이 좋다.  현재 수집 중인 로그에서 훈련 데이터를 만들 것인지도 고려해야 한다.",
      "url": "/2024/11/12/MachineLearningAtWork-04.html"
    },
  
    {
      "title": "Machine Learning At Work - 03",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 03 - 2024년11월11일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 03 - 2024년11월11일  tag : Machine_Learning_At_Work|Book|학습 결과 평가하기분류 평가정확도  \\(정확도 =  \\frac{정답과 일치한 수}{전체 데이터 수}\\)현실에서는 분류할 클래스별로 편차가 있는 경우가 많고 단순히 정확도를 적용하는 것은 대부분 큰 의미가 없다.정밀도  정밀도 : 출력 결과가 실제로 얼마나 정답이었는가재현울  재현율 : 출력 결과가 실제 정답 중 어느 정도의 비율을 차지하는가정밀도와 재현율은 상충관계에 있으며 문제 설정에 따라 중시하는 쪽이 달라진다.  놓치는 데이터 수가 많더라도 더욱 정확한 예측이 필요하다면 정밀도를 중시다소 잘못 걸러내는 비율이 높더라도 데이터 누락을 최소화하고자 할 때는 재현율을 중시F값  \\(F값 = \\frac{2}{\\frac{1}{정밀도} + \\frac{1}{재현율}}\\)정밀도와 재현율이 균형을 이룰수록 F값은 커진다.혼동행렬            –      –      예측      결과              –      –      양성(스팸)      음성(스팸 아님)              실제      양성(스팸)      참 양성(TP)      거짓 음성(FN)              결과      음성(스팸 아님)      거짓 양성(FP)      참 음성(TN)        \\(정밀도 = \\frac{TP}{TP + FP}\\)\\(재현율 = \\frac{TP}{TP + FN}\\)  다중 클래스 분류 평균 구하기  마이크로 평균          EX) 3클래스 분류 일 때 :  \\(정밀도_{마이크로평균} = \\frac{TP_{1} + TP_{2} + TP_{3}}{TP_{1} + TP_{2} + TP_{3} + FP_{1} + FP_{2} + FP_{3}}\\)        매크로 평균    \\[정밀도_{매크로 평균} = \\frac{정밀도_{1} + 정밀도_{2} + 정밀도_{3}}{3}\\]      ROC, AUC  ROC : 참 양성률(TPR)을 세로축, 거짓 양성률(FPR)을 가록축에 그린 것\\(TPR = \\frac{TP}{TP + FN}\\)\\(FPR = \\frac{FP}{FP + TN}\\)참 양성률은 높을수록, 거짓 양성률은 낮을수록 좋다AUC : ROC곡선 그래프 아래에 넓이를 더한 값회귀 평가평균제곱근오차(RMSE)\\[RMSE = \\sqrt{\\frac{ \\sum_{i}{(예측값_{i}-실제값_{i}})^2}{N} }\\]결정계수  \\(R^2 = 1 - \\frac{\\sum_{i}{(예측값_{i}-실젯값_{i})^2}}{\\sum_{i}{(예측값_{i}-실젯값의 평균_{i})^2}}\\)결정계수가 1에 가까울수록 성능이 좋은 것이며 0에 가까울수록 성능이 좋지 않은 것이다.",
      "url": "/2024/11/11/MachineLearningAtWork-03.html"
    },
  
    {
      "title": "Machine Learning At Work - 02",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 02 - 2024년11월10일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 02 - 2024년11월10일  tag : Machine_Learning_At_Work|Book|머신러닝으로 할 수 있는 일머신러닝 알고리즘 선택 방법  어떤 알고리즘을 사용할지 참고 사이트분류  알고리즘 종류          퍼셉트론 : 입력 벡터와 학습한 가중치 벡터를 곱한 값을 더해서 그 값이 0 이상이면 1, 아니면 클래스 2로 분류하는 알고리즘                  온라인 학습 방식으로 취한다.[온라인 학습 : 데이터를 하나씩 입력해 최적화하는 방식 vs 배치 학습 : 데이터를 전부 입력해 최적화하는 방식]예측 성능은 평이하지만 학습 속도가 빠르다.과적합되기 쉽다.선형 분리 가능한 문제만 해결할 수 있다.[선형 분리 가능 : 데이터를 둘로 나누기 좋은 경우]                    로지스틱 회귀                  출력과는 별도로 해당 출력의 클래스에 소속할 확률값을 반환온라인 학습, 배치 학습 모두 가능예측 성능은 뛰어나지 않으나 학습 및 추론 속도가 빠르다과적합을 방지하기 위한 정규화항이 추가되어 있다.                    서포트 벡터 머신                  마진 최대화를 이용해 매끄러운 초평면을 학습할 수 있다.커널을 사용해 비선형 데이터를 분리할 수 있다.선형 커널을 이용해 차원 수가 높은 희소 데이터도 학습할 수 있다.온라인 학습과 배치 학습을 모두 가능하다.                    신경망                  비선형 데이터를 분리할 수 있다.학습에 시간이 걸린다.파라미터 수가 많아 과적합되기 쉽다.가중치 초깃값이 존재하며 국소 최적해에 빠지기 쉽다.                    k-최근접 이웃 알고리즘                  데이터를 하나씩 순서대로 학습한다.기본적으로 모든 데이터와의 거리 계산을 해야 해 예측에 시간이 걸린다.k의 수에 따라 편차가 있으나 예측 성능은 좋은 편이다.                    결정 트리                  학습한 모델을 사람이 읽고 해석하기 쉽다.정규화할 필요가 없다.과적합되기 쉽다.비선형 분리 가능하지만 선형 분리 가능한 문제는 잘 풀지 못한다.예측 성능은 평범하고 배치 학습만 가능하다.                    랜덤 포레스트 : 이용할 샘플을 무작위로 선택[부트스트랩 샘플링 알고리즘]하고 이용할 특징량을 무작위로 선택해 여러 트리를 만든다.      경사 부스팅 결정트리(GBDT) : 샘플링한 데이터를 이용해 순차적으로 얕은 트리를 학습한느 경사 부스팅을 사용한다.        XGBOOST or LightGBM 사용대규모 데이터에 사용가능XGBOOST : 확률적 최적화를 수행해 대규모 데이터도 빠르게 처리할 수 있다.LightGBM : 처리 속도가 빠르고, 하이퍼 파라미터 튜닝을 위해 OSS인 Optuna의 확장 기능 LightGBM Tuner를 사용한다.Optuna회귀  알고리즘 종류          선형 회귀[직선], 다항식 회귀[곡선]      라쏘[학습한 가중치의 제곱을 L1정규화], 릿지[L2정규화], 일래스틱넷[둘 다]      회귀 트리[결정 트리 기반의 회귀로 비선형 데이터를 근사할 수 있다.]      SVR[SVM 기반의 회귀로 비선형 데이터를 근사할 수 있다.]      클러스터링과 차원 축소  클러스터링          비지도 학습의 한 가지 방법주로 데이터 경향을 확인하는 목적으로 사용EX) 계층적 클러스터링, k-평균        차원 축소          고차원 데이터의 정보를 가능한 유지하면서 저차원 데이터로 변환EX) 주성분 분석(PCA, principal component analysis), t-SNE[시각화에 많이 사용되며 PCA보다 관계성을 이해하기 쉽도록 시각화할 수 있다.]      기타  추천  이상 탐지  고빈도 패턴 마이닝  강화 학습",
      "url": "/2024/11/10/MachineLearningAtWork-02.html"
    },
  
    {
      "title": "Machine Learning At Work - 01",
      "tags": "Machine_Learning_At_Work, Book",
      "desc": "Machine Learning At Work - 01 - 2024년11월09일  tag : Machine_Learning_At_Work|Book|",
      "content": "Machine Learning At Work - 01 - 2024년11월09일  tag : Machine_Learning_At_Work|Book|머신러닝 실무 노하우학습 단계지도학습(supervised learning)  이미 알고있는 데이터에 알고리즘을 적용해서 입력 데이터와 출력 데이터 사이의 관계를 찾아내고 그 결과(모델)를 횔용해 새로운 데이터의 예측을 수행하는 프로그램을 구현  단계          학습 단계 : 기존 데이터의 입력과 출력의 관계를 찾아내는 단계      예측 단계 : 미지의 입력 데이터에서 모델을 사용해 예측한 출력을 얻는 단계      비지도 학습(unsupervised learning)  입력 데이터에서 데이터 구조를 얻는 학습강화 학습(reinforcement learning)  바둑이나 장기와 같은 게임에서 어떤 전략을 취할 것인지 전략을 얻는 학습머신러닝 프로젝트 과정과정  비즈니스 문제를 머신러닝 문제로 정의한다.  논문을 중심으로 유사한 문제들을 조사한다.  머신러닝을 사용하지 않는 방법은 없는지 검토한다.  시스템 설계를 고려한다.  특징량, 훈련 데이터와 로그를 설계한다.  실제 데이터를 수집하고 전처리한다.  탐색적 데이터 분석과 알고리즘을 선정한다.  실제 데이터를 수집하고 전처리한다.  시스템에 통합한다.  예측 정확도, 비즈니스 지표를 모니터링한다.머신러닝으로 해결한 문제 사례를 찾으려면 다음 세 가지를 확인  어떤 알고리즘을 사용했는가?  어떤 데이터를 특징량으로 사용했는가?  머신러닝 부분을 어떻게 통합했는가?어떤 문제에 머신러닝을 적용하면 좋을까?  대량의 데이터에 대해 빠르게 안정된 판단을 내려야 한다.  예측 결과에는 일정 수준의 오류를 용인한다.운용 시스템에서의 머신러닝 문제점 및 대처 방법문제점  확률적인 처리 때문에 자동 테스트하기 어렵다.  오래 운영하면 사용자 경향 변화 때문에 입력 경향도 달라진다.  처리 파이프라인이 복잡해진다.  데이터 의존관계가 복잡해진다.  실험 코드 또는 파라미터가 포함되기 쉽다.  개발 및 운영 시스템 간의 언어/프레임워크가 제각각이기 쉽다.대처 방법  황금 기준을 이용해 직접 예측 성능을 모니터링  예측 모델을 모듈화해서 알고리즘에 대해 A/B TEST 진행  모델 버전 관리를 통해 언제든 원하는 시점으로 돌아갈 수 있도록 한다.  데이터 처리 파이프라인을 저장한다.  개발 및 운영 시스템의 언어/프레임워크를 통일한다.머신러닝 시스템을 성공적으로 운영하기 위한 조건  제품에 관한 도메인 지식을 가진 사람  통계나 머신러닝을 잘 아는 사람  데이터 분석 인프라를 만드는 엔지니어링 역량을 가진 사람  실패 리스크를 책임지는 책임자[머신러닝이 실패 확률이 높은 투자라는 점을 인식하면서도 머신러닝을 사용해야만 만들어낼 수 있는 가치를 믿고 지지하는 존재]",
      "url": "/2024/11/09/MachineLearningAtWork-01.html"
    },
  
    {
      "title": "GROWTH HACKING 09",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 09 - 2024년11월08일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 09 - 2024년11월08일  tag : Growth_Hacking|Book|그로스 해킹 사례 : 닭과 달걀 문제를 피하는 방법  ‘닭과 달걀의 문제’공급자가 많아야 수요자가 생길텐데, 공급자가 생기기 위해서는 플랫폼 내에 많은 수요자가 존재해야 한다.  사업 계획 검증에 사용할 수 있는 5가지 질문          문제 : 당신이 풀기를 원하는 문제는 ?      해결법 : 당신이 가진 그 문제의 해결 방법은 ?      수익 모델 : 어떻게 돈을 버는지      전문성 : 당신이 이 일을 가장 잘할 수 있는 사람인지      시기 적절성 : 왜 지금이 이 일을 시작하기에 가장 좋은 시기인지      레진코닉스  개인적 역량 [ 블로그를 미리 운영하여 잠재 사용자들을 이미 확보한 상태 -&gt; 가지고 시작했던 ‘자산’]  지금 내가 만들 수 있는 것들을 ‘아주 작게’라도 먼저 만든 것          ‘유료 웹툰 서비스’라는 아이디어만을 가지고 작가들을 설득        런칭 방식 [ 정식으로 플랫폼이 준비되기 전에 티저 페이지를 준비했다. ]프렌트립  프렌트립의 티저 서비스  프렌트립 공식 페이스북 페이지와 그룹을 개설한 후, 창업자들이 자신들의 페이스북 친구들을 초대한다.  프렌트립에서 장차 제공하려고 생각하는 상품을 기획한 후 ‘페이스북 이벤트’를 생성한다.  ‘페이스북 이벤트’에서 ‘참석’버튼을 누르고 계좌 이체를 하면, 창업자들이 수동으로 확인해서 명단을 확정한다.로켓펀치  티저 페이지 [ 가설 세우기 : 사람들은 스타트업 정보만을 모아서 보고 싶을 것 ]  발로 뛰는 것[ 스타트업 정보를 수동으로 수집 ]",
      "url": "/2024/11/08/Growth_Hacking_09.html"
    },
  
    {
      "title": "GROWTH HACKING 08",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 08 - 2024년11월07일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 08 - 2024년11월07일  tag : Growth_Hacking|Book|그로스 해킹 도구데이터 수집과 처리, 분석  Google Analytics          웹 외에도 앱 데이터 분석을 지원, AARRR분석, 코호트 분석 수행 가능others : Mixpanel, KISSmetrics        Flurry by Yahoo!          모바일 앱에 특화된 데이터 분석 도구        Segment제품 오류 분석 자동화 도구  Fabric          앱에 간단한 코드를 삽입하는 것 만으로 강제 종료, 기능 오류에 대해서 세세한 분석을 제공받을 수 있다.        Sentry &amp; Rollbar          웹 기반 제품의 오류를 추적할 수 있는 도구      벤치마크 데이터 얻기  SimilarWeb          각 나라의 인터넷 서비스 제공자에서 제공받은 데이터등을 활용하여 각 웹사이트나 앱의 사용량, 튕김 지표, 주요 유입 경로 등을 보여준다.        Ghostery          웹 사이트의 과도한 사용자 추적을 막기 위한 ‘추적 차단 플러그인’으로 만들어졌다.        Website Grader          무료 웹사이트 분석 도구확인하고자 하는 URL을 입력하면 해당 웹사이트의 성능, 모바일 대응 수준, SEO 대응 수준, 보안 수준을 알 수 있다.      ",
      "url": "/2024/11/07/Growth_Hacking_08.html"
    },
  
    {
      "title": "GROWTH HACKING 07",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 07 - 2024년11월06일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 07 - 2024년11월06일  tag : Growth_Hacking|Book|유로 마케팅  Growth Hacking 관점에서 유료 마케팅은 콘텐츠나 입소문 모델을 활용한 유기적 성장을 돕는 수단이 되어야 한다.그로스 해킹 측면의 유료 마케팅 실행 전략  광고 채널[오프라인과 온라인, 데스크탑과 모바일], 광고 타겟[광고 타겟 지역]  메시지[광고 형태, 광고 메시지]  측정[고객 평생 가치]  광고 채널과 광고 타겟  이후에 나오는 메시지나 측정 모두 채널과 타겟에 종속적일 수밖에 없기 때문에사실상 유료 마케팅에서 가장 중요한 의사 결정이 발생하는 부분  타켓팅 기준 : Funnel 관점에서 타겟팅이 수준에 대한 기준볼륨 기준 : 한 번에 도달할 수 있는 광고 대상자 수에 따른 기준  메시지  메시지에서 중요한 것은 ‘명확한 전달성’원칙 : 명확한 가치를 제시할 것, 명확한 행동을 요구할 것, Funnel 일관성  측정\\[LTV = \\sum_{X=1}^{n}\\frac{ARPU - Costs_x}{(1+WACC)^x} - SAC\\]  CAC(사용자 획득 비용)LTV(사용자 평생 가치)ARPU(인당 평균 매출)Costs(고객 비용)WACC(자본의 이동 평균 비용)N : 기간유로 마케팅의 3가지 원칙  유료 마케팅은 유기적 성장이 시작된 후에 진행해야 한다.  유료 마케팅을 진행할 때는 소수의 채널부터 적은 예산으로 시작해서 천천히 확장해야 한다.  우리의 현재 상태와 제품의 특정에 맞는 채널은 무엇인지 생각하고 채널을 선택해야 한다.",
      "url": "/2024/11/06/Growth_Hacking_07.html"
    },
  
    {
      "title": "GROWTH HACKING 06",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 06 - 2024년11월05일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 06 - 2024년11월05일  tag : Growth_Hacking|Book|네트워크 효과와 입소문입소문 마케팅(Viral Marketing)제대로 이해하기  입소문 마케팅 : 소비자들이 자발적으로 메시지를 전달하게 하여 상품에 대한 긍정적인 입소문을 내게 하는 마케팅 기법하지만 기술적인 내용들만 언급하고 있어 부정적인 이미지가 생김Ex. 블로그 순위를 급상승시키는 OO가지 방법아날로그 시대의 입소문 마케팅 사례 - “Send-a-Dime” 체인 레터  사용법          우편물을 받으면, 자기 이름을 제외한 나머지 다섯 명에서 각각 1 Dime(10센트 = 0.1달러)을 보낸다.      가장 위에 있는 이름을 지우고, 제일 아래에 자신의 이름을 추가하여 5명의 친구에게 메일을 보낸다.      우편물 연결 고리(Chain)가 끝나지 않는다면 메일을 보낸 사람은 15,625개의 Dime을 받게 되므로 총 1,562.50달러를 받게 된다.        제안하는 가치(Value Proposition) : 목표가 완수되면 1526.5달러를 받을 수 있음행동 유도(Call to Action) : 3일 안에 5명의 사람들에게 우편물 전송확장 인수(Branching Factor) : 5전환율(Conversion) : X입소문 인수(Viral Factor) : 5 * X디지털 시대의 입소문 마케팅 이해하기  네트워크 가치 제안 : 사람들이 다른 사람들에게 우리 제품을 전달해야 하는 가치를 주어야 한다.  채널 전달율 : 이메일이나 문자 메시지 등은 스팸 필터나 기업 내부망 정책 등으로 100% 전달되지 않는다.  전달 빈도 : 자주 접속하는 제품일수록 바이럴 마케팅이 더 효율적이다.  채널 전환율 : 요즘 사람들은 자극에 둔감해졌고 전환율도 초창기에 비해서 크게 떨어졌다.  전달 속도 : 콘텐츠 소비 시간이 길어지면 길어질수록 입소문 단계는 ‘정체’되었다고 생각할 수 있다. 당연히 소비가 빠른 콘텐츠가 입소문도 빠르다.  최종 전환율(=활성화 비율)  복합성입소문 마케팅 실행 전략과 순환 고리  선형적인 사용자 획득 채널을 순환형 바이럴 채널로 전환  도구로 시작해서 네트워크로 확장  이미 존재하는 네트워크를 좀 더 쉽게 활용할 수 있는 방법 제공",
      "url": "/2024/11/05/Growth_Hacking_06.html"
    },
  
    {
      "title": "GROWTH HACKING 05",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 05 - 2024년11월04일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 05 - 2024년11월04일  tag : Growth_Hacking|Book|고객 활성화와 재사용  활성화 : ‘고객이 우리 제품’의 가장 핵심적인 기능을 경험하고 그 가치를 인지하는 단계  따라서 ‘활성화’는 단순히 ‘회원 가입’이 아니라 ‘사진 업로드’, ‘게시글 작성’, ‘구매’ 처럼 다양하게 정의될 수 있다.  재사용 : ‘고객이 그들의 삶에서 우리 제품의 가치를 지속적으로 소비하는 단계’DAU, WAU, MAU 다시 생각하기  DAU[Daily Active User] : 일간 활성 사용자  WAU[Weekly Active User] : 월간 활성 사용자  MAU[Monthly Active User] : 월간 활성 사용자  EX. 인스타그램 : MAU 보다는 DAU가 더 중요한 지표  EX. 에어비앤비 : ‘여행’에 관련된 제품 -&gt; 1년에 2~3번 정도 -&gt; DAU 보다는 MAU가 더 중요한 지표  고객 활성화와 재사용의 개선 원칙 - ‘자연스러움’아무리 뛰어난 제품을 만든다고 해도 고객을 생각하지 않고선 그 제품은 성공하기 어렵다.  고객 활성화와 재사용 전략 수립 - 트리거 모델트리거 모델 : 발생 조건 - 행동 - 보상EX. 입 냄새가 난다 -&gt; 치약을 써서 이를 닦는다 -&gt; 입 냄새가 없어진다",
      "url": "/2024/11/04/Growth_Hacking_05.html"
    },
  
    {
      "title": "GROWTH HACKING 04",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 04 - 2024년11월03일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 04 - 2024년11월03일  tag : Growth_Hacking|Book|채널채널 관점에서 바라본 페이스북  페이스북은 자신들의 서비스 내에 다른 회사들이 ‘위젯’ 형태로 직접 자신들의 서비스를 제공할 수 있는 방법을 무료로 공개했다.페이스북 플랫폼 초창기에 참여한 기업들이 어마어마한 사용자들을 모을 수 있었다.그 이유는?  충분히 큰 플랫폼  1:n으로 확산될 수 있는 플랫폼 구조와 프로그래밍 가능한 API  새로운 채널 : 전에 이런 종류의 서비스들을 사용해 본적이 없었기 때문에 높은 응답률을 보이는 현상채널의 특성시간에 따른 쇠퇴(Decay)  전형적인 채널의 쇠퇴 과정          채널의 시작      초기 사용자들의 긍정적인 반응      좋은 채널이라는 소문이 나면서, 그 채널을 통해 정보를 전달하는 사람들이 많아지기 시작      사용자들이 스팸으로 인지하면서 불만이 쌓이거나 반응이 무뎌짐      채널 쇠퇴        채이 쇠퇴할 때 나타나는 현상 : ‘채널의 쇠퇴의 5가지 현상들’          모방(Copycats) : 채널에 다른 사업자들을 모방하는 다수의 사업자들이 등장      이탈(Churn) : 채널에서 사용자들이 이탈하기 시작      퇴출(Castoff) : 채널의 관리자가 스팸 등의 이슈로 사업자들을 퇴출하기 시작      정복(Conquest) : 채널이 포화되어 초기 사업자들도 이탈하기 시작      비용(Costs) : 채널을 사용하는 비용이 증가        어떻게 새로운 채널을 발견할 것인가?‘고객 여정 지도(Customer Journey Map)’  고객 여정 지도 : 고객이 서비스를 경험하게 되는 과정을 정의하고, 그 과정에서 생기는 고객 체험을 시각화하기 위해 사용되는 방법",
      "url": "/2024/11/03/Growth_Hacking_04.html"
    },
  
    {
      "title": "GROWTH HACKING 03",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 03 - 2024년11월02일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 03 - 2024년11월02일  tag : Growth_Hacking|Book|성장 로드맵죽음의 계곡, 캐즘 그리고 Product-Market Fit  Early Adopter : 제품 런칭 초기에 관심을 가지고 사용하는 사람들하지만 이들의 수는 제한적이며, 무엇보다 금새 또 다른 새로운 어떤 것을 찾아 이동할 가능성이 높다.따라서 제대로 된 성장을 달성하기 위해서는 ‘Early Adopter’가 아닌, 진짜 고객들이 필요하다.  진짜 고객 : Early Majority [전기 다수 사용자] : 좀 더 보수적으로 제품을 받아들이고 새로운 기능을 익히는 것에도 덜 능숙한 집단  초반에 반짝 상승했던 사용량[by Early Adopter]이 떨어지는 것은, ‘제자리를 찾아가는 것’이라고 표현  이런 현상을 ‘죽음의 계곡’ or ‘캐즘[Chasm]’ or ‘Prodict-Market Fit(제품-시장 적합성)이 달성되지 못했다’라고 표현  Chasm[캐즘] : 지각변동 등의 이유로 인해 지층 사이에 큰 틈이 생겨 서로 단절되어 있다는 것을 의미Prodict-Market Fit(제품-시장 적합성) : 우리들이 만든 제품이 시장의 다수 사용자들의 니즈에  사용량이나 매출이 급격하게 상승하는 상황성장 로드맵 작성 1단계 - 성장 기여도 측정하기  프로젝트의 성장 기여도 = 도달률 * 상한선 * 성공률          도달률 : 얼마나 많은 사용자들을 대상으로 하는가상한선 : 얼마나 개선할 수 있나?성공률 : 성공할 가능성이 얼마나 있을까?      성장 로드맵 작성 2단계 - 프로젝트 총 비용 측정하기  어떤 아이디어가 아무리 성장에 큰 도움이 될 것 같더라도, 현재 우리 팀 규모에서 진행하기 어렵다면 그 프로젝트는 당분간 보류할 수 밖에 없다.",
      "url": "/2024/11/02/Growth_Hacking_03.html"
    },
  
    {
      "title": "GROWTH HACKING 02",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 02 - 2024년11월01일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 02 - 2024년11월01일  tag : Growth_Hacking|Book|성장 모델  ‘시장을 이기는 사업은 없다’ == ‘고객의 삶을 이기는 제품은 없다’  성장 모델(Growth Model) : ‘다름’을 이해하고, 우리 사업과 서비스에 맞는 성장 전략을 선택하는 것3가지 서비스 순환 고리와 성장 승수  직선형 관점 : 서비스 접근 - 재방문 - 구매 - 종료  순환형 관점 : 서비스 접근 - 재방문 - 구매 - 추천 - 새로운 고객의 서비스 접근 - 재방문 - 구매 - 추천 -&gt; 새로운 고객의 서비스 접근 -&gt; (순환 계속)  콘텐츠형 순환 고리 : 사용자가 콘텐츠 생산에 참여하는 서비스들이 여기에 해당  유로 광고형 순환 고리 : 유료 서비스를 판매하는 사업들이 여기에 해당  바이럴형 순환 고리 : 콘텐츠형과 유료 광고형 순환 고리와 함께 동작하는 경우  1/(1-r) : 성장 승수 [ r : 새 사용자로 전환되는 비율 , EX) 1000 -&gt; 600 : r = 0.6 ]최종 사용자 : a * 1/(1-r) [ a : 최초에 있던 사용자의 수 ]급속 성장을 달섬하는 법  우리가 원하는 것은 “J 커브”  달성하기 위해서는 성장 승수의 r값이 1이 넘어가면 된다.시장의 크기는 제한되어 있으므로 결국 시장의 크기만큼 사용자를 획득할 수 있다.",
      "url": "/2024/11/01/Growth_Hacking_02.html"
    },
  
    {
      "title": "데이터 과학을 위한 통계 - 비지도 학습",
      "tags": "Practical_Statistics_for_Data_Scientists, Python, Book",
      "desc": "데이터 과학을 위한 통계 - 비지도 학습 - 2024년10월30일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|",
      "content": "데이터 과학을 위한 통계 - 비지도 학습 - 2024년10월30일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|  비지도 학습 : 레이블이 달린 데이터를 이용해 모델을 학습하는 과정 없이 데이터로부터 의미를 이끌어내는 통계적 기법주성분분석  주성분분석(PCA, principal components analysis) : 수치형 변수가 어떤 식으로 공변하는지 알아내는 기법  주성분 : 예측변수들의 선형결합  부하(loading) : 예측변수들을 성분으로 변형할 때 사용되는 가중치  스크리그래프(screeplot) : 성분들의 변동을 표시한 그림, 설명된 분산 혹은 설명된 분산의 비율을 이용하여 성분들의 상대적인 중요도를 보여줌  explained_variance_ratio_ : 각 주성분이 데이터의 분산을 얼마나 잘 설명하는지 나타냄K-평균 클러스터링  cluster(군집) : 서로 유사한 레코드들의 집합  k : 클러스터의 개수  elbow method : 언제 클러스터 세트가 데이터의 분산의 ‘대부분’을 설명하는지를 알려주는 기법# Elbow Method를 위한 SSD 값 저장용 리스트ssd = []# 여러 K 값에 대해 K-Means 모델을 학습시키고 SSD 값 계산K_range = range(1, 11)for k in K_range:    kmeans = KMeans(n_clusters=k, random_state=42)    kmeans.fit(X)    ssd.append(kmeans.inertia_)  # inertia_는 클러스터 내 분산의 합(SSE 또는 SSD)  최적의 k 값은 3Sum of Squared Distances(SSD) : 클러스터링에서 각 데이터 포인트와 해당 클러스터의 중심 간 거리의 제곱 합을 의미SSD 값이 급격히 감소하는 지점을 찾아 그 지점을 최적의 K 값으로 선택계층적 클러스터링  hierarchical clustering : k-평균 대신 사용하는 클러스터링 방법으로 k-평균과는 다른 결과를 보여준다.특이점이나 비정상적인 그룹이나 레코드를 발견하는 데 더 민감하다.직관적인 시각화가 가능  dendrogram : 레코드들, 그리고 레코드들이 속한 계층적 클러스터를 시작적으로 표현  distance : 한 레코드가 다른 레코드들과 얼마나 가까운지를 보여주는 측정 지표  비유사도(dissimilarity) : 한 클러스터가 다른 클러스터들과 얼마나 가까운지를 보여주는 측정 지표  거리 : 높으면 다른 클러스터와 멀리 떨어져 있다고 판단(EX] 8번 샘플)클러스터 그룹 : Distance=0.6 정도에서 덴드로그램을 잘랐을 때 클러스터를 3개로 나눌 수 있다.1번 그룹 : 82번 그룹 : 5, 2, 63번 그룹 : 9, 1, 3, 10, 4, 7  비유사도 행렬서로 유사한 경우 0에 가까운 값이 나옴[큰 값을 가질수록 차이가 크다]모델 기반 클러스터링  이 기법은 통계 이론에 기초하고 있으며 클러스터의 성질과 수를 결정하는 더 엄격한 방법을 제공  다변량정규분포  정규혼합  클러스터 개수 결정하기 [ BIC ]# BIC를 저장할 리스트bic_values = []n_components_range = range(1, 11)  # 클러스터 개수 1에서 10까지for n_components in n_components_range:    gmm = GaussianMixture(n_components=n_components)    gmm.fit(X)    bic_values.append(gmm.bic(X))스케일링과 범주형 변수  스케일링  정규화 : 원래 변수 값에서 평균을 뺀 후에 표준편차로 나누는 방법  고위거리(Gower’s distance) : 수치형과 범주형 데이터가 섞여 있는 경우에 모든 변수가 0~1 사이로 오도록 하는 스케일링 방법",
      "url": "/2024/10/30/Practical_Statistics_for_Data_Scientists_07.html"
    },
  
    {
      "title": "GROWTH HACKING 01",
      "tags": "Growth_Hacking, Book",
      "desc": "GROWTH HACKING 01 - 2024년10월29일  tag : Growth_Hacking|Book|",
      "content": "GROWTH HACKING 01 - 2024년10월29일  tag : Growth_Hacking|Book|그로스 해킹 시작하기그로스 해킹이란?  제품 판매와 노출을 목적으로 기술 벤처 기업에 의해 개발 된 창의성과 분석적 사고, 소셜 분석을 활용하는 마케팅 기법그로스 해킹의 시작점 - ‘고객을 이해하는 것’  그로스 해킹은 ‘고객을 이해하는 것’에서 시작한다.그로스 해킹에 대한 개념을 잘못 이해하여, 단순히 ‘마케팅적 가술’로 해석하여, 새 사용자를 획득하는 것에만 집중하는 경향을 피하자!고객을 이해하기 위한 이론1 - AARRR or Funnel Analysis  AARRR          Acquisition(인지) : 사용자가 서비스를 최초로 인지하여      Activation(활성화) : 첫 사용을 하고      Retention(재사용) : 나중에 또 사용을 하러 와서      Revenue(매출 발생) : 매출도 발생시키고      Referral(추천) : 주변 사람들에게 써보라고 추천하는 과정        AARRR 개념을 활용할 때 주의해야 할 점은 AARRR 개념에 우리 제품을 ‘끼워 맞추는 것’이다.AARRR을 제대로 활용하기 위해서는 우리가 만드는 제품의 가치에 따라 AARRR의 각 단계별 의미가 달라질 수 있고, 단계들의 순서도 바뀔수 있다는 것을 이해해야 한다.  EX          AARRR 단계 정의 : 페이스북 가치에 맞는 AARRR 단꼐를 정의      문제 인지 : 페이스북에 가입(=활성화)하는 비율에 비해서 재방문(=재사용)하는 비율이 낮은데, 가입 후 지속적으로 사용하는 사람들과 한 번 가입 후엔 쓰지 않는 사람들 사이에는 어떤 차이점이 있을까?      데이터 분석 : 두 집단 비교 분석      분석 결론 : 10일 이내에 7명의 친구가 생기면 페이스북을 재사용하는 비율이 급격히 올라감      적용 : 10일 이내에 7명의 친구를 만들어 주니까 가입 후 지속적으로 페이스북을 재사용하는 비율이 증가      고호트 분석(Cohort Analysis)  코호트 : 통계에서 같은 인자를 공유하는 집단‘특정 기간 내에 가입한 사용자’등을 하나의 집단으로 묶어서 시간에 따른 변화를 측정할 수 있다.  시간을 코호트로 잡아 분석하는 올바른 방법은 우리가 제공하는 가치에 맞는 적절한 시간 단위를 선택해야 한다.(보통 ‘주’단위 사용)  EX          코호트 설정 : 10일 동안 7명의 친구를 만들어주는 가장 좋은 방법을 확인하기 위해 두 집단을 정의한다. 새 사용자의 전화번호부에 있는 친구 정보를 가져온 후 1) 새 사용자가 전화번호부에 있는 친구들에게 친구 신청을 하게 하는 집단 2) 전화번호부에 있는 친구들에게 당신의 친구가 새 사용자로 가입했으니 친구 신청을 하라고 알려주는 집단으로 구분      데이터 누적 : 충분한 기간 동안 두 집단의 데이터를 기록한다.      데이터 비교 : 누적된 데이터를 분석하여 어떤 방법이 더 효율적으로 10일 동안 7명의 친구를 만들어 주고, 최종적으로 더 많은 사용자들을 페이스북에 남게 하는지 기록      결론 적용 : 더 좋다고 확인된 방법을 전체 사용자들을 대상으로 적용      ",
      "url": "/2024/10/29/Growth_Hacking_01.html"
    },
  
    {
      "title": "A/B TEST",
      "tags": "Blog, AB_TEST",
      "desc": "A/B TEST - 2024년10월28일  tag : Blog|AB_TEST|",
      "content": "A/B TEST - 2024년10월28일  tag : Blog|AB_TEST|정의  A/B TEST : 두 가지 이상의 버전을 비교하여 어떤 것이 더 나은 성과를 내는지 판단하는 실험 방법A/B TEST 하는 이유  상관관계로부터 인과관계[ 정확히 말하면 인과관계일 가능성이 높은 것 ]를 찾아내기 위함이다.A/B TEST 절차  목표 설정 : 테스트의 목적과 개선하고자 하는 지표를 명확히 정의  가설 수립 : 어떤 변경이 목표 달성에 도움이 될 것인지 가설을 세우기  테스트 설계 : A안(기존안)과 B안(변경안)을 구체화하기 -&gt; 테스트 기간, 샘플 크기, 분석 방법 등을 결정  테스트 실행 : 사용자를 무작위로 A그룹과 B그룹으로 나누고, 각 그룹에 해당 버전을 노출시키기  데이터 수집 : 설정한 기간 동안 각 그룹의 성과 데이터를 수집  결과 분석 : 통계적 유의성을 검증하여 A안과 B안의 성과 차이를 분석  결론 도출 및 적용 : 분석 결과를 바탕으로 더 나은 버전을 선택 -&gt; 선택한 버전을 전체 사용자에게 적용  지속적 개선 : 테스트 결과를 바탕으로 새로운 가설을 수립하고 추가 테스트를 진행주의해야 할 점  Correlation does not imply causation : 상관관계는 인과관계를 뜻하지 않는다.      - A/B TEST를 통해 인과관계를 알아내려면 두 집단을 나눠야 한다 =&gt; 임의적 할당(random assignment)  &gt; EX ] 남성 VS 여성, 이번주 사용자 VS 다음주 사용자  - A/B TEST를 통해 찾아낸 결과가 범용성을 지니려면 애초에 실험에 참가한 집단이 모집단을 대표할 수 있어야 한다.        테스트를 많이/자주하면 단기적으로 손해가 발생할 수 있다.      &gt; A/B 테스트 결과는 해당 테스트에만 국한되며, 새로운 테스트마다 새로운 기준선이 필요    &gt; 이는 끊임없는 테스트 사이클로 이어질 수 있으며, 장기적인 전략 수립을 방해할 수 있다.        A/B 테스팅의 결과는 계절 변화나 취향 변화 등 시간의 흐름에 따라 바뀔 수 있다.      &gt; EX ] 작년 겨울에 진행한 테스트는 언제까지 유효할까?        사용자 행동의 완전한 이해 부족      &gt; A/B 테스트는 어떤 버전이 더 나은 성과를 보이는지는 알려주지만, 왜 그런지에 대한 깊이 있는 이해를 제공하지 않는다.        참고 사이트",
      "url": "/2024/10/28/AB_test.html"
    },
  
    {
      "title": "데이터 과학을 위한 통계 - 통계적 머신러닝",
      "tags": "Practical_Statistics_for_Data_Scientists, Python, Book",
      "desc": "데이터 과학을 위한 통계 - 통계적 머신러닝 - 2024년10월26일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|",
      "content": "데이터 과학을 위한 통계 - 통계적 머신러닝 - 2024년10월26일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|k-최근접 이웃  K-nearest neighbors(KNN)  과정          특징들이 가장 유사한 k개의 레코드를 찾는다.      분류 : 이 유사한 레코드들 중에 다수가 속한 클래스가 무엇인지 찾은 후에 새로운 레코드를 그 클래스에 할당한다.      예측(KNN 회귀) : 유사한 레코드들의 평균을 찾아서 새로운 레코드에 대한 예측값으로 사용한다.      트리 모델  회귀 및 분석 트리(classification and regression tree, CART), Decision treeRandom Forest, 부스팅 트리  재귀 분할 : 마지막 분할 영역에 해당하는 출력이 최대한 비슷한 결과를 보이도록 데이터를 반복적으로 분할하는 것  분할값 : 분할값을 기준으로 예측변수를 그 값보다 작은 영역과 큰 영역으로 나눈다.  loss : 분류하는 과정에서 발생하는 오분류의 수, 손실이 클수록 불순도가 높다고 할 수 있다.  불순도(impurity) : 데이터를 분할한 집합에서 서로 다른 클래스의 데이터가 얼마나 섞여 있는지를 나타냄  가지치기(pruning) : 학습이 끝난 트리 모델에서 오버피팅을 줄이기 위해 가지들을 하나씩 잘라내는 과정배깅과 랜덤 포레스트  앙상블(ensemble) : 여러 모델의 집합을 이용해서 하나의 예측을 이끌어내는 방식  배깅(bagging) : 데이터를 부트스트래핑해서 여러 모델을 만드는 일반적인 방법  랜덤 포레스트 : 의사 결정 트리 모델에 기반을 둔 배깅 추정 모델  변수 중요도                   앙상블      배깅              정의      여러 모델을 결합하여 성능 향상      중복된 데이터 샘플링과 모델 결합으로 성능 향상              주요 목적      다양한 방법(배깅, 부스팅 등)결합      분산 감소, 과적합 방}              결합 방식      배깅, 부스팅, 스태킹 등      보팅 or 평균화              대표 알고리즘      랜덤 포레스트, 그래디언트 부스팅      랜덤 포레스트      부스팅  boosting : 연속된 라운드마다 잔차가 큰 레코드들에 가중치를 높여 일련의 모델들을 생성하는 일반 기법  AdaBoost : 잔차에 따라 데이터의 가중치를 조절하는 부스팅의 초기 버전  gradient boosting : 비용 함수를 최소화 하는 방향으로 부스팅을 활용하는 좀 더 일반적인 형태  확률적 그래디언트 부스팅(stochastic gradient boosting) : 각 라운드마다 레코드와 열을 재표뵨추출하는 것을 포함하는 부스팅의 가장 일반적인 형태  hyperparameter : 알고리즘을 피팅하기 전에 미리 세팅해야 하는 파라미터",
      "url": "/2024/10/26/Practical_Statistics_for_Data_Scientists_06.html"
    },
  
    {
      "title": "데이터 과학을 위한 통계 - 분류",
      "tags": "Practical_Statistics_for_Data_Scientists, Python, Book",
      "desc": "데이터 과학을 위한 통계 - 분류 - 2024년10월25일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|",
      "content": "데이터 과학을 위한 통계 - 분류 - 2024년10월25일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|나이브 베이즈  naive Bayes 알고리즘은 주어진 결과에 대해 예측변숫값을 관찰할 확률을 사용예측변수가 주어졌을 때, 결과 Y = i를 관찰할 확률, 즉 정말 관심 있는 것을 추정  조건부확률 : 어떤 사건(Y= i)이 주어졌을 때, 사건(X = i)을 관찰할 확률  사후확률 : 예측 정보를 통합한 후 결과의 확률(이와 달리, 사전확률에서는 예측변수에 대한 정보를 고려하지 않는다.)# Iris 데이터셋 로드iris = load_iris()X = iris.data  # 특징 데이터y = iris.target  # 레이블 데이터# 학습 데이터와 테스트 데이터로 분리X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)# Gaussian Naive Bayes 모델 생성 및 학습model = GaussianNB()model.fit(X_train, y_train)# 테스트 데이터에 대해 예측y_pred = model.predict(X_test)# 정확도 및 분류 성능 평가accuracy = accuracy_score(y_test, y_pred) # Accuracy: 97.78%판별분석  공분산 : 하나의 변수가 다른 변수와 함께 변화하는 정도(유사한 크기와 방향)를 측정하는 지표  판별함수 : 예측변수에 적용했을 때, 클래스 구분을 최대화하는 함수  판별 가중치 : 판별함수를 적용하여 얻은 점수를 말하며, 어떤 클래스에 속할 확률을 추정하는 데 사용된다.import numpy as np# 두 변수 데이터 (예시 데이터)x = [2.1, 2.5, 3.6, 3.9]y = [8, 10, 12, 14]# 공분산 계산cov_matrix = np.cov(x, y)# 공분산 값cov_xy = cov_matrix[0, 1]로지스틱 회귀  로지스틱 회귀는 결과가 이진형 변수이다.  logit : (0 ~ 1이 아니라) \\(\\pm \\infty\\)의 범위에서 어떤 클래스에 속할 확률을 결정하는 함수  odds : ‘실패(0)’에 대한 ‘성공(1)’의 비율  log odds : 변환 모델(선형)의 응답변수, 이 값을 통해 확률을 구한다.# 데이터셋 로드 (유방암 진단 데이터셋)data = load_breast_cancer()X = data.data  # 특징 데이터y = data.target  # 레이블 (이진 클래스: 0과 1)# 학습 데이터와 테스트 데이터로 분할X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 로지스틱 회귀 모델 생성 및 학습model = LogisticRegression(max_iter=10000)  # 반복 횟수 설정model.fit(X_train, y_train)# 테스트 데이터에 대한 확률 예측y_proba = model.predict_proba(X_test)  # 클래스 1에 속할 확률# 클래스 1의 확률 가져오기 (양성 클래스)probs = y_proba[:, 1]# odds 및 log odds 계산odds = probs / (1 - probs)log_odds = np.log(odds)  출력First 10 samples' odds:[7.22592689e+00 3.06485905e-08 1.60786038e-03 8.17214888e+02 7.07486668e+03 2.00329645e-10 6.15220705e-11 1.96425715e-02 6.23237035e+01 1.74551484e+02]First 10 samples' log odds:[  1.97767552 -17.30067916  -6.43285094   6.70590208   8.86430388 -22.33105688 -23.51162513  -3.93005605   4.13234183   5.16221973]분류 모델 평가하기  정확도  혼동행렬(confusion matrix)  민감도 : 1을 정확히 1로 분류한 비울(= recall[재현율])  특이도 : 0을 정확히 0으로 분류한 비율  정밀도 : 1이라고 예측한 것들 중에 1이 맞는 경우의 비율  ROC 곡선 : 민감도와 특이성을 표시한 그림  lift : 모델이 다른 확률 컷오프에 대해 (비교적 드문) 1을 얼마나 더 효과적으로 구분하는지 나타내는 지표# 데이터셋 로드data = load_breast_cancer()X = data.data  # 특징 데이터y = data.target  # 레이블 (이진 클래스: 0과 1)# 학습 데이터와 테스트 데이터로 분할X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# 로지스틱 회귀 모델 생성 및 학습model = LogisticRegression(max_iter=10000)model.fit(X_train, y_train)# 예측 및 확률 예측y_pred = model.predict(X_test)y_proba = model.predict_proba(X_test)[:, 1]  # 클래스 1에 대한 확률# 혼동행렬 생성conf_matrix = confusion_matrix(y_test, y_pred)# 민감도(Recall), 특이도(Specificity), 정밀도(Precision), 정확도(Accuracy) 계산tn, fp, fn, tp = conf_matrix.ravel()sensitivity = tp / (tp + fn)  # 민감도 (TPR)specificity = tn / (tn + fp)  # 특이도 (TNR)precision = precision_score(y_test, y_pred)  # 정밀도accuracy = accuracy_score(y_test, y_pred)  # 정확도# ROC Curve 및 AUC 계산fpr, tpr, thresholds = roc_curve(y_test, y_proba)roc_auc = auc(fpr, tpr)# ROC 곡선 그리기plt.figure(figsize=(10, 6))plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')plt.plot([0, 1], [0, 1], color='gray', linestyle='--')plt.xlim([0.0, 1.0])plt.ylim([0.0, 1.05])plt.xlabel('False Positive Rate (1 - Specificity)')plt.ylabel('True Positive Rate (Sensitivity)')plt.title('ROC Curve')plt.legend(loc=\"lower right\")plt.show()# Lift 곡선 그리기precision, recall, thresholds_pr = precision_recall_curve(y_test, y_proba)lift = precision / (np.sum(y_test) / len(y_test))  # Lift 계산plt.figure(figsize=(10, 6))plt.plot(recall, lift, color='green', label='Lift curve')plt.xlabel('Recall')plt.ylabel('Lift')plt.title('Lift Curve')plt.legend(loc=\"upper right\")plt.show()  출력Confusion Matrix:[[39  4] [ 1 70]]Sensitivity (Recall): 0.99Specificity: 0.91Precision: 0.95Accuracy: 0.96불균형 데이터 다루기  과소표본 : 분류 모델에서 개수가 많은 클래스 데이터 중 일부 소수만을 사용하는 것(= 다운샘플)  과잉표본 : 분류 모델에서 희귀 클래스 데이터를 중복하여, 필요하면 부트스트랩에서 사용하는 것(= 업샘플)  상향 가중치 or 하향 가중치 : 모델에서 회귀(혹은 다수) 클래스에 높은(혹은 낮은) 가중치를 주는 것  데이터 생성 : 부트스트랩과 비슷하게 다시 샘플링한 레코드를 빼고 원래 원본과 살짝 다르게 데이터를 생성하는 것  z-score : 표준화 결과  k : 최근접 이웃 알고리즘에서 이웃들의 개수",
      "url": "/2024/10/25/Practical_Statistics_for_Data_Scientists_05.html"
    },
  
    {
      "title": "데이터 과학을 위한 통계 - 회귀와 예측",
      "tags": "Practical_Statistics_for_Data_Scientists, Python, Book",
      "desc": "데이터 과학을 위한 통계 - 회귀와 예측 - 2024년10월21일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|",
      "content": "데이터 과학을 위한 통계 - 회귀와 예측 - 2024년10월21일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|단순선형회귀  종속변수 : 예측하고자 하는 변수  독립변수 : 응답치를 예측하기 위해 사용되는 변수  절편 : 회귀직선의 절편, 즉 X = 0일 때 예측값  회귀계수 : 회귀직선의 기울기  잔차(residual) : 관측값과 적합값의 차이(= 오차)  최소제곱 : 잔차의 제곱합을 최소화하여 회귀를 피팅하는 방법  \\(y = ax + b\\) [ a : 회귀계수, b : 절편, x : 독립변수, y : 종속변수]# 샘플 데이터 생성X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # 입력 변수y = np.array([1.2, 1.9, 3.1, 3.9, 5.2])  # 실제 출력 값# 선형 회귀 모델 학습model = LinearRegression()model.fit(X, y)# 예측 값 구하기y_pred = model.predict(X)# 잔차 계산 (실제 값 - 예측 값)residuals = y - y_pred다중선형회귀  제곱근평균제곱오차(RMSE) : 회귀 시 평균제곱오차의 제곱근, 회귀모형을 평가하는 데 가장 널리 사용되는 측정 지표  잔차 표준오차(RSE, Root Standard Error) : 평균제곱오차와 동일하지만 자유도에 따라 보정된 값  r-squared : 0에서 1까지 모델에 의해 설명된 분산의 비율(= 결정계수)  t-statistic : 계수의 표준오차로 나눈 예측변수의 계수, 모델에서 변수의 중요도를 비교하는 기준  가중회귀 : 다른 가중치를 가진 레코드들을 회귀하는 방법X = np.random.rand(100, 3)  # 100개의 샘플, 3개의 독립 변수y = 3.5*X[:,0] + 2.1*X[:,1] + 4.7*X[:,2] + np.random.randn(100)  # 종속 변수# 데이터프레임 생성df = pd.DataFrame(X, columns=['feature1', 'feature2', 'feature3'])df['target'] = y# 독립 변수(X)와 종속 변수(y) 분리X = df[['feature1', 'feature2', 'feature3']]y = df['target']# 상수항 추가 (절편 계산을 위해)X = sm.add_constant(X)# 1. 다중 선형 회귀 모델 학습model = sm.OLS(y, X).fit()# 2. 예측 값 구하기y_pred = model.predict(X)# 1. RMSE 계산rmse = np.sqrt(mean_squared_error(y, y_pred))# 2. R-squared 값 계산r_squared = model.rsquared# 3. t-statistic 값 (각 독립 변수의 t-statistic)t_statistics = model.tvaluesprint(model.summary())OLS Regression Results  Dep. Variable:         target        R-squared:             0.796  Model:                   OLS         Adj. R-squared:        0.789  Method:             Least Squares    F-statistic:           124.7  Date:             Wed, 23 Oct 2024   Prob (F-statistic): 5.31e-33  Time:                 01:02:25       Log-Likelihood:      -134.64  No. Observations:         100        AIC:                   277.3  Df Residuals:              96        BIC:                   287.7  Df Model:                   3                                      Covariance Type:      nonrobust                                                coef     std err      t      P&gt;|t|  [0.025    0.975]    const       -0.0498     0.297    -0.168  0.867    -0.639     0.540  feature1     3.0913     0.324     9.544  0.000     2.448     3.734  feature2     2.0800     0.326     6.372  0.000     1.432     2.728  feature3     4.9532     0.344    14.394  0.000     4.270     5.636  Omnibus:        0.268   Durbin-Watson:         2.444  Prob(Omnibus):  0.875   Jarque-Bera (JB):      0.049  Skew:           0.036   Prob(JB):              0.976  Kurtosis:       3.080   Cond. No.               6.19Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  RMSE : 값이 낮을수록 예측이 더 정확하다는 것을 의미R-squared (결정계수) : 회귀 모델이 데이터의 분산을 얼마나 잘 설명하는지를 의미, 값이 1에 가까울수록 모델이 데이터를 잘 설명하는 것t-statistic: 각 독립 변수에 대한 t-통계량이 크면 그 변수가 종속 변수에 미치는 영향이 유의미하다는 것 -&gt; 보통 절대값이 2 이상일 때 유의미하다고 판단회귀를 이용한 예측  예측구간 : 개별 예측값 주위의 불확실한 구간  외삽법 : 모델링에 사용된 데이터 범위를 벗어난 부분까지 모델을 확장하는 것회귀에서의 요인변수  dummy variable : 회귀나 다른 모델에서 요인 데이터를 사용하기 위해 0과 1의 이진변수로 부호화한 변수  one-hot encoding  deviation coding : 기준 수준과는 반대로 전체 평균에 대해 각 수준을 비교하는 부호화 방법회귀방정식 해석  변수 간 상관  다중공선성 : 독립변수들이 완벽하거나 거의 완벽에 가까운 상관성을 갖는다고 할 때, 회귀는 불완정하며 계산이 불가능하다.  교란변수 : 중요한 독립변수지만 회귀방정식에 누락되어 결과를 잘못되게 이끄는 변수회귀진단  표준화잔차 : 잔차를 표준오차로 나눈 값  outlier : 나머지 데이터와 멀리 떨어진 레코드  hat-value : 회귀식에 한 레코드가 미치는 영향력의 정도  이분산성 : 어떤 범위 내 출력값의 잔차가 매우 높은 분산을 보이는 경향 ( 어떤 예측변수를 회귀식이 놓치고 있다는 것을 의미 )# 이분산성을 가지는 데이터 생성np.random.seed(0)X = np.linspace(1, 10, 100)  # 독립 변수 Xy = 2 * X + np.random.randn(100) * X  # 종속 변수 y, X의 값에 따라 잔차의 크기가 커짐 (이분산성)다항회귀와 스플라인 회귀  다항회귀  스플라인 회귀 : 다항 구간들을 부드러운 곡선 형태로 피팅  knot : 스플라인 구간을 구분하는 값들  GAM(generalized additive model) : 자동으로 구간을 결정하는 스플라인 모델from pygam import LinearGAM, s# 샘플 데이터 생성np.random.seed(0)X = np.linspace(0, 10, 100)  # 독립 변수y = 5 * X**2 - 3 * X + np.random.normal(0, 10, 100)  # 종속 변수, 2차 다항식 형태# 2D 형태로 변환 (GAM에서 X는 2D로 요구됨)X = X[:, np.newaxis]# GAM 모델 생성 (s()는 스플라인을 의미)gam = LinearGAM(s(0, n_splines=20, spline_order=3)).fit(X, y)# 예측 값 구하기XX = np.linspace(0, 10, 100)predictions = gam.predict(XX)",
      "url": "/2024/10/21/Practical_Statistics_for_Data_Scientists_04.html"
    },
  
    {
      "title": "Funnel_Analysis",
      "tags": "Blog, Funnel_Analysis",
      "desc": "Funnel_Analysis - 2024년10월18일  tag : Blog|Funnel_Analysis|",
      "content": "Funnel_Analysis - 2024년10월18일  tag : Blog|Funnel_Analysis|퍼널 분석  퍼널 분석이란 ?고객들이 우리가 설계한 유저 경험 루트를 따라 잘 도착하고 있는지 확인해보기 위해 최초 유입부터 최종 목적지까지 단계를 나누어서 살펴보는 분석 기법장점  얼마나 많은 사람들이 최종 단계까지 도착하는지, 또 어디에서 많이 이탈하는지 확인할 수 있다.  개선 방향 제시: 이탈률이 높은 단계를 개선함으로써 전체 전환율을 효과적으로 높일 수 있다.  데이터 기반 의사결정: 직관이 아닌 실제 데이터를 바탕으로 개선 전략을 수립할 수 있다.  비즈니스 목표 달성: 전환율 향상을 통해 매출 증대 등 비즈니스 목표 달성에 기여한다.  사용자 경험 개선: 퍼널 분석을 통해 사용자 경험의 문제점을 파악하고 개선할 수 있다.단점  단순화된 시각: 복잡한 사용자 행동을 지나치게 단순화할 수 있다.  제한된 인사이트: 이탈 원인에 대한 깊이 있는 이해를 제공하지 않을 수 있다.  시간 소요: 정확한 분석을 위해서는 충분한 데이터 수집 기간이 필요하다.  기술적 요구사항: 정확한 데이터 추적과 분석을 위한 기술적 인프라가 필요하다.  과대추정 가능성: 전반적으로 전환율이 과대추정되는 경향이 있을 수 있다.용어  전환(Conversion) : 각각의 단계를 넘어가는 것  전환율(Conversion rate) : 전환의 비율예시  목표 설정  퍼널 단계 정의          EX                  웹사이트 방문상품 페이지 조회장바구니 담기결제 페이지 진입구매 완료                      데이터 수집          EX                  웹사이트 방문자 수 : 100,000명상품 페이지 조회 : 50,000명장바구니 담기: 10,000명결제 페이지 진입: 5,000명구매 완료: 2,000명                      퍼널 시각화  데이터 분석 : 퍼널 데이터를 분석하여 주요 이탈 지점을 파악          EX                  웹사이트 방문에서 상품 페이지 조회로의 전환율: 50%상품 페이지에서 장바구니 담기로의 전환율 : 20%장바구니에서 결제 페이지로의 전환율: 50%  결제 페이지에서 구매 완료로의 전환율: 40%                      문제점 식별 : 분석 결과를 바탕으로 가장 큰 이탈이 발생하는 지점을 파악          예시에서 상품 페이지에서 장바구니로의 전환율(20%)이 가장 낮다        원인 분석 : 이탈률이 높은 단계의 원인을 분석          EX                  상품 설명이 불충분한가?가격이 경쟁력이 없는가?‘장바구니 담기’ 버튼이 눈에 잘 띄지 않는가?                      개선 전략 수립 : 분석된 원인을 바탕으로 개선 전략을 수립          EX                  상품 설명 개선 및 상세 이미지 추가경쟁사 대비 가격 조정‘장바구니 담기’ 버튼의 디자인 및 위치 개선                      A/B 테스트 실행 : 개선 전략을 적용한 새로운 버전(B)과 기존 버전(A)을 비교 테스트  결과 분석 및 적용 : A/B 테스트 결과를 분석하여 효과적인 개선사항을 최종 적용          EX :  ‘장바구니 담기’ 버튼의 디자인을 변경하여 전환율이 20%에서 30%로 상승했다면 이를 전체 사이트에 적용        지속적인 모니터링 : 개선 사항 적용 후에도 퍼널을 지속적으로 모니터링하여 전환율의 변화를 관찰하고, 필요시 추가적인 개선을 진행  참고 site",
      "url": "/2024/10/18/Funnel_Analysis.html"
    },
  
    {
      "title": "데이터 과학을 위한 통계 - 통계적 실험과 유의성검정",
      "tags": "Practical_Statistics_for_Data_Scientists, Python, Book",
      "desc": "데이터 과학을 위한 통계 - 통계적 실험과 유의성검정 - 2024년10월16일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|",
      "content": "데이터 과학을 위한 통계 - 통계적 실험과 유의성검정 - 2024년10월16일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|  전형적인 통계 추론 과정  가설을 세운다.  실험을 설계한다.  데이터를 수집한다.  추론 및 결론을 도출한다.A/B 검정  A/B 검정 : 두 가지 처리 방법, 제품, 절차 중 어느 쪽이 다른 쪽보다 더 우월하다는 것을 입증하기 위해 실험군을 두 그룹으로 나누어 진행하는 실험  처리(treatment) : 어떤 대상에 주어지는 특별한 환경이나 조건  처리군(처리 그룹, treatment group) : 특정 처리에 노출된 대상들의 집단  대조군(대조 그룹, control group) : 어떤 처리도 하지 않은 대상들의 집단  임의화(랜덤화, randomization) : 처리를 적용할 대상을 임의로 결정하는 과정  대상(subject) : 처리를 적용할 개체 대상(유의어 : 피실험자)  검정통계량(test statistic) : 처리 효과를 측정하기 위한 지표# A/B 테스트 데이터# A 그룹의 데이터 (예: A 버전의 전환율)A_conversions = 45  # 전환한 사람 수A_total = 500       # A 그룹의 총 사용자 수# B 그룹의 데이터 (예: B 버전의 전환율)B_conversions = 60  # 전환한 사람 수B_total = 520       # B 그룹의 총 사용자 수# 비율 계산 (전환율)A_rate = A_conversions / A_total # 0.0900B_rate = B_conversions / B_total # 0.1154# 각 그룹의 표준 오차 계산A_std_error = np.sqrt(A_rate * (1 - A_rate) / A_total)B_std_error = np.sqrt(B_rate * (1 - B_rate) / B_total)# Z-통계량 계산 (Z-score)z_score = (B_rate - A_rate) / np.sqrt(A_std_error**2 + B_std_error**2) # 1.3377# p-value 계산p_value = stats.norm.sf(abs(z_score)) * 2  # 양측 테스트이므로 * 2 , p-value : # 0.1810  p_value &gt; 유의수준(0.05) 이므로 통계적으로 유의한 차이가 없다.가설검정  귀무가설 : 우연 때문이라는 가설(=영가설)  대립가설 : 귀무가설과의 대조(증명하고자 하는 가설)  일원검정 : 한 방향으로만 우연히 일어날 확률을 계산하는 가설검정  이원검정 : 양방향으로 우연히 일어날 확률을 계산하는 가설검정  위 A/B test 에서 p-value 값이 0.1810 이므로 귀무가설을 기각할 수 없다.재표본추출  재표본추출 : 랜덤한 변동성을 알아보자는 일반적인 목표를 가지고, 관찰된 데이터의 값에서 표본을 반복적으로 추출하는 것을 의미  순열검정 : 두 개 이상의 표본을 함께 결합하여 관측값들을 무작위로 재표본으로 추출하는 과정  복원/비복원 : 표본을 추출할 때, 이미 한 번 뽑은 데이터를 다음번 추출을 위해 다시 제자리에 돌려 놓거나/다음 추출에서 제외하는 표본추출 방법통계적 유의성과 P 값  p-value : 귀무가설을 구체화한 기회 모델이 주어졌을 때 관측된 결과와 같이 특이하거나 극단적인 결과를 얻을 확률  alpha : 실제 결과가 통계적으로 의미 있는 것으로 간주되기 위해, 우연에 의한 결과가 능가해야 하는 ‘비정상적인’ 기능성의 임계 확률  제 1종 오류 : 우연에 의한 효과를 실제 효과라고 잘못 결론 내리는 것  제 2종 오류 : 실제 효과를 우연에 의한 효과라고 잘못 결론 내리는 것t 검정  검정통계량 : 관심의 차이 또는 효과에 대한 측정 지표  t 통계량 : 평균과 같이 표준화된 형태의 일반적인 검정통계량  t 분포 : 관측된 t 통계량을 비교할 수 있는 (귀무가설에서 파생된)기준분포# 두 집단의 데이터 예시group_A = np.array([23, 25, 27, 22, 20, 30, 28, 26, 24, 29])group_B = np.array([31, 33, 35, 29, 27, 40, 36, 34, 32, 38])# 두 집단에 대한 독립 표본 t-검정 수행t_stat, p_value = stats.ttest_ind(group_A, group_B) # t : -5.0138, p : 0.0001# 자유도 계산df = len(group_A) + len(group_B) - 2다중검정  FDR(거짓 발견 비율, false discovery rate) : 다중검정에서 1종 오류가 발생하는 비율  알파 인플레이션 : 1종 오류를 만들 확률인 알파가 더 많은 테스트를 수행할수록 증가하는 다중검정 현상  p값 조정(adjustment of p-value) : 동일한 데이터에 대해 다중검정을 수행하는 경우에 필요하다자유도  n-1 을 자유도라 한다.그 이유는 표본을 통해 모집단의 분석을 추정하고자 할 때 분모의 n을 사용하면 추정치가 살짝 아래쪽으로 편향이 발생하기 때문  d.f.(degrees of freedom) : 자유도분산분석  분산분석(analysis of variance, ANOVA) : 여러 그룹 간의 통계적으로 유의미한 차이를 검정하는 통계적 절차  pairwise comparison : 여러 그룹 중 두 그룹 간의 가설검정  omnibus test : 여러 그룹 평균들의 전체 분산에 관한 단일 가설검정                              분산분해(decomposition of variance) : 구성 요소 분리          Ex. 전체 평균, 처리 평균, 잔차 오차로부터 개별 값들에 대한 기여를 뜻함                      F-statistic : 그룹 평균 간의 차이가 랜덤 모델에서 예상되는 것에서 벗어나는 정도를 측정하는 표준화된 통계량  SS(sum of squares) : 어떤 평균으로부터의 편차들의 제곱합# 세 그룹의 데이터 (임의의 데이터 생성)group_1 = np.array([23, 25, 27, 22, 20, 30, 28, 26, 24, 29])group_2 = np.array([31, 33, 35, 29, 27, 40, 36, 34, 32, 38])group_3 = np.array([19, 21, 17, 23, 18, 20, 22, 19, 21, 24])# 일원분산분석 (ANOVA) 수행f_stat, p_value = stats.f_oneway(group_1, group_2, group_3) # F : 42.2481, p-value &lt; alpha카이제곱검정  카이제곱검정 : 횟수 관련 데이터에 주로 사용되며 예상되는 분포에 얼마나 잘 맞는지를 검정  카이제곱통계량 : 기댓값으로부터 어떤 관찰값까지의 거리를 나타내는 측정치  기댓값 : 어떤 가정(보통 귀무가설)으로부터 데이터가 발생할 때, 그에 대해 기대하는 정도  피어슨 잔차 : \\(R = \\frac{관측값 - 기댓값}{\\sqrt{기댓값}}\\)  자유도 : (r-1)(c-1) [ r : 행, c : 열 ]# 교차표 (contingency table) 데이터 생성# 예시: 두 범주형 변수의 교차 빈도#       예를 들어, 그룹 A와 그룹 B에서 특정 결과를 얻은 빈도observed = np.array([[20, 30],                     [25, 35]])# 카이제곱 검정 수행chi2_stat, p_value, dof, expected = stats.chi2_contingency(observed)# 결과 출력print(f\"Chi-Square 통계량: {chi2_stat:.4f}\") # 카이제곱 통계량 : 1print(f\"P-값: {p_value:.4f}\") # 0print(f\"자유도: {dof}\") # 1print(\"기대빈도:\\n\", expected)# [[20.45454545 29.54545455]# [24.54545455 35.45454545]]멀티암드 밴딧 알고리즘  멀티암드 밴딧 알고리즘은 실험설계에 대한 전통적인 통계적 접근 방식보다 명시적인 최적화와 좀 더 빠른 의사 결정을 가능하게 하며,여러 테스트, 특히 웹 테스트를 위해 사용된다.  멀티암드 밴딧(MAB) : 고객이 선택할 수 있는 손잡이가 여러 개인 가상의 슬롯머신을 말하며, 각 손잡이는 각기 다른 수익을 가져다준다.  손잡이(arm) : 실험에서 어떤 하나의 처리를 말한다.  상금(수익) : 슬롯머신으로 딴 상금에 대한 실험적 비유검정력과 표본크기  효과크기 : 통계 검정을 통해 판단할 수 있는 효과의 최소 크기  검정력 : 주어진 표본크기로 주어진 효과크기를 알아낼 확률  유의수준 : 검증 시 사용할 통계 유의수준",
      "url": "/2024/10/16/Practical_Statistics_for_Data_Scientists_03.html"
    },
  
    {
      "title": "데이터 과학을 위한 통계 - 데이터와 표본분포",
      "tags": "Practical_Statistics_for_Data_Scientists, Python, Book",
      "desc": "데이터 과학을 위한 통계 - 데이터와 표본분포 - 2024년10월12일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|",
      "content": "데이터 과학을 위한 통계 - 데이터와 표본분포 - 2024년10월12일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|임의표본추출과 표본편향  모집단(어떤 데이터 집합을 구성하는 전체 대상 혹은 전체 집합) -&gt; 표본(sample)(모집단에서 얻은 부분집합)      랜덤표본추출 : 무작위로 표본 추출    import pandas as pd# 예시 데이터프레임 생성df = pd.DataFrame({    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry', 'Ivy', 'Jack'],    'Grade': ['A', 'B', 'A', 'C', 'B', 'A', 'C', 'B', 'A', 'B'],    'Score': [85, 72, 90, 65, 78, 88, 62, 75, 92, 70]})# 랜덤 표본 추출drawn_sample = df.sample(n=3)# 결과 출력print(drawn_sample)                  Name      Grade      Score                  Grace      C      62              Ivy      C      92              Charlie      C      90            층화표본추출 : 모집단을 층으로 나눈 뒤, 각 층에서 무작위로 표본을 추출하는 것    from sklearn.model_selection import train_test_splitX = df.drop('Grade', axis=1)  # 특성y = df['Grade']  # 목표 변수# 70%는 훈련 세트, 30%는 테스트 세트로 층화 분할X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)print(\"훈련 세트 크기:\", len(X_train))  # 7print(\"테스트 세트 크기:\", len(X_test)) # 3        편향(bias) : 계통상의 오류  표본편향 : 모집단을 잘못 대표하는 표본선택편향  선택편향 : 관측 데이터를 선택하는 방식 때문에 생기는 편향  데이터 스누핑 : 뭔가 흥미로운 것을 찾아 광범위하게 데이터를 살피는 것  방대한 검색 효과 : 중복 데이터 모델링이나 너무 많은 예측변수를 고려하는 모델링에서 비롯되는 편향 혹은 비재현성-&gt; 해결방법 : 홀드아웃, 목푯값 섞기통계학에서의 표본분포  표본통계량 : 표본 데이터들로부터 얻은 측정 지표  데이터 분포  표본분포 : 표본들의 분포  중심극한정리 : 표본크기가 커질수록 표본분포가 정규분포를 따르는 경향  표준오차(Standard Error) \\(SE =  \\frac{s}{sqrt(n)} [ s : 표준편차, n : 표본크기 ]\\)부트스트랩  부트스트랩 : 표본에서 추가적으로 표본을 복원추출하고 각 표본에 대한 통계량과 모델을 다시 계산하는 것  부트스트랩 표본 : 관측 데이터 집합으로부터 얻은 복원추출 표본  재표본추출(리샘플링) : 관측 데이터로부터 반복해서 표본추출하는 과정, 부트스트랩과 순열(셔플링) 과정을 포함한다.신뢰구간  신뢰수준  구간끝점 : 신뢰구간의 양 끝점  # 신뢰수준 설정 (예: 95%)  confidence_level = 0.95  # 'Score' 열에 대한 신뢰구간 계산  scores = df['Score']  n = len(scores)  mean = np.mean(scores)  std_err = stats.sem(scores) # 표준오차  # scipy.stats.t.interval 함수를 사용하여 신뢰구간 계산  lower_bound, upper_bound = stats.t.interval(confidence_level, df=n-1, loc=mean, scale=std_err)  print(f\"{confidence_level*100}% 신뢰구간:\")  print(f\"하한: {lower_bound:.2f}\") # 70.08  print(f\"상한: {upper_bound:.2f}\") # 85.32정규분포  오차 : 데이터 포인트와 예측값 혹은 평균 사이의 차이  표준화 : 평균을 빼고 표준편차로 나눈 값  z-score : 개별 데이터 포인트를 정규화한 값  표준정규분포 : 평균=0, 표준편차=1 인 정규분포  QQ-plot : 표본분포가 특정 분포에 얼마나 가까운지를 보여주는 그림긴 꼬리 분포  꼬리 : 적은 수의 극단값이 주로 존재하는, 도수분포의 길고 좁은 부분  왜도(skewness) : 분포의 한쪽 꼬리가 반대쪽 다른 꼬리보다 긴 정도스튜던트 t분포  n : 표본의 크기  자유도 : 다른 표본크기, 통계량, 그룹의 수에 따라 t분포를 조절하는 변수이항분포  시행 : 독립된 결과를 가져오는 하나의 사건  성공 : 시험에 대한 관심의 결과  이항식 : 두 가지 결과를 갖는다.  이항시행 : 두 가지 결과를 가져오는 시행  이항분포 : n번 시행에서 성공한 횟수에 대한 분포카이제곱분포  카이제곱통계량 : 검정 결과가 독립성에 대한 귀무 기댓값에서 벗어난 정도를 측정하는 통계량카이제곱분포 : 귀무 모델에서 반복적으로 재표본추출한 통계량 분포이다.F분포  F 통계량의 분포 : 모든 그룹의 평균이 동일한 경우 무작위 순열 데이터에 의해 생성되는 모든 값의 빈도 분포푸아송 분포와 그 외 관련 분포들  람다 : 단위 시간이나 단위 면적당 사건이 발생하는 비율  푸아송 분포 : 표집된 단위 시간 혹은 단위 공간에서 발생한 사건의 도수분포  지수분포 : 한 사건에서 그 다음 사건까지의 시간이나 거리에 대한 도수분포  베이불 분포 : 사건 발생률이 시간에 따라 변화하는, 지수분포의 일반화된 버전",
      "url": "/2024/10/12/Practical_Statistics_for_Data_Scientists_02.html"
    },
  
    {
      "title": "데이터 과학을 위한 통계 - 탐색적 데이터 분석",
      "tags": "Practical_Statistics_for_Data_Scientists, Python, Book",
      "desc": "데이터 과학을 위한 통계 - 탐색적 데이터 분석 - 2024년10월09일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|",
      "content": "데이터 과학을 위한 통계 - 탐색적 데이터 분석 - 2024년10월09일  tag : Practical_Statistics_for_Data_Scientists|Python|Book|탐색적 데이터 분석  Exploratory Data Analysis ( EDA )정형화된 데이터의 요소  수치형(연속, 이산)과 범주형(이진, 순서) 존재테이블 데이터  DataFrame, Feature, Outcome(종속변수), record(DataFrame 에서 한 행을 의미)            col1      col2      col3                  1      5      10              2      10      20              4      20      50              8      40      80      위치 추정  평균 : mean(col1) = (1 + 2 + 4 + 8) / 4 = 3.75  가중평균 : 가중치 = [a, b, c, d] -&gt; 가중평균(col1) = (1 * a + 2 * b + 4 * c + 8 * d) / 4  중간값 : median(col1) = (2 + 4) / 2 = 3  백분위수 : percentile(col1, [0, 25, 50, 75, 100]) = [1, 1.75, 3, 5, 8]          파이썬에서 선형 보간법 사용                        Q1 = 1 + 0.75*(2-1) = 1.75                                      Q3 = 4 + 0.25*(8-4) = 5                      가중 중간값  절사평균(정해진 개수의 극단값을 제외한 나머지 값들의 평균) : trim_mean(col1, 0.25) = (2 + 4) / 2 = 3  로버스트하다(극단값들에 민감하지 않다는 것을 의미), 특잇값변이 추정  편차(EX. 오차, 잔차)  분산(평균과의 편차를 제곱한 값들의 합을 n-1로 나눈 값)          var(col1) = 7.1875 [ n 으로 계산 ]      var(col1) = 9.5833 [ n-1 으로 계산 ]        표준편차 : sqrt(분산)  평균절대편차(평균과의 편차의 절대값의 평균)  중간값의 중위절대편차(중간값과의 편차의 절댓값의 중간값)  범위(데이터의 최댓값과 최솟값의 차이), 순서통계량, 백분위수, 사분위범위(IQR, 75번째 백분위수와 25번째 백분위수 사이의 차이)데이터 분포 탐색하기  상자그림(boxplot)                                                                          도수분포표  히스토그램  밀도 그램(히스토그램을 부드러운 곡선으로 나타낸 그램)이진 데이터와 범주 데이터 탐색하기  최빈값(mode), 기댓값, 막대도표, 파이그림(pie chart)상관관계  상관행렬, 산점도상관계수 : \\(r = \\frac{\\sum (x_{i} - \\bar{x})(y_{i}-\\bar{y})}{(n-1)s_{x}s_{y}}\\)두 개 이상의 변수 탐색하기  분할표()두 가지 이상의 범주형 변수의 빈도수를 기록한 표), 육각형 구간, 등고 도표, 바이올린 도표",
      "url": "/2024/10/09/Practical_Statistics_for_Data_Scientists_01.html"
    }
  
]
