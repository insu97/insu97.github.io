---
layout : markdown
title : 2024 생명연구자원 AI활용 경진대회 인공지능 활용 부문
tags : [multiclass-classification, dacon]
toc : true
---

{% include markdown.html %}

site : [Dacon_site](https://dacon.io/competitions/official/236355/overview/description)

---

# 생각

- 공모전 신청 후 데이터를 다운받아 **train.shape** 을 찍어보니까 컬럼 수가 약 4천 개가 넘어가는 것을 확인
- **WT** 라는 값이 매우 많고 아주 적게 다른 값이 존재함을 확인
- 평소처럼 원-핫 인코딩을 하면 컬럼 수가 더 증가하는 것을 생각하여 사용 X
- **WT** 값은 중요하지 않다고 판단 -> 전처리 방법 생각하기

# 전처리

## 데이터 value

### WT = 0 else 1

```python
train[test.columns] = train[test.columns].map(lambda x:0 if x == 'WT' else 1)
test[test.columns] = test[test.columns].map(lambda x:0 if x == 'WT' else 1)
```

### 행의 모든 값이 WT값인 경우

```python
X = train[test.columns]
y = train['SUBCLASS']

all_wt_train = []

for i in tqdm(range(len(X))):
    if (X.iloc[i, :] == 0).all():
        all_wt_train.append(i)

len(all_wt_train) # 출력 : 94

train.iloc[all_wt_train, :]['SUBCLASS'].value_counts()
```
![image](/assets/images/dacon/Dacon_001.png)

- 모든 행 값이 WT인데 종속변수 값이 나뉘므로 SUBCLASS 값을 가장 많은 24로 통일  
- 참고로 test 데이터에도 모든 행이 WT값인 행이 존재

```python
for i in all_wt_train:
    train.loc[i, 'SUBCLASS'] = 24
```

### WT = 0 and 숫자만 남기기

> 데이터가 숫자가 중심에 있고 영어가 감싸져 있는 형태로 확인  
> 정규식 사용해서 숫자만 분류 후 행으로 값 더하여 새로 데이터 구축

```python
import re

def extract_numbers(s):
    # 입력값이 문자열이 아닐 경우 문자열로 변환
    if not isinstance(s, str):
        s = str(s)
    match = re.search(r'\d+', s)
    if match:
        return match.group()
    else:
        return None
```

```python
for i in tqdm(test.columns):
    A = train.loc[:, i].str.split(expand=True).fillna(0)
    for j in range(len(A.columns)):
        A[j] = A[j].apply(extract_numbers)
    A = A.astype('int')

    train.loc[:, i] = A.sum(axis=1)
```

-> 지금까지 진행했던 학습 중 가장 점수가 높음  
-> 데이터 값이 45 23 25 이렇게 있을 경우 행으로 더해서 값을 바꿔주는 방식이 맞을까?  
-> 감싸져 있는 문자는 아무 의미가 없는걸까?

### WT = 0 and hashlib 라이브러리 사용

> hashlib 라이브러리를 이용해서 모든 문자열을 바이트 -> MD5 -> 16진수 순서로 변환  
> 그 다음 다시 숫자로 변환

```python
import hashlib

# 해시 함수 정의
def hash_variation(variation):
    # 문자열을 바이트로 인코딩
    encoded = variation.encode()
    # hashlib으로 md5 해시 생성
    hash_object = hashlib.md5(encoded)
    # 해시값을 16진수 형태로 반환
    return hash_object.hexdigest()

for i in tqdm(X.columns):
    X[i] = X[i].apply(hash_variation)
    X[i] = X[i].apply(lambda x: np.mean([ord(char) for char in x]))
```

- 코드가 간단한 것에 비해서 성능은 좋음  
- 하지만 데이터에 대한 정보를 잘 살리고 있는지 판단이 안됨.  

### WT = 0, [ original, position, new ]

- 데이터 값이 '문자열' + '숫자' + '문자열' 로 구성되어 있음을 확인
- 한 개의 컬럼을 3개로 나누어 데이터 프레임 재구성

```python
for i in tqdm(X.columns):
    X[i + '_original'] = 0
    X[i + '_position'] = 0
    X[i + '_new'] = 0

    A = X[i].apply(extract_numbers)
    A = A.fillna(0)

    for j in range(len(X)):
        try:
            X.loc[j, i + '_original'], X.loc[j, i + '_position'], X.loc[j, i + '_new'] = A.loc[j][0], A.loc[j][1], A.loc[j][2]
        except:
            pass

    X[i + '_position'] = X[i + '_position'].astype('int')
    X.drop(columns=i, axis=1, inplace=True)
```

- 다음 original 값이 단일 문자 또는 문자열이기 때문에 구분하여 전처리  
- 그 후 단백질 구성 성분으로 맵핑  
  - EX. origianl : RAR -> R A R 로 구분 후 숫자로 맵핑 후 더하기  

> 단일 문자 처리

```python
amino_acid_dict = {
    'A': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'K': 9, 'L': 10,
    'M': 11, 'N': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'V': 18, 'W': 19, 'Y': 20, 'fs' : -1
}

amino_acid_array = np.array([amino_acid_dict[aa] for aa in amino_acid_dict])

# 빠른 벡터화된 매핑 적용 (문자만 변환, 정수는 그대로 유지)
def encode_dataframe(df):
    def encode_cell(cell):
        # 문자열일 경우에만 amino_acid_dict 변환 적용
        if isinstance(cell, str):
            return amino_acid_dict.get(cell, cell)  # 해당하지 않는 경우 원래 값을 반환
        return cell  # 정수는 그대로 반환

    # applymap으로 셀별로 함수 적용
    return df.applymap(encode_cell)

X = encode_dataframe(X)
```

> 문자열 처리

```python
a = []

for i in tqdm(X.columns):
    try:
        X[i].astype('int')
    except:
        a.append(i)

tmp = X[a]

for i in tqdm(tmp.columns):
    for j in range(len(tmp)):
        try:
            tmp.loc[j, i] = int(tmp.loc[j, i])
        except:
            tmp.loc[j, i] = sum(encode_sequence(list(tmp.loc[j,i].upper())))
            pass


tmp = tmp.astype('int')
X[a] = tmp
X = X.astype('int')

```

> 전처리 방법 중 가장 데이터 정보 값을 잘 처리했다고 생각.  

## 변수 선택

- 데이터가 약 6천개의 행과 약 4천개의 컬럼으로 컬럼 수가 많다고 판단  
-- 1. feature 선택 : VarinaceThreshold, Unvariate Feature Selection  
-- 2. 차원축소 : PCA[밀집행렬에만 적용가능], SVD[희소행렬에 적용 가능]

### VarianceThreshold

> 분산 임계값을 충족하지 않는 모든 피처를 제거하는 방식

```python
from sklearn.feature_selection import VarianceThreshold

VT = VarianceThreshold(threshold=(0.02)) # 임계값 정하기
VT.fit_transform(train[test.columns])

VT.get_feature_names_out()  # 남은 피처 확인

# 학습 데이터 재구성
vt_col = VT.get_feature_names_out().tolist()
vt_col.append("SUBCLASS")

train_vt = train[vt_col]
test_vt = test[VT.get_feature_names_out()]
```

### Univariate Feature Selection(UFS)

> 일변량 통계 검정을 기반으로 최적의 피처를 선택  
> 회귀분석일 때 파라미터 : f_rregression, mutual_info_regression  
> 분류분석일 때 파라미터 : chi2, f_classif, mutual_info_classif  

-> 여기서는 분류문제 이므로 chi2를 선택하여 사용했음  

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2 # 카이제곱값 이용

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# 가능한 k 값 (몇 개의 변수를 선택할지)
k_values = range(10, 500, 10)  # 10개씩 증가하면서 최대 500개까지 탐색 (필요에 따라 조정 가능)

# 결과 저장 리스트
mean_scores = []

X = train[test.columns]
y = train['SUBCLASS']

# 각 k 값에 대해 교차 검증을 통한 성능 평가
for k in k_values:
    selector = SelectKBest(chi2, k=k)
    X_new = selector.fit_transform(X, y)

    # 분류 모델 정의 (랜덤 포레스트로 예시)
    clf = RandomForestClassifier()

    # 교차 검증을 통한 성능 평가
    scores = cross_val_score(clf, X_new, y, cv=5)  # 5-fold 교차 검증
    mean_scores.append(np.mean(scores))

# 최적의 k 값 찾기
optimal_k = k_values[np.argmax(mean_scores)]
print(f"최적의 k 값: {optimal_k}")
```

-> 최적의 k값을 찾고 학습시킨 후 제출하였는데 **VarianceThreshold** 보다 정확도가 너무 낮게 나왔다.  
* 이유 ( 내 생각 )
  1. RandomForest 분류 모델이 성능이 좋지 않을 때 [ 서포트 벡터 머신 분류 보다 ]
  2. 파라미터인 카이제곱값이 성능이 좋지 않은 경우
  3. UFS 피쳐 선택방법이 희소행렬일 때 성능이 좋지 않은 경우

### TruncatedSVD

> [나무위키](https://ko.wikipedia.org/wiki/%ED%8A%B9%EC%9E%87%EA%B0%92_%EB%B6%84%ED%95%B4)  

- 특잇값 분해라고 불리는 이 기법은 고윳값을 기저로 하여 대각행렬로 나타낸 행렬 중 상위 일부분(n_components)만 추출해내는 방식  
- 0과 1로 구성된 희소행렬 데이터에 사용하기 적합하다고 판단하여 사용  
- n_components 값이 너무 작으면 데이터 정보가 소실될 수 있다.

```python
from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=1000)
X_train_reduced = svd.fit_transform(X)
```

-> xgboost를 사용하여 과적합이 나온 것을 확인  
-> 규제와 파라미터 값을 변경하여 과적합 해소 후 결과 확인  
-> train_score : 0.4, test_score : 0.3 [정확도] / 제출 점수 : 0.2  
-> 모델이 문제가 아니라 전처리 단계에서 다른 방법을 사용해야 함을 느낌

---

! 추가적으로 기술해야 할 것
1. 전처리 [희소행렬로 바꾸는 방법 말고 다른 방법이 있는지]
2. PCA를 사용해서 희소행렬 처리 시 어떤 문제가 발생하는지

---
# 참고 사이트
1. [머신러닝, 개념과 실습을 한번에](https://curriculum.cosadama.com/machine-learning/2-5/)
2. [PCA, 주성분의 개수는 어떤 기준으로 설정할까?](https://techblog-history-younghunjo1.tistory.com/134)
