---
layout : markdown
title : 2024 생명연구자원 AI활용 경진대회 인공지능 활용 부문
tags : [multiclass-classification, dacon]
toc : true
---

{% include markdown.html %}

site : [Dacon_site](https://dacon.io/competitions/official/236355/overview/description)

---

# 생각

- 공모전 신청 후 데이터를 다운받아 **train.shape** 을 찍어보니까 컬럼 수가 약 4천 개가 넘어가는 것을 확인
- **WT** 라는 값이 매우 많고 아주 적게 다른 값이 존재함을 확인
- 평소처럼 원-핫 인코딩을 하면 컬럼 수가 더 증가하는 것을 생각하여 사용 X
- **WT** 값은 중요하지 않다고 보고 **WT** 값은 0 나머지 값은 1로 변경

# WT값 처리

```python
train[test.columns] = train[test.columns].map(lambda x:0 if x == 'WT' else 1)
test[test.columns] = test[test.columns].map(lambda x:0 if x == 'WT' else 1)
```

- 행의 모든 값이 WT값인 경우 확인

```python
X = train[test.columns]
y = train['SUBCLASS']

all_wt_train = []

for i in tqdm(range(len(X))):
    if (X.iloc[i, :] == 0).all():
        all_wt_train.append(i)

len(all_wt_train) # 출력 : 94

train.iloc[all_wt_train, :]['SUBCLASS'].value_counts()
```
![image](/assets/images/dacon/Dacon_001.png)

- 모든 행 값이 WT인데 종속변수 값이 나뉘므로 SUBCLASS 값을 24로 통일
- 참고로 test 데이터에도 모든 행이 WT값인 행이 존재

```python
for i in all_wt_train:
    train.loc[i, 'SUBCLASS'] = 24
```

# 변수 선택

- 데이터가 약 6천개의 행과 약 4천개의 컬럼으로 컬럼 수가 많다고 판단  
-- 1. feature 선택 : VarinaceThreshold, Unvariate Feature Selection  
-- 2. 차원축소 : PCA[밀집행렬에만 적용가능], SVD[희소행렬에 적용 가능]

## VarianceThreshold

> 분산 임계값을 충족하지 않는 모든 피처를 제거하는 방식

```python
from sklearn.feature_selection import VarianceThreshold

VT = VarianceThreshold(threshold=(0.02)) # 임계값 정하기
VT.fit_transform(train[test.columns])

VT.get_feature_names_out()  # 남은 피처 확인

# 학습 데이터 재구성
vt_col = VT.get_feature_names_out().tolist()
vt_col.append("SUBCLASS")

train_vt = train[vt_col]
test_vt = test[VT.get_feature_names_out()]
```

## Univariate Feature Selection(UFS)

> 일변량 통계 검정을 기반으로 최적의 피처를 선택  
> 회귀분석일 때 파라미터 : f_rregression, mutual_info_regression  
> 분류분석일 때 파라미터 : chi2, f_classif, mutual_info_classif  

-> 여기서는 분류문제 이므로 chi2를 선택하여 사용했음  

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2 # 카이제곱값 이용

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
import numpy as np

# 가능한 k 값 (몇 개의 변수를 선택할지)
k_values = range(10, 500, 10)  # 10개씩 증가하면서 최대 500개까지 탐색 (필요에 따라 조정 가능)

# 결과 저장 리스트
mean_scores = []

X = train[test.columns]
y = train['SUBCLASS']

# 각 k 값에 대해 교차 검증을 통한 성능 평가
for k in k_values:
    selector = SelectKBest(chi2, k=k)
    X_new = selector.fit_transform(X, y)

    # 분류 모델 정의 (랜덤 포레스트로 예시)
    clf = RandomForestClassifier()

    # 교차 검증을 통한 성능 평가
    scores = cross_val_score(clf, X_new, y, cv=5)  # 5-fold 교차 검증
    mean_scores.append(np.mean(scores))

# 최적의 k 값 찾기
optimal_k = k_values[np.argmax(mean_scores)]
print(f"최적의 k 값: {optimal_k}")
```

-> 최적의 k값을 찾고 학습시킨 후 제출하였는데 **VarianceThreshold** 보다 정확도가 너무 낮게 나왔다.  
* 이유 ( 내 생각 )
  1. RandomForest 분류 모델이 성능이 좋지 않을 때 [ 서포트 벡터 머신 분류 보다 ]
  2. 파라미터인 카이제곱값이 성능이 좋지 않은 경우
  3. UFS 피쳐 선택방법이 희소행렬일 때 성능이 좋지 않은 경우

## TruncatedSVD

> [나무위키](https://ko.wikipedia.org/wiki/%ED%8A%B9%EC%9E%87%EA%B0%92_%EB%B6%84%ED%95%B4)  

- 특잇값 분해라고 불리는 이 기법은 고윳값을 기저로 하여 대각행렬로 나타낸 행렬 중 상위 일부분(n_components)만 추출해내는 방식  
- 0과 1로 구성된 희소행렬 데이터에 사용하기 적합하다고 판단하여 사용  
- n_components 값이 너무 작으면 데이터 정보가 소실될 수 있다.

```python
from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=1000)
X_train_reduced = svd.fit_transform(X)
```

-> xgboost를 사용하여 과적합이 나온 것을 확인  
-> 규제와 파라미터 값을 변경하여 과적합 해소 후 결과 확인  
-> train_score : 0.4, test_score : 0.3 [정확도] / 제출 점수 : 0.2  
-> 모델이 문제가 아니라 전처리 단계에서 다른 방법을 사용해야 함을 느낌

---

! 추가적으로 기술해야 할 것
1. 전처리 [희소행렬로 바꾸는 방법 말고 다른 방법이 있는지]
2. PCA를 사용해서 희소행렬 처리 시 어떤 문제가 발생하는지

---
# 참고 사이트
1. [머신러닝, 개념과 실습을 한번에](https://curriculum.cosadama.com/machine-learning/2-5/)
2. [PCA, 주성분의 개수는 어떤 기준으로 설정할까?](https://techblog-history-younghunjo1.tistory.com/134)
