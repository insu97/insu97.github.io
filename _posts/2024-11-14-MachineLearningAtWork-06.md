---
layout : markdown
title : Machine Learning At Work - 06
tags : [Machine_Learning_At_Work, Book]
toc : true
---
{% include markdown.html %}

# 지속적인 머신러닝 활용을 위한 기반 구축하기

## 머신러닝 시스템만의 독특한 어려움

1. 데이터 과학자 vs 소프트웨어 개발자
```
> 데이터 과학자의 KPI는 분석 결과에서 도출한 후 액션이나 높은 정확도의 예측 모델을 사용한 매출 개선 등이다.
> 소프트웨어 개발자는 시스템이 운영 환경에서 잘 운영되도록 개발하는 것을 중시
```
2. 동일한 예측 결과를 얻기 어렵다.
```
> 머신러닝 시스템에서는 조금이라도 변경(EX.데이터)하면 시스템 전체의 행동이 바뀐다.
> 이 특성을 CACE(change anything, change everything)이라고 부른다.
```
3. 지속적인 학습(Continuous Training)과 서빙이 필요하다.
```
> 예측 모델을 업데이트할 때는 새로운 데이터 준비만으로는 충분하지 않다.
> 데이터양이 많아지면 예측 모델을 학습하는 데 매우 많은 시간이 걸려 계산 시간을 줄일 수 있는 구조도 필요하다.
```

## 지속적인 학습과 MLOps

> MLOPS : ML 시스템 개발(Dev)과 ML시스템 운영(Ops)의 통합을 목표로 하는 ML 엔지니어링 문화 및 방식  
> MLOps를 통해 통합, 테스트, 출시, 배포, 인프라 관리를 비롯한 ML 시스템 구성의 모든 단계에서 자동화 및 모니터링을 지원할 수 있다.

- 수행
1. CI(Continuous Integration) : 소스 관리, 단위 테스트, 통합 테스트의 지속적 통합
2. CD(Continuous Delivery) : 소프트웨어 모듈 또는 패키지의 지속적 배포
3. CT(Continuous Training) : 모델의 지속적 학습

## 머신러닝 인프라 구축 단계

1. 공통 실험 환경
- 공통 실험용 도커 이미지 등을 만들고 버전 관리를 해 실험 환경을 쉽게 재현
- 공통 주피터 랩 또는 클라우드 서비스에서 제공하는 주피터 환경을 사용해 실험 내용을 공유하고 재 실행해서 쉽게 확인 및 리뷰를 할 수 있다.
- MLflow와 같은 실험 관리 도구를 활용해 같은 모델을 학습하는 데 필요한 정보나 실험 결과를 쉽게 공유하도록 한다.
> IF 데이터 과학자가 도커 이미지를 만들기 어려워한다면 cookiecutter or cookiecutter-docker-science등의 도구를 활용해 도커 이미지 템플릿을 만드는 것이 좋다.  
> Hydra등을 활용해 설정 파일로 다양한 파라미터를 관리하면 더욱 쉽게 실험을 재현할 수 있다.
2. 예측 결과 서빙
> 학습한 예측 모델을 실제 운영 환경에 배포하고 예측 API 서버를 통해 결과를 반환하는 구조  
> flask 같은 간단한 웹 애플리케이션을 적용해 간단히 구현할 수 있다.
```
클라우드 서비스 : 구글 클라우드 AI 플랫폼 프리딕션, 아마존 세이지메이커의 포스팅 서비스
OSS로 직접 구축
  - 특정 프레임워크에 특화된 서빙 : 텐서플로 서빙, 터치서브
  - 여러 프레임워크에 대응하는 서빙 : 샐던 코어, BentoML, Cortex
모델 버전 관리 : MLflow 모델 레지스트리, 아마존 세이지메이커, 구글 클라우드 AI 플랫폼 프리딕션
```
3. 학습 및 예측의 공통 처리 파이프라인
- 워크플로 엔진 사용
> 범용 소프트웨어 : 아파치 에어플로, 퍼펙트  
> 클라우드 서비스 : 구글 클라우드 컴포저, 워크플로스, 아마존 매니지드 워크플로스 포 아파치 에어플로  
> 머신러닝용으로 만들어진 것 : 메타플로, 쿠베플로 파이프라인, 케드로  
```
전처리를 공통화하는 의미 : 피처 스토어 컴포넌트
클라우드 서비스 : 아마존 세이지메이커 피처 스토어, OSS인 피스트, 홈스웍스
```
4. 지속적인 모델 학습과 배포
- 새로운 모델의 재학습과 배포를 자동화하기 위해 갖춰야 할 구조
```
- 특징량 엔지니어링 로직 단위 테스트
- 학습 시의 손실 수령 여부 테스트
- 파이프라인 산출물 생성 확인 테스트(모델 등)
- 예측 시 초당 쿼리 수
- 예측 정확도의 임계값 초과 여부 테스트
- 스테이징 환경으로의 풀 리퀘스트 병합 등을 트리거로 하는 모델 배포 테스트
- 스테이징 환경에서의 파이프라인 동장 검증 테스트
```

## 지속적인 예측 결과 서빙

1. 감시 및 모니터링
- 예측 결과 서빙 시 지표
> 메모리/CPU 등 하드웨어 리소스 사용량  
> 예측 응답 시간  
> 예측값이 존재하는 윈도우의 평균값, 최댓값, 최솟값, 표준편차 등 통곗값이나 분포  
> 입력값의 통계값, 특히 결손값 또는 NAN의 빈도
- 학습 시 지표
2. 정기적인 테스트
- 운영 환경에서 예측한 결과와 최신 정답 데이터와의 예측 정확도 검증
- 데이터 품질 검증
