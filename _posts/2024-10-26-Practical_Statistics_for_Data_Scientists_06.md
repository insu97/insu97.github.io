---
layout : markdown
title : 데이터 과학을 위한 통계 - 통계적 머신러닝
tags : [Practical_Statistics_for_Data_Scientists, Python, Book]
toc : true
---
{% include markdown.html %}

# k-최근접 이웃

> K-nearest neighbors(KNN)

- 과정
1. 특징들이 가장 유사한 k개의 레코드를 찾는다.
2. 분류 : 이 유사한 레코드들 중에 다수가 속한 클래스가 무엇인지 찾은 후에 새로운 레코드를 그 클래스에 할당한다.
3. 예측(KNN 회귀) : 유사한 레코드들의 평균을 찾아서 새로운 레코드에 대한 예측값으로 사용한다.

{% include plotly/knn_classification.html %}
---
{% include plotly/knn_regression.html %}

# 트리 모델

> 회귀 및 분석 트리(classification and regression tree, CART), Decision tree  
> Random Forest, 부스팅 트리

- 재귀 분할 : 마지막 분할 영역에 해당하는 출력이 최대한 비슷한 결과를 보이도록 데이터를 반복적으로 분할하는 것
- 분할값 : 분할값을 기준으로 예측변수를 그 값보다 작은 영역과 큰 영역으로 나눈다.
- loss : 분류하는 과정에서 발생하는 오분류의 수, 손실이 클수록 불순도가 높다고 할 수 있다.
- 불순도(impurity) : 데이터를 분할한 집합에서 서로 다른 클래스의 데이터가 얼마나 섞여 있는지를 나타냄
- 가지치기(pruning) : 학습이 끝난 트리 모델에서 오버피팅을 줄이기 위해 가지들을 하나씩 잘라내는 과정

# 배깅과 랜덤 포레스트

- 앙상블(ensemble) : 여러 모델의 집합을 이용해서 하나의 예측을 이끌어내는 방식
- 배깅(bagging) : 데이터를 부트스트래핑해서 여러 모델을 만드는 일반적인 방법
- 랜덤 포레스트 : 의사 결정 트리 모델에 기반을 둔 배깅 추정 모델
- 변수 중요도

| | 앙상블 | 배깅 |
| 정의 | 여러 모델을 결합하여 성능 향상 | 중복된 데이터 샘플링과 모델 결합으로 성능 향상 |
| 주요 목적 | 다양한 방법(배깅, 부스팅 등)결합 | 분산 감소, 과적합 방} |
| 결합 방식 | 배깅, 부스팅, 스태킹 등 | 보팅 or 평균화 |
| 대표 알고리즘 | 랜덤 포레스트, 그래디언트 부스팅 | 랜덤 포레스트 |

# 부스팅

- boosting : 연속된 라운드마다 잔차가 큰 레코드들에 가중치를 높여 일련의 모델들을 생성하는 일반 기법
- AdaBoost : 잔차에 따라 데이터의 가중치를 조절하는 부스팅의 초기 버전
- gradient boosting : 비용 함수를 최소화 하는 방향으로 부스팅을 활용하는 좀 더 일반적인 형태
- 확률적 그래디언트 부스팅(stochastic gradient boosting) : 각 라운드마다 레코드와 열을 재표뵨추출하는 것을 포함하는 부스팅의 가장 일반적인 형태
- hyperparameter : 알고리즘을 피팅하기 전에 미리 세팅해야 하는 파라미터
