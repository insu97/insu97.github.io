---
layout : markdown
title : 데이터·AI를 활용한 물가 예측 경진대회 농산물 가격을 중심으로
tags : [dacon]
toc : true
---
{% include markdown.html %}

- site : [Dacon_site](https://www.dacon.io/competitions/official/236381/overview/description)
- 기간 : 2024-10-01 ~ 2024-10-14

---

# 문제설명

1. train_data : 2018 ~ 2021년의 순 단위(10일)
2. test_data : 추론 시점 T가 비식별화된 2022년의 순 단위의 데이터
3. sub_data : 추론 시점 T 기준으로 T+1순, T+2순, T+3순의 평균가격을 예측하여 제출

- 조건  
![image](/assets/images/dacon/Dacon_002.png)

# 전처리

## 품목별로 전처리

### 건고추

- 건고추

```python
train = pd.read_csv("../data/raw_data/train/train.csv")

train_건고추 = train[(train['품목명'] == '건고추') & (train['품종명'] == '화건') & (train['거래단위'] == '30 kg') & (train['등급'] == '상품')]

train_건고추 = train_건고추[['평균가격(원)']]
```

```python
test_00 = pd.read_csv("../data/raw_data/test/TEST_00.csv")

test_00_건고추 = test_00[(test_00['품목명'] == '건고추') & (test_00['품종명'] == '화건') & (test_00['거래단위'] == '30 kg') & (test_00['등급'] == '상품')]

test_00_건고추 = test_00_건고추[['평균가격(원)']]
```

## Scaler

### MinMaxScaler

```python
# MinMaxScaler를 이용한 스케일링
scaler = MinMaxScaler(feature_range=(0, 1))
train_scaled = scaler.fit_transform(train_건고추)
test_scaled = scaler.fit_transform(test_00_건고추)
```

## dataset

```python
# 시계열 데이터를 위한 시퀀스 생성 함수
# T-8 ~ T 시점까지 데이터를 가지고 T+1, T+2, T+3 인 시점 예측
def create_sequences(data, seq_length):
    X = []
    y = []
    for i in range(len(data) - seq_length - 3):
        X.append(data[i:i+seq_length, 0])
        y.append(data[i+seq_length:i+seq_length+3, 0])
    return np.array(X), np.array(y)
```

# Model

## LSTM

```python
# LSTM 모델 생성
model = Sequential()
model.add(LSTM(units=50, return_sequences=False, input_shape=(seq_length, 1)))
model.add(Dense(3))  # T+1, T+2, T+3 예측

model.compile(optimizer='adam', loss='mean_squared_error')

# 모델 학습
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))  # LSTM에 맞게 데이터 형태 변환
model.fit(X_train, y_train, epochs=20, batch_size=32)
```

```python
# 테스트 데이터도 시퀀스 형태로 변환
X_test = np.array([test_scaled.flatten()])  # 테스트 데이터의 마지막 9개 시점을 입력으로 사용
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# 예측 수행
predicted = model.predict(X_test)
# 예측값을 원래 스케일로 역변환
predicted_prices = scaler.inverse_transform(np.concatenate((predicted, np.zeros((predicted.shape[0], 1))), axis=1))[:, :3]

# T+1, T+2, T+3 값을 개별적으로 출력
T_plus_1 = predicted_prices[0, 0]
T_plus_2 = predicted_prices[0, 1]
T_plus_3 = predicted_prices[0, 2]

# 결과 출력
print("예측값 (T+1):", T_plus_1)
print("예측값 (T+2):", T_plus_2)
print("예측값 (T+3):", T_plus_3)
```

# 전처리부터 모델 학습까지

```python
condition = {
    '건고추' : {
        '품종명' : '화건', '거래단위' : '30 kg', '등급' : '상품'
    },
    '사과' : {
        '품종명' : ['홍로', '후지'], '거래단위' : '10 개', '등급' : '상품'
    },
    '감자' : {
        '품종명' : '감자 수미', '거래단위' : '20키로상자', '등급' : '상'
    },
    '배' : {
        '품종명' : '신고', '거래단위' : '10 개', '등급' : '상품'
    },
    '깐마늘(국산)' : {
        '품종명' : '깐마늘(국산)', '거래단위' : '20 kg', '등급' : '상품'
    },
    '무' : {
        '품종명' : '무', '거래단위' : '20키로상자', '등급' : '상'
    },
    '상추' : {
        '품종명' : '청', '거래단위' : '100 g', '등급' : '상품'
    },
    '배추' : {
        '품종명' : '배추', '거래단위' : '10키로망대', '등급' : '상'
    },
    '양파' : {
        '품종명' : '양파', '거래단위' : '1키로', '등급' : '상'
    },
    '대파' : {
        '품종명' : '대파(일반)', '거래단위' : '1키로단', '등급' : '상'
    }
}
```

```python
for i in tqdm(condition):
    try:
        train_c = train[(train['품목명'] == i ) & (train['품종명'] == condition[i]['품종명']) & (train['거래단위'] == condition[i]['거래단위']) & (train['등급'] == condition[i]['등급'])]
    except:
        train_c = train[(train['품목명'] == i ) & (train['품종명'].isin(condition[i]['품종명'])) & (train['거래단위'] == condition[i]['거래단위']) & (train['등급'] == condition[i]['등급'])]
    train_c = train_c[['평균가격(원)']]

    # MinMaxScaler를 이용한 스케일링
    scaler = MinMaxScaler(feature_range=(0, 1))
    train_scaled = scaler.fit_transform(train_c)

    X_train, y_train = create_sequences(train_scaled, seq_length)

    # LSTM 모델 생성
    model = Sequential()
    model.add(LSTM(units=50, return_sequences=False, input_shape=(seq_length, 1)))
    model.add(Dense(3))  # T+1, T+2, T+3 예측

    model.compile(optimizer='adam', loss='mean_squared_error')

    # 모델 학습
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))  # LSTM에 맞게 데이터 형태 변환
    model.fit(X_train, y_train, epochs=20, batch_size=32)

    k = 0

    for j in range(25):
        test = pd.read_csv(f"../data/raw_data/test/TEST_{j:02d}.csv")
        try:
            test = test[(test['품목명'] == i ) & (test['품종명'] == condition[i]['품종명']) & (test['거래단위'] == condition[i]['거래단위']) & (test['등급'] == condition[i]['등급'])]
        except:
            test = test[(test['품목명'] == i ) & (test['품종명'].isin(condition[i]['품종명'])) & (test['거래단위'] == condition[i]['거래단위']) & (test['등급'] == condition[i]['등급'])]

        test = test[['평균가격(원)']]
        test_scaled = scaler.fit_transform(test)

        # 테스트 데이터도 시퀀스 형태로 변환
        X_test = np.array([test_scaled.flatten()])  # 테스트 데이터의 마지막 9개 시점을 입력으로 사용
        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

        # 예측 수행
        predicted = model.predict(X_test)

        # 예측값을 원래 스케일로 역변환
        predicted_prices = scaler.inverse_transform(np.concatenate((predicted, np.zeros((predicted.shape[0], 1))), axis=1))[:, :3]

        # T+1, T+2, T+3 값을 개별적으로 출력
        T_plus_1 = predicted_prices[0, 0]
        T_plus_2 = predicted_prices[0, 1]
        T_plus_3 = predicted_prices[0, 2]

        sub.loc[k, i] = T_plus_1
        k += 1
        sub.loc[k, i] = T_plus_2
        k += 1
        sub.loc[k, i] = T_plus_3
        k += 1
```

# 모델 관련 정리

## RNN

> **RNN (Recurrent Neural Network)**는 순환 신경망으로, 시계열 데이터나 연속된 데이터를 처리하기 위해 고안된 신경망 구조  
> 일반적인 인공 신경망(ANN)은 독립적인 입력을 처리하는 반면, RNN은 이전 단계의 정보를 기억하며 그 정보를 다음 단계의 계산에 반영할 수 있다.  
> RNN의 가장 큰 특징은 순환 구조 -> 네트워크가 **각 타임 스텝(time step)**마다 현재 입력과 **이전의 숨겨진 상태(hidden state)**를 이용해 출력을 계산하는 방식  
> 이를 통해 과거의 정보를 활용할 수 있게 된다.

## LSTM

> **LSTM (Long Short-Term Memory)**는 RNN의 단점을 보완하기 위해 만들어진 변형된 네트워크  
> 기본 RNN의 경우 **장기 의존성 문제(long-term dependency problem)**가 존재  
> > **장기 의존성 문제(long-term dependency problem)** : 시간이 길어지면서 과거의 정보를 잘 기억하지 못하는 현상을 의미  

> LSTM은 이러한 문제를 해결하기 위해 **셀 상태(cell state)**와 **게이트 구조(gates)**를 도입  
> LSTM은 기억하고자 하는 정보를 선택적으로 유지하거나 버릴 수 있게 하여, **장기 및 단기 메모리(long-term and short-term memory)**를 모두 관리할 수 있다.  
> 이로 인해 LSTM은 긴 시퀀스 데이터에서 중요한 정보를 유지하고, 불필요한 정보를 잊어버리는 방식으로 성능을 향상시킬 수 있다.

### 느낀점

- RNN 이 이번 공모전 데이터에 더 좋은 성능을 보였다.[같은 파라미터를 사용]  
- 그 이유는 데이터의 시간이 장기가 아닌 것으로 판단하여 과거의 정보를 잘 기억하여 학습하였다고 생각
