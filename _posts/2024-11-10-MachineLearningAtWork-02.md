---
layout : markdown
title : Machine Learning At Work - 02
tags : [Machine_Learning_At_Work, Book]
toc : true
---
{% include markdown.html %}

# 머신러닝으로 할 수 있는 일

## 머신러닝 알고리즘 선택 방법

> [어떤 알고리즘을 사용할지 참고 사이트](https://scikit-learn.org/stable/machine_learning_map.html)

### 분류

- 알고리즘 종류
1. 퍼셉트론 : 입력 벡터와 학습한 가중치 벡터를 곱한 값을 더해서 그 값이 0 이상이면 1, 아니면 클래스 2로 분류하는 알고리즘
> 온라인 학습 방식으로 취한다.[온라인 학습 : 데이터를 하나씩 입력해 최적화하는 방식 vs 배치 학습 : 데이터를 전부 입력해 최적화하는 방식]  
> 예측 성능은 평이하지만 학습 속도가 빠르다.  
> 과적합되기 쉽다.  
> 선형 분리 가능한 문제만 해결할 수 있다.[선형 분리 가능 : 데이터를 둘로 나누기 좋은 경우]
2. 로지스틱 회귀
> 출력과는 별도로 해당 출력의 클래스에 소속할 확률값을 반환  
> 온라인 학습, 배치 학습 모두 가능  
> 예측 성능은 뛰어나지 않으나 학습 및 추론 속도가 빠르다  
> 과적합을 방지하기 위한 정규화항이 추가되어 있다.
3. 서포트 벡터 머신
> 마진 최대화를 이용해 매끄러운 초평면을 학습할 수 있다.  
> 커널을 사용해 비선형 데이터를 분리할 수 있다.  
> 선형 커널을 이용해 차원 수가 높은 희소 데이터도 학습할 수 있다.  
> 온라인 학습과 배치 학습을 모두 가능하다.  
4. 신경망
> 비선형 데이터를 분리할 수 있다.  
> 학습에 시간이 걸린다.  
> 파라미터 수가 많아 과적합되기 쉽다.  
> 가중치 초깃값이 존재하며 국소 최적해에 빠지기 쉽다.
5. k-최근접 이웃 알고리즘
> 데이터를 하나씩 순서대로 학습한다.  
> 기본적으로 모든 데이터와의 거리 계산을 해야 해 예측에 시간이 걸린다.  
> k의 수에 따라 편차가 있으나 예측 성능은 좋은 편이다.
6. 결정 트리
> 학습한 모델을 사람이 읽고 해석하기 쉽다.  
> 정규화할 필요가 없다.  
> 과적합되기 쉽다.  
> 비선형 분리 가능하지만 선형 분리 가능한 문제는 잘 풀지 못한다.  
> 예측 성능은 평범하고 배치 학습만 가능하다.
7. 랜덤 포레스트 : 이용할 샘플을 무작위로 선택[부트스트랩 샘플링 알고리즘]하고 이용할 특징량을 무작위로 선택해 여러 트리를 만든다.
8. 경사 부스팅 결정트리(GBDT) : 샘플링한 데이터를 이용해 순차적으로 얕은 트리를 학습한느 경사 부스팅을 사용한다.

> XGBOOST or LightGBM 사용  
> 대규모 데이터에 사용가능
> XGBOOST : 확률적 최적화를 수행해 대규모 데이터도 빠르게 처리할 수 있다.  
> LightGBM : 처리 속도가 빠르고, 하이퍼 파라미터 튜닝을 위해 OSS인 Optuna의 확장 기능 LightGBM Tuner를 사용한다.[Optuna](https://github.com/optuna/optuna)

### 회귀

- 알고리즘 종류
1. 선형 회귀[직선], 다항식 회귀[곡선]
2. 라쏘[학습한 가중치의 제곱을 L1정규화], 릿지[L2정규화], 일래스틱넷[둘 다]
3. 회귀 트리[결정 트리 기반의 회귀로 비선형 데이터를 근사할 수 있다.]
4. SVR[SVM 기반의 회귀로 비선형 데이터를 근사할 수 있다.]

### 클러스터링과 차원 축소

- 클러스터링
> 비지도 학습의 한 가지 방법  
> 주로 데이터 경향을 확인하는 목적으로 사용
> EX) 계층적 클러스터링, k-평균

- 차원 축소
> 고차원 데이터의 정보를 가능한 유지하면서 저차원 데이터로 변환
> EX) 주성분 분석(PCA, principal component analysis), t-SNE[시각화에 많이 사용되며 PCA보다 관계성을 이해하기 쉽도록 시각화할 수 있다.]

### 기타

1. 추천
2. 이상 탐지
3. 고빈도 패턴 마이닝
4. 강화 학습
