---
layout : markdown
title : 2024 생명연구자원 AI활용 경진대회 인공지능 활용 부문-2
tags : [Dacon, Multiclass-Classification, ML, DL]
toc : true
---
{% include markdown.html %}

site : [Dacon_site](https://dacon.io/competitions/official/236355/overview/description)

---

# train_test_split

> 전처리한 데이터를 갖고 학습 및 평가하기 위해 진행

```python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

# 학습

## SVC

> [참고](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)  
> 서포트 벡터 머신 기법 중 다중 분류 모델 사용  
> 데이터 스케일에 매우 민감 -> 스케일러 적용 후 학습

- 매개변수  
-- 1. gamma : 하나의 샘플이 미치는 영향의 범위  
-- 2. C : 정규화 매개변수, 정규화의 강도는 C에 반비례

```python
from sklearn.svm import SVC

# 학습
model = SVC(C=1, gamma='scale',random_state=42)
model.fit(x_train, y_train)
```

```python
model.score(x_train, y_train)  # 0.30806451612903224
model.score(x_test, y_test)    # 0.17033852767329394
```

## DL - Tensorflow

> 딥러닝 라이브러리인 Tensorflow 를 사용하여 학습  

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import BatchNormalization

# 모델 정의
model = Sequential()
model.add(Dense(1000, input_shape=(len(x_train.columns), ), activation='relu', kernel_regularizer=l2(0.1)))  # 입력 레이어
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(500, activation='relu', kernel_regularizer=l2(0.1))) # 은닉층
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(100, activation='relu', kernel_regularizer=l2(0.1)) # 은닉층
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(26, activation='softmax'))  # 출력 레이어 (다중 클래스)
```

> 가장 많이 신경 쓴 점은 과적합  
>> 사용한 기법은 가중치 규제(L2), Dropout, BatchNormalization

```python
from tensorflow.keras.optimizers import Adam

# 모델 컴파일
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

from tensorflow.keras.callbacks import Callback
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)

# 모델 학습
history = model.fit(x_train, y_train,
                    validation_data=(x_test, y_test),
                    epochs=50, batch_size=32,
                    callbacks=[reduce_lr, early_stopping])
```

> optimizer 로 Adam 을 사용  
> learning_rate=0.001  
> 손실 함수 : sparse_categorical_crossentropy  
> EarlyStopping : 5번 학습 후 val_loss 값이 좋아지지 않았다면 조기 종료  
> ReduceLROnPlateau : 3번 학습 후 val_loss 값이 좋아지지 않았다면 learning_rate * 0.5

---

# 결과

1. 전처리를 어떤 방식으로 하고, 어떤 방식으로 학습을 시켜도 과적합이 발생
2. 과적합을 해결하면 train_set, test_set 전부 20% 정도의 좋지 않은 정확도를 보여줌
3. 내 생각은 데이터 전처리에 대한 생각을 다시 해야 하거나, 데이터가 좋지 않음을 생각  
